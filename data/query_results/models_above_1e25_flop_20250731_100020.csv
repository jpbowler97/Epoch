name,developer,release_date,parameters,training_flop,training_flop_confidence,estimation_method,status,reasoning,sources,benchmarks
Llama 3.1 405B,Meta,,,3.8e+25,high,company_disclosure,confirmed_above_1e25,"Direct FLOP disclosure from Meta: 3.80e+25 FLOP. Mock data: Found Meta's disclosure of training Llama 3.1 405B on 16,000 H100 GPUs with 15 trillion tokens. Direct FLOP disclosure: 3.8e25 FLOP.",https://ai.meta.com/blog/meta-llama-3/,
Gemini 1.5 Pro,Google,,300000000000.0,2.7e+25,medium,scaling_laws,likely_above_1e25,"Scaling law estimate: 6 × 300,000,000,000 params × 15,000,000,000,000 tokens",https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/,
GPT-4,Unknown,,,2.15e+25,high,company_disclosure,confirmed_above_1e25,Direct FLOP disclosure from Unknown: 2.15e+25 FLOP. Mock data: GPT-4 technical report mentions predictive scaling methods but no direct FLOP disclosure. Estimated from infrastructure costs and performance benchmarks.,https://arxiv.org/abs/2303.08774,
Claude 3.5 Sonnet,Anthropic,,250000000000.0,1.6875e+25,medium,scaling_laws,likely_above_1e25,"Scaling law estimate: 6 × 250,000,000,000 params × 11,250,000,000,000 tokens",https://www.anthropic.com/news/claude-3-5-sonnet,
