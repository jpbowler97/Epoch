name,developer,release_date,parameters,training_flop,training_flop_confidence,estimation_method,status,reasoning,sources,benchmarks
Llama 3.1 405B,Meta,,,3.8e+25,high,company_disclosure,confirmed_above_1e25,"Direct FLOP disclosure from Meta: 3.80e+25 FLOP. Mock data: Found Meta's disclosure of training Llama 3.1 405B on 16,000 H100 GPUs with 15 trillion tokens. Direct FLOP disclosure: 3.8e25 FLOP.",https://ai.meta.com/blog/meta-llama-3/,
Gemini 1.5 Pro,Google,,300000000000.0,2.7e+25,medium,scaling_laws,likely_above_1e25,"LiveBench-based estimate: 6 × 300,000,000,000 params × 15,000,000,000,000 tokens (frontier model assumption)",LiveBench dataset: https://huggingface.co/datasets/lmms-lab/LiveBenchDetailedResults/tree/main/2024-06; LiveBench dataset: https://huggingface.co/datasets/lmms-lab/LiveBenchDetailedResults/tree/main/2024-07,
Llama-3.1-Tulu-3-405B,allenai,2025-01-09T23:43:32+00:00,405000000000.0,2.460375e+25,medium,scaling_laws,likely_above_1e25,"Estimated using scaling laws: 6 × 405,000,000,000 parameters × 10,125,000,000,000 tokens = 2.46e+25 FLOP. Token count estimated from parameter scaling assumptions.",https://huggingface.co (huggingface scraper); https://huggingface.co/allenai/Llama-3.1-Tulu-3-405B (Hugging Face model page),
Meta-Llama-3.1-405B-Instruct-AWQ-INT4,hugging-quants,2024-07-17T19:14:46+00:00,405000000000.0,2.460375e+25,medium,scaling_laws,likely_above_1e25,"Estimated using scaling laws: 6 × 405,000,000,000 parameters × 10,125,000,000,000 tokens = 2.46e+25 FLOP. Token count estimated from parameter scaling assumptions.",https://huggingface.co (huggingface scraper); https://huggingface.co/hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4 (Hugging Face model page),
Hermes-3-Llama-3.1-405B,NousResearch,2024-08-13T04:57:53+00:00,405000000000.0,2.460375e+25,medium,scaling_laws,likely_above_1e25,"Estimated using scaling laws: 6 × 405,000,000,000 parameters × 10,125,000,000,000 tokens = 2.46e+25 FLOP. Token count estimated from parameter scaling assumptions.",https://huggingface.co (huggingface scraper); https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-405B (Hugging Face model page),
Llama-3.1-405B-Instruct,meta-llama,2024-07-16T18:24:44+00:00,405000000000.0,2.460375e+25,medium,scaling_laws,likely_above_1e25,"Estimated using scaling laws: 6 × 405,000,000,000 parameters × 10,125,000,000,000 tokens = 2.46e+25 FLOP. Token count estimated from parameter scaling assumptions.",https://huggingface.co (huggingface scraper); https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct (Hugging Face model page),
Llama-3.1-405B-Instruct-FP8,nvidia,2024-08-29T09:33:35+00:00,405000000000.0,2.460375e+25,medium,scaling_laws,likely_above_1e25,"Estimated using scaling laws: 6 × 405,000,000,000 parameters × 10,125,000,000,000 tokens = 2.46e+25 FLOP. Token count estimated from parameter scaling assumptions.",https://huggingface.co (huggingface scraper); https://huggingface.co/nvidia/Llama-3.1-405B-Instruct-FP8 (Hugging Face model page),
Meta-Llama-3.1-405B-Instruct-FP8-dynamic,RedHatAI,2024-07-23T23:03:49+00:00,405000000000.0,2.460375e+25,medium,scaling_laws,likely_above_1e25,"Estimated using scaling laws: 6 × 405,000,000,000 parameters × 10,125,000,000,000 tokens = 2.46e+25 FLOP. Token count estimated from parameter scaling assumptions.",https://huggingface.co (huggingface scraper); https://huggingface.co/RedHatAI/Meta-Llama-3.1-405B-Instruct-FP8-dynamic (Hugging Face model page),
Meta-Llama-3.1-405B-BNB-NF4-BF16,hugging-quants,2024-07-28T18:14:19+00:00,405000000000.0,2.460375e+25,medium,scaling_laws,likely_above_1e25,"Estimated using scaling laws: 6 × 405,000,000,000 parameters × 10,125,000,000,000 tokens = 2.46e+25 FLOP. Token count estimated from parameter scaling assumptions.",https://huggingface.co (huggingface scraper); https://huggingface.co/hugging-quants/Meta-Llama-3.1-405B-BNB-NF4-BF16 (Hugging Face model page),
Llama-3.1-405B-FP8,meta-llama,2024-07-20T03:20:19+00:00,405000000000.0,2.460375e+25,medium,scaling_laws,likely_above_1e25,"Estimated using scaling laws: 6 × 405,000,000,000 parameters × 10,125,000,000,000 tokens = 2.46e+25 FLOP. Token count estimated from parameter scaling assumptions.",https://huggingface.co (huggingface scraper); https://huggingface.co/meta-llama/Llama-3.1-405B-FP8 (Hugging Face model page),
