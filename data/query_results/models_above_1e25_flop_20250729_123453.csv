name,developer,release_date,parameters,training_flop,training_flop_confidence,estimation_method,status,reasoning,sources,benchmarks
gpt-4-turbo-2024-04-09,OpenAI,,,4.222274606041069e+25,speculative,benchmark_based,confirmed_above_1e25,FLOP estimated from Chatbot Arena ELO rating (82.63) using benchmark-based interpolation relative to known models,https://huggingface.co (chatbot_arena scraper),"{""chatbot_arena_elo"": 82.63}"
Llama 3.1 405B,Meta,,,3.8e+25,high,company_disclosure,confirmed_above_1e25,"Direct FLOP disclosure from Meta: 3.80e+25 FLOP. Mock data: Found Meta's disclosure of training Llama 3.1 405B on 16,000 H100 GPUs with 15 trillion tokens. Direct FLOP disclosure: 3.8e25 FLOP.",https://ai.meta.com/blog/meta-llama-3/,
claude-3-5-sonnet-20240620,Anthropic,,,3.399928176446819e+25,speculative,benchmark_based,confirmed_above_1e25,FLOP estimated from Chatbot Arena ELO rating (79.35) using benchmark-based interpolation relative to known models,https://huggingface.co (chatbot_arena scraper),"{""chatbot_arena_elo"": 79.35}"
gpt-4o-2024-05-13,OpenAI,,,3.3787515838880905e+25,speculative,benchmark_based,confirmed_above_1e25,FLOP estimated from Chatbot Arena ELO rating (79.21) using benchmark-based interpolation relative to known models,https://huggingface.co (chatbot_arena scraper),"{""chatbot_arena_elo"": 79.21}"
gpt-4-0125-preview,OpenAI,,,3.1954259593683518e+25,speculative,benchmark_based,confirmed_above_1e25,FLOP estimated from Chatbot Arena ELO rating (77.96) using benchmark-based interpolation relative to known models,https://huggingface.co (chatbot_arena scraper),"{""chatbot_arena_elo"": 77.96}"
gpt-4o-mini-2024-07-18,OpenAI,,,2.7919086684708916e+25,speculative,benchmark_based,confirmed_above_1e25,FLOP estimated from Chatbot Arena ELO rating (74.94) using benchmark-based interpolation relative to known models,https://huggingface.co (chatbot_arena scraper),"{""chatbot_arena_elo"": 74.94}"
Gemini 1.5 Pro,Google,,300000000000.0,2.7e+25,medium,scaling_laws,likely_above_1e25,"LiveBench-based estimate: 6 × 300,000,000,000 params × 15,000,000,000,000 tokens (frontier model assumption)",LiveBench dataset: https://huggingface.co/datasets/lmms-lab/LiveBenchDetailedResults/tree/main/2024-07; LiveBench dataset: https://huggingface.co/datasets/lmms-lab/LiveBenchDetailedResults/tree/main/2024-06,
Llama-3.1-Tulu-3-405B,allenai,2025-01-09T23:43:32+00:00,405000000000.0,2.460375e+25,medium,scaling_laws,likely_above_1e25,"Estimated using scaling laws: 6 × 405,000,000,000 parameters × 10,125,000,000,000 tokens = 2.46e+25 FLOP. Token count estimated from parameter scaling assumptions.",https://huggingface.co (huggingface scraper); https://huggingface.co/allenai/Llama-3.1-Tulu-3-405B (Hugging Face model page),
Meta-Llama-3.1-405B-Instruct-AWQ-INT4,hugging-quants,2024-07-17T19:14:46+00:00,405000000000.0,2.460375e+25,medium,scaling_laws,likely_above_1e25,"Estimated using scaling laws: 6 × 405,000,000,000 parameters × 10,125,000,000,000 tokens = 2.46e+25 FLOP. Token count estimated from parameter scaling assumptions.",https://huggingface.co (huggingface scraper); https://huggingface.co/hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4 (Hugging Face model page),
Hermes-3-Llama-3.1-405B,NousResearch,2024-08-13T04:57:53+00:00,405000000000.0,2.460375e+25,medium,scaling_laws,likely_above_1e25,"Estimated using scaling laws: 6 × 405,000,000,000 parameters × 10,125,000,000,000 tokens = 2.46e+25 FLOP. Token count estimated from parameter scaling assumptions.",https://huggingface.co (huggingface scraper); https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-405B (Hugging Face model page),
