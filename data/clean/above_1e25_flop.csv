model,developer,release_date,parameters,parameter_source,training_flop,confidence,confidence_explanation,estimation_method,alternative_methods,threshold_classification,reasoning,sources,verified,last_updated,blacklist_status,notes
claude_opus_4,Anthropic,,,,1.5e+26,low,Speculative research estimate,epoch_estimate,Benchmark Based: 5.04e+25 (Low),high_confidence_above_1e25,https://epoch.ai/data-insights/models-over-1e25-flop: Speculative estimate for next-gen Claude,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.046524,allowed,
gpt_5_mini,OpenAI,,,,7.329999999999999e+25,low,Distant benchmark interpolation or single source,benchmark_based,,high_confidence_above_1e25,Multi-benchmark estimation from 2 benchmarks: aai_score: 9.49e+25 (Low); mmlu_pro_score: 5.17e+25 (Low) → weighted average: 7.33e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.077555,allowed,
grok_4,xAI,,,,6.749999999999999e+25,low,Distant benchmark interpolation or single source,benchmark_based,,high_confidence_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.30e+25 (Low); coding_score: 5.20e+25 (Low); aai_score: 1.07e+26 (Low); mmlu_pro_score: 5.78e+25 (Low) → weighted average: 6.75e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.045994,allowed,
gpt_4.5_preview,OpenAI,,,,6.4e+25,low,Speculative research estimate,epoch_estimate,Benchmark Based: 4.75e+25 (Low),high_confidence_above_1e25,https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate for GPT-4.5,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.045856,allowed,
gemini_2.0_flash,Google,,,,6.000000000000001e+25,low,Distant benchmark interpolation or single source,benchmark_based,,high_confidence_above_1e25,Benchmark-based estimation - official parameters not disclosed,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),y,2025-07-31T18:56:49.505562,,
deepseek_coder,DeepSeek,,671000000000.0,known_specification:deepseek,5.959999999999999e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 3.26e+25 (Medium),high_confidence_above_1e25,"Known model specification 'deepseek': Chinchilla scaling law: 6 × 671,000,000,000 params × 14,800,000,000,000 tokens = 5.96e+25 FLOP",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-01T21:13:48.807190,,
deepseek_v3,DeepSeek,,671000000000.0,known_specification:deepseek_v3,5.959999999999999e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 4.07e+25 (Low),high_confidence_above_1e25,"Known model specification 'deepseek_v3': Chinchilla scaling law: 6 × 671,000,000,000 params × 14,800,000,000,000 tokens = 5.96e+25 FLOP",https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),y,2025-08-15T20:36:24.540407,allowed,Manual review - confirmed above 1e25 FLOP. Reason: Parameter estimate is high enough to justify
gemini_2.5_flash,Google,,,,5.76e+25,low,Distant benchmark interpolation or single source,benchmark_based,,high_confidence_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.04e+25 (Low); coding_score: 4.95e+25 (Low); aai_score: 7.79e+25 (Low); mmlu_pro_score: 5.23e+25 (Low) → weighted average: 5.76e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.046858,allowed,
o1,OpenAI,,,,5.559999999999999e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.55e+25 (Medium); coding_score: 4.54e+25 (Medium); aai_score: 8.88e+25 (Low); mmlu_pro_score: 5.38e+25 (Medium) → weighted average: 5.56e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),y,2025-08-04T20:23:58.155429,allowed,Manual review - confirmed above 1e25 FLOP. Reason: Known large reasoning model
gpt_5_nano,OpenAI,,,,5.55e+25,low,Distant benchmark interpolation or single source,benchmark_based,,high_confidence_above_1e25,Multi-benchmark estimation from 2 benchmarks: aai_score: 6.76e+25 (Low); mmlu_pro_score: 4.34e+25 (Low) → weighted average: 5.55e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.078689,allowed,
gpt_5,OpenAI,,,,5.47e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 5.94e+25 (Medium); superclue_math: 5.70e+25 (Medium); superclue_reasoning: 2.77e+25 (Medium); superclue_code: 6.24e+25 (Medium); superclue_agents: 6.69e+25 (Medium) → weighted average: 5.47e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),y,2025-08-11T21:33:30.642316,allowed,Manual review - confirmed above 1e25 FLOP. Reason: I just know GPT5 must be big!
o3,OpenAI,,,,5.3299999999999995e+25,low,Distant benchmark interpolation or single source,benchmark_based,,high_confidence_above_1e25,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 5.64e+25 (Low); superclue_math: 6.62e+25 (Low); superclue_reasoning: 3.52e+25 (Low); superclue_code: 5.53e+25 (Low); superclue_agents: 5.36e+25 (Low) → weighted average: 5.33e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-15T20:32:45.045511,allowed,
grok_3,xAI,,,,4.99e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.15e+25 (Low); coding_score: 5.17e+25 (Low); aai_score: 4.90e+25 (Low); mmlu_pro_score: 4.73e+25 (Low) → weighted average: 4.99e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.046713,allowed,
gemini_2.5_pro,Google,,,,4.86e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.76e+25 (Low); superclue_math: 5.20e+25 (Low); superclue_reasoning: 2.57e+25 (Low); superclue_code: 5.30e+25 (Low); superclue_agents: 6.47e+25 (Low) → weighted average: 4.86e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-15T20:32:45.045308,allowed,
gpt_4.1,OpenAI,,,,4.84e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.68e+25 (Low); coding_score: 4.72e+25 (Low); aai_score: 5.12e+25 (Low); mmlu_pro_score: 4.83e+25 (Low) → weighted average: 4.84e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.046986,allowed,
claude_sonnet_4,Anthropic,,,,4.7699999999999994e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.28e+25 (Low); coding_score: 4.60e+25 (Low); aai_score: 4.90e+25 (Low); mmlu_pro_score: 5.31e+25 (Low) → weighted average: 4.77e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.047723,allowed,
chatgpt_4o,OpenAI,,,,4.7e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.19e+25 (Low); coding_score: 5.11e+25 (Low); aai_score: 3.71e+25 (Low); mmlu_pro_score: 4.79e+25 (Low) → weighted average: 4.70e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.045692,allowed,
claude_opus_4_thinking_16k,Anthropic,,,,4.5799999999999996e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1421.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 4.58e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-15T20:32:45.046088,allowed,
kimi_k2_preview,Moonshot,,,,4.55e+25,medium,,benchmark_based,,not_sure,"Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1380.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling α=3.0 = 4.55e+25 FLOP",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),y,2025-08-01T15:52:05.757110,,Manual review - confirmed above 1e25 FLOP. Reason: Example reasoning
claude_3_7_sonnet,Anthropic,,,,4.47e+25,medium,,benchmark_based,,not_sure,"Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1372.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling α=3.0 = 4.47e+25 FLOP",LMArena Manual Data Collection (CSV file with manually collected leaderboard data),y,2025-08-01T17:43:00.800819,,Manual review - confirmed above 1e25 FLOP. Reason: This is a big model... pretty sure it's over 1e25 flop
gemini_2.0_pro,Google,,,,4.439999999999999e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.87e+25 (Low); coding_score: 4.72e+25 (Low); aai_score: 3.35e+25 (Low); mmlu_pro_score: 4.82e+25 (Low) → weighted average: 4.44e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.077145,allowed,
claude_sonnet_4_thinking_32k,Anthropic,,,,4.379999999999999e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1400.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 4.38e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-15T20:32:45.047064,allowed,
gemini_2.5_flash_lite,Google,,,,4.32e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 2 benchmarks: aai_score: 4.49e+25 (Low); mmlu_pro_score: 4.16e+25 (Low) → weighted average: 4.32e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.078180,allowed,
gpt_4.1_mini,OpenAI,,,,4.32e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.27e+25 (Low); coding_score: 4.45e+25 (Low); aai_score: 4.09e+25 (Low); mmlu_pro_score: 4.47e+25 (Low) → weighted average: 4.32e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.048450,allowed,
qwen3_coder_480b,Alibaba,,480000000000.0,known_specification:qwen3_coder_480b,4.32e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 4.63e+25 (Low),uncertain,"Known model specification 'qwen3_coder_480b': Chinchilla scaling law: 6 × 480,000,000,000 params × 15,000,000,000,000 tokens = 4.32e+25 FLOP",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.047447,allowed,
claude_3_7_sonnet_thinking_32k,Anthropic,,,,4.26e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1387.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 4.26e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-15T20:32:45.047814,allowed,
o1_preview,OpenAI,,,,4.24e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1385.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 4.24e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-15T20:32:45.047907,allowed,
mistral_medium_3,Mistral,,,,4.2299999999999994e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.58e+25 (Low); coding_score: 4.63e+25 (Low); aai_score: 3.52e+25 (Low); mmlu_pro_score: 4.17e+25 (Low) → weighted average: 4.23e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.077809,allowed,
o1_mini,OpenAI,,,,4.18e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.10e+25 (Low); coding_score: 4.42e+25 (Low); aai_score: 4.28e+25 (Low); mmlu_pro_score: 3.93e+25 (Low) → weighted average: 4.18e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.053899,allowed,
claude_opus_4_reasoning,Anthropic,,,,4.1e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.43e+25 (Low); superclue_math: 3.09e+25 (Low); superclue_reasoning: 1.83e+25 (Low); superclue_code: 4.99e+25 (Low); superclue_agents: 6.18e+25 (Low) → weighted average: 4.10e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-15T20:32:45.090923,allowed,
gemini_2.0_flash_001,Google,,,,4.07e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1366.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 4.07e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-15T20:32:45.049025,allowed,
claude_3.7_sonnet,Anthropic,,,,4.0199999999999995e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.93e+25 (Low); coding_score: 4.18e+25 (Low); aai_score: 3.17e+25 (Low); mmlu_pro_score: 4.79e+25 (Low) → weighted average: 4.02e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.078588,allowed,
qwen2.5_max,Alibaba,,,,3.9799999999999995e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.56e+25 (Low); coding_score: 4.49e+25 (Low); aai_score: 2.68e+25 (Low); mmlu_pro_score: 4.20e+25 (Low) → weighted average: 3.98e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.048723,allowed,
claude_3_5_sonnet,Anthropic,,,,3.8399999999999997e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1340.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.84e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-15T20:32:45.048924,allowed,
gpt_4o,OpenAI,,1760000000000.0,known_specification:gpt_4o,3.7999999999999996e+25,medium,Reliable research estimate from industry analysis,epoch_estimate,Scaling Laws: 1.37e+26 (Medium); Benchmark Based: 3.88e+25 (Low),uncertain,https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from OpenAI patterns,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-15T20:32:45.053508,allowed,
llama_3.1_405b,Meta,,405000000000.0,known_specification:llama_3.1_405b,3.7999999999999996e+25,high,Official disclosure or high-precision research estimate,epoch_estimate,Scaling Laws: 3.65e+25 (High),uncertain,https://epoch.ai/data-insights/models-over-1e25-flop: High-precision estimate from Meta disclosure,https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects),,2025-08-15T20:32:45.076283,allowed,
gemini_advanced,Google,,,,3.7699999999999996e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1332.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.77e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-15T20:32:45.054377,allowed,
amazon_nova_chat_05_14,Amazon,,,,3.7299999999999995e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.14e+25 (Low); coding_score: 4.14e+25 (Low); aai_score: 2.84e+25 (Low); mmlu_pro_score: 3.81e+25 (Low) → weighted average: 3.73e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.078463,allowed,
grok_2_mini_08_13,xAI,,,,3.7e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.77e+25 (Low); coding_score: 3.63e+25 (Low) → weighted average: 3.70e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.086892,allowed,
magistral_medium,Mistral,,,,3.7e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.51e+25 (Low); coding_score: 3.87e+25 (Low); aai_score: 3.35e+25 (Low); mmlu_pro_score: 4.08e+25 (Low) → weighted average: 3.70e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.057835,allowed,
gemini_1.5_pro_002,Google,,,,3.68e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.11e+25 (Low); coding_score: 3.91e+25 (Low); aai_score: 2.68e+25 (Low); mmlu_pro_score: 4.04e+25 (Low) → weighted average: 3.68e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.051391,allowed,
gemini_1.5_pro_001,Google,,,,3.67e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1320.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.67e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-15T20:32:45.055896,allowed,
qwen_max,Alibaba,,,,3.67e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.02e+25 (Low); superclue_math: 3.91e+25 (Low); superclue_reasoning: 1.98e+25 (Low); superclue_code: 4.28e+25 (Low); superclue_agents: 4.15e+25 (Low) → weighted average: 3.67e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-15T20:32:45.056390,allowed,
amazon_nova_experimental_chat_05_14,Amazon,,,,3.66e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1318.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.66e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-15T20:32:45.056013,allowed,
claude_3_5_haiku,Anthropic,,,,3.65e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1317.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.65e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-15T20:32:45.056741,allowed,
qwen2.5_plus,Alibaba,,,,3.6299999999999998e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1315.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.63e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-15T20:32:45.056641,allowed,
claude_3.5_sonnet,Anthropic,,,,3.5999999999999997e+25,medium,,company_disclosure,,not_sure,Epoch AI: Low-precision estimate from benchmarks,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),y,2025-07-31T18:56:49.506343,,
grok_2_mini,xAI,,,,3.5699999999999997e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1307.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.57e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-15T20:32:45.058077,allowed,
gpt_4_preview,OpenAI,,,,3.5499999999999996e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.63e+25 (Low); coding_score: 3.48e+25 (Low) → weighted average: 3.55e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.057117,allowed,
gemini_2.0_flash_lite,Google,,,,3.5399999999999996e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.21e+25 (Low); coding_score: 4.15e+25 (Low); aai_score: 2.09e+25 (Low); mmlu_pro_score: 3.70e+25 (Low) → weighted average: 3.54e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.050934,allowed,
mistral_small,Mistral,,,,3.5399999999999996e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.25e+25 (Low); coding_score: 4.37e+25 (Low); aai_score: 2.37e+25 (Low); mmlu_pro_score: 3.17e+25 (Low) → weighted average: 3.54e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.051941,allowed,
qwen_plus,Alibaba,,,,3.4599999999999995e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.42e+25 (Low); coding_score: 3.49e+25 (Low) → weighted average: 3.46e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.052574,allowed,
deepseek_v2_api,DeepSeek,,,,3.44e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.41e+25 (Low); coding_score: 3.47e+25 (Low) → weighted average: 3.44e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.087382,allowed,
gemini_1.5_flash_001,Google,,,,3.38e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1284.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.38e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-15T20:32:45.059746,allowed,
grok_2_08_13,xAI,,,,3.2699999999999996e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.98e+25 (Low); coding_score: 3.79e+25 (Low); aai_score: 1.82e+25 (Low); mmlu_pro_score: 3.51e+25 (Low) → weighted average: 3.27e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.079055,allowed,
gpt_4.1_nano,OpenAI,,,,3.18e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.81e+25 (Low); coding_score: 3.92e+25 (Low); aai_score: 2.09e+25 (Low); mmlu_pro_score: 2.90e+25 (Low) → weighted average: 3.18e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.055418,allowed,
amazon_nova_pro,Amazon,,,,3.12e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.59e+25 (Low); coding_score: 3.65e+25 (Low); aai_score: 1.95e+25 (Low); mmlu_pro_score: 3.29e+25 (Low) → weighted average: 3.12e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.059103,allowed,
gpt_4_turbo,OpenAI,,,,3.12e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.70e+25 (Low); coding_score: 3.64e+25 (Low); aai_score: 1.82e+25 (Low); mmlu_pro_score: 3.33e+25 (Low) → weighted average: 3.12e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.055569,allowed,
gemini_1.5_flash_002,Google,,,,3.1e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.84e+25 (Low); coding_score: 3.58e+25 (Low); aai_score: 1.82e+25 (Low); mmlu_pro_score: 3.16e+25 (Low) → weighted average: 3.10e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.057556,allowed,
mistral_large,Mistral,,,,3.0899999999999998e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.65e+25 (Low); coding_score: 3.67e+25 (Low); aai_score: 1.69e+25 (Low); mmlu_pro_score: 3.36e+25 (Low) → weighted average: 3.09e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.057688,allowed,
gemini_pro_dev_api,Google,,,,2.9999999999999996e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1234.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.00e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-15T20:32:45.063143,allowed,
grok_2,xAI,,,,2.9999999999999996e+25,high,Official disclosure or high-precision research estimate,epoch_estimate,Benchmark Based: 3.77e+25 (Low),uncertain,https://epoch.ai/data-insights/models-over-1e25-flop: High-precision estimate from xAI disclosure,CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-15T20:32:45.054493,allowed,
gpt_4o_mini,OpenAI,,,,2.9799999999999996e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.20e+25 (Low); superclue_math: 2.52e+25 (Low); superclue_reasoning: 1.32e+25 (Low); superclue_code: 3.27e+25 (Low); superclue_agents: 4.59e+25 (Low) → weighted average: 2.98e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-15T20:32:45.056989,allowed,
gemini_pro,Google,,,,2.91e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1222.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 2.91e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-15T20:32:45.063661,allowed,
claude_1,Anthropic,,,,2.8199999999999998e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.92e+25 (Low); coding_score: 2.71e+25 (Low) → weighted average: 2.82e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.088052,allowed,
deepseek_v2.5,DeepSeek,,,,2.8199999999999998e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.05e+25 (Low); superclue_math: 3.12e+25 (Low); superclue_reasoning: 1.20e+25 (Low); superclue_code: 2.95e+25 (Low); superclue_agents: 3.81e+25 (Low) → weighted average: 2.82e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-15T20:32:45.055247,allowed,
claude_3.5_haiku,Anthropic,,,,2.7599999999999997e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.84e+25 (Low); superclue_math: 1.88e+25 (Low); superclue_reasoning: 8.52e+24 (Low); superclue_code: 3.15e+25 (Low); superclue_agents: 5.07e+25 (Low) → weighted average: 2.76e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-15T20:32:45.087163,allowed,
amazon_nova_lite,Amazon,,,,2.61e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.35e+25 (Low); coding_score: 3.41e+25 (Low); aai_score: 1.45e+25 (Low); mmlu_pro_score: 2.22e+25 (Low) → weighted average: 2.61e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.061905,allowed,
gemini_1.0_pro_001,Google,,,,2.61e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.75e+25 (Low); coding_score: 2.47e+25 (Low) → weighted average: 2.61e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.088311,allowed,
gpt_3.5_turbo,OpenAI,,,,2.59e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.63e+25 (Low); coding_score: 2.54e+25 (Low) → weighted average: 2.59e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.063795,allowed,
marco_o1,OpenAI,,,,2.5399999999999998e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.68e+25 (Low); superclue_math: 3.20e+25 (Low); superclue_reasoning: 1.27e+25 (Low); superclue_code: 1.57e+25 (Low); superclue_agents: 4.00e+25 (Low) → weighted average: 2.54e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-15T20:32:45.092549,allowed,
pixtral_large,Mistral,,,,2.4899999999999997e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 2 benchmarks: aai_score: 1.57e+25 (Low); mmlu_pro_score: 3.41e+25 (Low) → weighted average: 2.49e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.089188,allowed,
palm_2,Google,,,,2.37e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1141.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 2.37e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-15T20:32:45.073460,allowed,
claude_3_sonnet,Anthropic,,,,2.3e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.27e+25 (Low); coding_score: 3.24e+25 (Low); aai_score: 5.93e+24 (Low); mmlu_pro_score: 2.11e+25 (Low) → weighted average: 2.30e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.059957,allowed,
amazon_nova_micro,Amazon,,,,2.2599999999999997e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.20e+25 (Low); coding_score: 3.21e+25 (Low); aai_score: 9.27e+24 (Low); mmlu_pro_score: 1.70e+25 (Low) → weighted average: 2.26e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.062330,allowed,
gemini_1.5_pro,Google,,300000000000.0,known_specification:gemini_1.5_pro,2.16e+25,medium,Known parameters with estimated training tokens,scaling_laws,,uncertain,"Known model specification 'gemini_1.5_pro': Chinchilla scaling law: 6 × 300,000,000,000 params × 12,000,000,000,000 tokens = 2.16e+25 FLOP",https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects),,2025-08-15T20:32:45.076174,allowed,
gpt_4,OpenAI,,1760000000000.0,known_specification:gpt_4,2.1e+25,medium,Reliable research estimate from industry analysis,epoch_estimate,Scaling Laws: 1.37e+26 (Medium); Benchmark Based: 3.33e+25 (Low),uncertain,https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from scaling analysis,CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-15T20:32:45.059522,allowed,
claude_3_haiku,Anthropic,,,,1.9899999999999997e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.09e+25 (Low); coding_score: 3.06e+25 (Low); aai_score: 3.34e+24 (Low); mmlu_pro_score: 1.47e+25 (Low) → weighted average: 1.99e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.061597,allowed,
mistral_medium,Mistral,,,,1.84e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.87e+25 (Low); coding_score: 2.79e+25 (Low); aai_score: 2.80e+24 (Low); mmlu_pro_score: 1.40e+25 (Low) → weighted average: 1.84e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.048036,allowed,
claude_3_opus,Anthropic,,175000000000.0,known_specification:claude_3_opus,1.6e+25,medium,Reliable research estimate from industry analysis,epoch_estimate,Scaling Laws: 8.40e+24 (Medium); Benchmark Based: 2.96e+25 (Low),uncertain,https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from industry analysis,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.055768,allowed,
claude_opus,Anthropic,,175000000000.0,known_specification:claude_opus,1.6e+25,medium,Reliable research estimate from industry analysis,epoch_estimate,Scaling Laws: 8.40e+24 (Medium); Benchmark Based: 6.30e+25 (Low),uncertain,https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from industry analysis,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.076961,allowed,
claude_instant_1,Anthropic,,,,1.55e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.61e+25 (Low); coding_score: 2.54e+25 (Low); aai_score: 9.27e+22 (Low); mmlu_pro_score: 1.03e+25 (Low) → weighted average: 1.55e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.088494,allowed,
llama_3.1_405b_instruct_bf16,Meta,,405000000000.0,extracted_from_name,1.4799999999999998e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 3.34e+25 (Low),uncertain,"Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 × 405,000,000,000 params × 6,075,000,000,000 tokens = 1.48e+25 FLOP (Modern large model: 405B params * 15 tokens/param)",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.054057,allowed,
llama_3.1_405b_instruct_fp8,Meta,,405000000000.0,extracted_from_name,1.4799999999999998e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 3.32e+25 (Low),uncertain,"Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 × 405,000,000,000 params × 6,075,000,000,000 tokens = 1.48e+25 FLOP (Modern large model: 405B params * 15 tokens/param)",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.054248,allowed,
qwen3_235b,Alibaba,,235000000000.0,known_specification:qwen3_235b,1.4099999999999999e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 4.94e+25 (Low),uncertain,"Known model specification 'qwen3_235b': Chinchilla scaling law: 6 × 235,000,000,000 params × 10,000,000,000,000 tokens = 1.41e+25 FLOP",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.048596,allowed,
runway_gen_3,Runway,,,,1.0999999999999998e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,uncertain,Threshold-based (physics_iq_score): 22.8 > sora (10.0) → assumed >1e25 FLOP (reference model assumed frontier-level),https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models),,2025-08-15T20:32:45.090364,allowed,
stable_video_diffusion,Stability AI,,,,1.0999999999999998e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,uncertain,Threshold-based (physics_iq_score): 14.8 > sora (10.0) → assumed >1e25 FLOP (reference model assumed frontier-level),https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models),,2025-08-15T20:32:45.090469,allowed,
veo_2,Google,,,,1.0999999999999998e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 1.10e+25 (Medium); video_quality: 1.10e+25 (Medium) → weighted average: 1.10e+25 FLOP,https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings),,2025-08-15T20:32:45.092883,allowed,
veo_3_preview,Google,,,,1.0999999999999998e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 1.10e+25 (Medium); video_quality: 1.10e+25 (Medium) → weighted average: 1.10e+25 FLOP,https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings),,2025-08-15T20:32:45.092740,allowed,
nemotron_4_340b,Nvidia,,340000000000.0,extracted_from_name,1.04e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 3.23e+25 (Low),uncertain,"Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 × 340,000,000,000 params × 5,100,000,000,000 tokens = 1.04e+25 FLOP (Generic estimate: 340B params * 15 tokens/param)",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:32:45.060190,allowed,
doubao_pro_256k,ByteDance,,,,9.9e+24,low,Distant benchmark interpolation or single source,benchmark_based,Benchmark Based: 4.27e+25 (Medium); Original (uncapped): 4.2699999999999995e+25 FLOP,not_sure,FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Limited disclosure on training methodology and data sources for frontier models. Original estimate: 4.27e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.16e+25 (Medium); superclue_math: 4.12e+25 (Medium); superclue_reasoning: 4.01e+25 (Medium); superclue_code: 4.81e+25 (Medium); superclue_agents: 4.27e+25 (Medium) → weighted average: 4.27e+25 FLOP),https://www.supercluebench.com/leaderboard (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T16:03:15.514626,capped,
sora_i2v,Unknown,,,,9.9e+24,low,Distant benchmark interpolation or single source,benchmark_based,,not_sure,Threshold-based (physics_iq_score): 10.0 < sora (10.0) → scaled estimate 9.90e+24 FLOP (α=2.0),https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models),,2025-08-04T16:03:15.513763,allowed,
doubao_lite_32k,ByteDance,,,,,speculative,Speculative confidence from manual research method,manual_research,,not_sure,,https://github.com/openai/open-agent-leaderboard (Open Agent Leaderboard - Multi-task agent performance evaluation),,2025-08-04T14:10:04.589866,allowed,
doubao_lite_4k,ByteDance,,,,,speculative,Speculative confidence from manual research method,manual_research,,not_sure,,https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects),,2025-08-04T14:10:03.915468,allowed,
doubao_pro_128k,ByteDance,,,,,speculative,Speculative confidence from manual research method,manual_research,,not_sure,,https://github.com/openai/open-agent-leaderboard (Open Agent Leaderboard - Multi-task agent performance evaluation),,2025-08-04T14:10:04.589912,allowed,
doubao_pro_32k,ByteDance,,,,,speculative,Speculative confidence from manual research method,manual_research,,not_sure,,https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects),,2025-08-04T14:10:03.915417,allowed,
gemini_1.0_pro,Google,,,,,speculative,Speculative confidence from manual research method,manual_research,,uncertain,,https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects),,2025-08-15T20:21:37.200794,allowed,
hunyuan_standard_vision,Tencent,,,,,speculative,Speculative confidence from manual research method,manual_research,,uncertain,,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:21:37.782525,allowed,
pangu_ultra,Unknown,,,,,speculative,Speculative confidence from manual research method,manual_research,,not_sure,,https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects),,2025-08-04T14:10:03.915579,allowed,
qwen_vl_max,Alibaba,,,,,speculative,Speculative confidence from manual research method,manual_research,,uncertain,,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:21:37.782239,allowed,
step_1o_vision_32k,StepFun,,,,,speculative,Speculative confidence from manual research method,manual_research,,uncertain,,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:21:37.782129,allowed,
step_1v_32k,StepFun,,,,,speculative,Speculative confidence from manual research method,manual_research,,uncertain,,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:21:37.782315,allowed,
yi_large_turbo,01 AI,,,,,speculative,Speculative confidence from manual research method,manual_research,,not_sure,,https://llmbench.github.io/ChineseSimpleQA/ (ChineseSimpleQA - Question answering benchmark for Chinese LLMs),,2025-08-04T14:10:03.623272,allowed,
yi_vision,01 AI,,,,,speculative,Speculative confidence from manual research method,manual_research,,uncertain,,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-15T20:21:37.782612,allowed,
