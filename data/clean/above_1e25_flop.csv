model,developer,release_date,parameters,parameter_source,training_flop,confidence,confidence_explanation,estimation_method,alternative_methods,threshold_classification,status,reasoning,sources,verified,last_updated,blacklist_status,original_estimate,notes
claude_opus_4,Anthropic,,,,1.50e+26,low,Speculative research estimate,epoch_estimate,Benchmark Based: 5.77e+25 (Medium),high_confidence_above_1e25,uncertain,https://epoch.ai/data-insights/models-over-1e25-flop: Speculative estimate for next-gen Claude,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.945575,allowed,,
gemini_2.5_pro,Google,,800000000000.0,known_specification:gemini_2.5_pro,1.20e+26,low,Extracted parameters with uncertain training tokens,scaling_laws,Benchmark Based: 5.02e+25 (Low),high_confidence_above_1e25,uncertain,"Known model specification 'gemini_2.5_pro': Chinchilla scaling law: 6 × 800,000,000,000 params × 25,000,000,000,000 tokens = 1.20e+26 FLOP",https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T19:46:43.944023,allowed,,
grok_4,xAI,,,,7.01e+25,low,Distant benchmark interpolation or single source,benchmark_based,,high_confidence_above_1e25,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.28e+25 (Low); coding_score: 5.19e+25 (Low); aai_score: 1.24e+26 (Low); mmlu_pro_score: 5.78e+25 (Medium) → weighted average: 7.01e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.944900,allowed,,
gpt_4.5_preview,OpenAI,,,,6.40e+25,low,Speculative research estimate,epoch_estimate,Benchmark Based: 5.42e+25 (Medium),high_confidence_above_1e25,uncertain,https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate for GPT-4.5,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.944749,allowed,,
gemini_2.0_flash,Google,,500000000000.0,,6.000000000000001e+25,low,,scaling_laws,,high_confidence_above_1e25,uncertain,"Known model specification: Chinchilla scaling law: 6 × 500,000,000,000 params × 20,000,000,000,000 tokens = 6.00e+25 FLOP",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),y,2025-07-31T18:56:49.505562,,,
deepseek_coder,DeepSeek,,671000000000.0,known_specification:deepseek,5.959999999999999e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 3.26e+25 (Medium),high_confidence_above_1e25,likely_above_1e25,"Known model specification 'deepseek': Chinchilla scaling law: 6 × 671,000,000,000 params × 14,800,000,000,000 tokens = 5.96e+25 FLOP",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-01T21:13:48.807190,,,
deepseek_v3,DeepSeek,,671000000000.0,known_specification:deepseek_v3,5.96e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 4.25e+25 (Low),high_confidence_above_1e25,likely_above_1e25,"Known model specification 'deepseek_v3': Chinchilla scaling law: 6 × 671,000,000,000 params × 14,800,000,000,000 tokens = 5.96e+25 FLOP",https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T19:46:43.947494,allowed,,
grok_3,xAI,,,,5.67e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,high_confidence_above_1e25,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.15e+25 (Low); coding_score: 5.18e+25 (Low); aai_score: 7.29e+25 (Medium); mmlu_pro_score: 4.73e+25 (Medium) → weighted average: 5.67e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.945849,allowed,,
o1,OpenAI,,,,5.56e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,high_confidence_above_1e25,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.55e+25 (Medium); coding_score: 4.54e+25 (Medium); aai_score: 8.88e+25 (Low); mmlu_pro_score: 5.38e+25 (Medium) → weighted average: 5.56e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.946665,allowed,,
o3,OpenAI,,,,5.33e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,high_confidence_above_1e25,likely_above_1e25,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 5.64e+25 (Medium); superclue_math: 6.62e+25 (Medium); superclue_reasoning: 3.52e+25 (Medium); superclue_code: 5.53e+25 (Medium); superclue_agents: 5.36e+25 (Medium) → weighted average: 5.33e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T19:46:43.944334,allowed,,
chatgpt_4o,OpenAI,,,,5.27e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,high_confidence_above_1e25,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.22e+25 (Low); coding_score: 5.13e+25 (Low); aai_score: 5.86e+25 (Medium); mmlu_pro_score: 4.79e+25 (Medium) → weighted average: 5.27e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.944558,allowed,,
o3_mini,OpenAI,,,,5.23e+25,low,Distant benchmark interpolation or single source,benchmark_based,,high_confidence_above_1e25,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.09e+25 (Medium); coding_score: 4.36e+25 (Medium); aai_score: 9.17e+25 (Low); mmlu_pro_score: 4.61e+25 (Medium) → weighted average: 5.23e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.960227,allowed,,
gpt_4.1,OpenAI,,,,5.19e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,high_confidence_above_1e25,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.70e+25 (Medium); coding_score: 4.74e+25 (Medium); aai_score: 6.48e+25 (Medium); mmlu_pro_score: 4.83e+25 (Medium) → weighted average: 5.19e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.946276,allowed,,
claude_sonnet_4,Anthropic,,,,5.18e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,high_confidence_above_1e25,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.28e+25 (Medium); coding_score: 4.62e+25 (Medium); aai_score: 6.51e+25 (Medium); mmlu_pro_score: 5.31e+25 (Medium) → weighted average: 5.18e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.947648,allowed,,
gemini_2.0_pro,Google,,,,5.02e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,high_confidence_above_1e25,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.87e+25 (Low); coding_score: 4.74e+25 (Medium); aai_score: 5.61e+25 (Medium); mmlu_pro_score: 4.82e+25 (Medium) → weighted average: 5.02e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.987383,allowed,,
gpt_4.1_mini,OpenAI,,,,4.90e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.27e+25 (Medium); coding_score: 4.45e+25 (Medium); aai_score: 6.41e+25 (Medium); mmlu_pro_score: 4.47e+25 (Medium) → weighted average: 4.90e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.949718,allowed,,
gemini_2.5_flash,Google,,400000000000.0,known_specification:gemini_2.5_flash,4.80e+25,low,Extracted parameters with uncertain training tokens,scaling_laws,Benchmark Based: 6.15e+25 (Medium),not_sure,uncertain,"Known model specification 'gemini_2.5_flash': Chinchilla scaling law: 6 × 400,000,000,000 params × 20,000,000,000,000 tokens = 4.80e+25 FLOP",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.946036,allowed,,
o1_mini,OpenAI,,,,4.79e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.10e+25 (Medium); coding_score: 4.42e+25 (Medium); aai_score: 6.71e+25 (Medium); mmlu_pro_score: 3.93e+25 (Medium) → weighted average: 4.79e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.962928,allowed,,
mistral_medium_3,Mistral,,,,4.74e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.59e+25 (Medium); coding_score: 4.64e+25 (Medium); aai_score: 5.56e+25 (Medium); mmlu_pro_score: 4.17e+25 (Medium) → weighted average: 4.74e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.987998,allowed,,
magistral_medium,Mistral,,,,4.68e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.51e+25 (Medium); coding_score: 3.87e+25 (Medium); aai_score: 7.27e+25 (Medium); mmlu_pro_score: 4.08e+25 (Medium) → weighted average: 4.68e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.967016,allowed,,
claude_opus_4_thinking_16k,Anthropic,,,,4.58e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,"Benchmark-based (lmarena_score): 1421.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 4.58e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T19:46:43.945013,allowed,,
claude_3.7_sonnet,Anthropic,,,,4.57e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.93e+25 (Medium); coding_score: 4.18e+25 (Medium); aai_score: 5.38e+25 (Medium); mmlu_pro_score: 4.79e+25 (Medium) → weighted average: 4.57e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.988654,allowed,,
kimi_k2_preview,Moonshot,,,,4.55e+25,medium,,benchmark_based,,not_sure,likely_above_1e25,"Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1380.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling α=3.0 = 4.55e+25 FLOP",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),y,2025-08-01T15:52:05.757110,,,Manual review - confirmed above 1e25 FLOP. Reason: Example reasoning
qwen2.5_max,Alibaba,,,,4.50e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.56e+25 (Medium); coding_score: 4.49e+25 (Medium); aai_score: 4.75e+25 (Medium); mmlu_pro_score: 4.20e+25 (Medium) → weighted average: 4.50e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.959528,allowed,,
claude_3_7_sonnet,Anthropic,,,,4.47e+25,medium,,benchmark_based,,not_sure,likely_above_1e25,"Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1372.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling α=3.0 = 4.47e+25 FLOP",LMArena Manual Data Collection (CSV file with manually collected leaderboard data),y,2025-08-01T17:43:00.800819,,,Manual review - confirmed above 1e25 FLOP. Reason: This is a big model... pretty sure it's over 1e25 flop
claude_sonnet_4_thinking_32k,Anthropic,,,,4.38e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,"Benchmark-based (lmarena_score): 1400.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 4.38e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T19:46:43.946386,allowed,,
qwen3_coder_480b,Alibaba,,480000000000.0,known_specification:qwen3_coder_480b,4.32e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 5.37e+25 (Medium),not_sure,likely_above_1e25,"Known model specification 'qwen3_coder_480b': Chinchilla scaling law: 6 × 480,000,000,000 params × 15,000,000,000,000 tokens = 4.32e+25 FLOP",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.947286,allowed,,
claude_opus_4_reasoning,Anthropic,,,,4.27e+25,low,Distant benchmark interpolation or single source,benchmark_based,,not_sure,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.43e+25 (Medium); superclue_math: 3.09e+25 (Medium); superclue_reasoning: 1.83e+25 (Low); superclue_code: 4.99e+25 (Medium); superclue_agents: 6.18e+25 (Medium) → weighted average: 4.27e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T19:46:43.993155,allowed,,
claude_3_7_sonnet_thinking_32k,Anthropic,,,,4.26e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,"Benchmark-based (lmarena_score): 1387.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 4.26e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T19:46:43.947746,allowed,,
o1_preview,OpenAI,,,,4.24e+25,low,Distant benchmark interpolation or single source,benchmark_based,,not_sure,uncertain,"Benchmark-based (lmarena_score): 1385.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 4.24e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T19:46:43.947834,allowed,,
gemini_1.5_pro_002,Google,,,,4.17e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.11e+25 (Medium); coding_score: 3.91e+25 (Medium); aai_score: 4.61e+25 (Medium); mmlu_pro_score: 4.04e+25 (Medium) → weighted average: 4.17e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.961121,allowed,,
amazon_nova_chat_05_14,Amazon,,,,4.07e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.13e+25 (Medium); coding_score: 4.12e+25 (Medium); aai_score: 4.20e+25 (Medium); mmlu_pro_score: 3.81e+25 (Medium) → weighted average: 4.07e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.988502,allowed,,
gemini_2.0_flash_001,Google,,,,4.07e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,"Benchmark-based (lmarena_score): 1366.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 4.07e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T19:46:43.960080,allowed,,
gemini_2.0_flash_lite,Google,,,,4.01e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.21e+25 (Medium); coding_score: 4.15e+25 (Medium); aai_score: 3.97e+25 (Medium); mmlu_pro_score: 3.70e+25 (Medium) → weighted average: 4.01e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.960941,allowed,,
mistral_small,Mistral,,,,3.98e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.25e+25 (Medium); coding_score: 4.37e+25 (Medium); aai_score: 4.15e+25 (Medium); mmlu_pro_score: 3.17e+25 (Medium) → weighted average: 3.98e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.961273,allowed,,
claude_3_5_sonnet,Anthropic,,,,3.84e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,"Benchmark-based (lmarena_score): 1340.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.84e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T19:46:43.959915,allowed,,
gpt_4o,OpenAI,,1760000000000.0,known_specification:gpt_4o,3.80e+25,medium,Reliable research estimate from industry analysis,epoch_estimate,Scaling Laws: 1.37e+26 (Medium); Benchmark Based: 4.00e+25 (Low),not_sure,likely_above_1e25,https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from OpenAI patterns,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T19:46:43.962389,allowed,,
llama_3.1_405b,Meta,,405000000000.0,known_specification:llama_3.1_405b,3.80e+25,high,Official disclosure or high-precision research estimate,epoch_estimate,Scaling Laws: 3.65e+25 (High),not_sure,confirmed_above_1e25,https://epoch.ai/data-insights/models-over-1e25-flop: High-precision estimate from Meta disclosure,https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects),,2025-08-04T19:46:43.986892,allowed,,
qwen_max,Alibaba,,,,3.79e+25,low,Distant benchmark interpolation or single source,benchmark_based,,not_sure,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.02e+25 (Medium); superclue_math: 3.91e+25 (Medium); superclue_reasoning: 1.98e+25 (Low); superclue_code: 4.28e+25 (Medium); superclue_agents: 4.15e+25 (Medium) → weighted average: 3.79e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T19:46:43.965792,allowed,,
gemini_advanced,Google,,,,3.77e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,"Benchmark-based (lmarena_score): 1332.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.77e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T19:46:43.963569,allowed,,
grok_2_08_13,xAI,,,,3.71e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.98e+25 (Medium); coding_score: 3.79e+25 (Medium); aai_score: 3.56e+25 (Medium); mmlu_pro_score: 3.51e+25 (Medium) → weighted average: 3.71e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.988923,allowed,,
grok_2_mini_08_13,xAI,,,,3.70e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.77e+25 (Medium); coding_score: 3.63e+25 (Medium) → weighted average: 3.70e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.989288,allowed,,
gemini_1.5_pro_001,Google,,,,3.67e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,"Benchmark-based (lmarena_score): 1320.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.67e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T19:46:43.965282,allowed,,
amazon_nova_experimental_chat_05_14,Amazon,,,,3.66e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,"Benchmark-based (lmarena_score): 1318.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.66e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T19:46:43.965371,allowed,,
claude_3_5_haiku,Anthropic,,,,3.65e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,"Benchmark-based (lmarena_score): 1317.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.65e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T19:46:43.966124,allowed,,
gpt_4.1_nano,OpenAI,,,,3.63e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.81e+25 (Medium); coding_score: 3.92e+25 (Medium); aai_score: 3.89e+25 (Medium); mmlu_pro_score: 2.90e+25 (Medium) → weighted average: 3.63e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.964843,allowed,,
qwen2.5_plus,Alibaba,,,,3.63e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,"Benchmark-based (lmarena_score): 1315.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.63e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T19:46:43.966033,allowed,,
claude_3.5_sonnet,Anthropic,,,,3.5999999999999997e+25,medium,,company_disclosure,,not_sure,likely_above_1e25,Epoch AI: Low-precision estimate from benchmarks,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),y,2025-07-31T18:56:49.506343,,,
grok_2_mini,xAI,,,,3.57e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,"Benchmark-based (lmarena_score): 1307.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.57e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T19:46:43.967420,allowed,,
gpt_4_preview,OpenAI,,,,3.55e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.63e+25 (Medium); coding_score: 3.48e+25 (Medium) → weighted average: 3.55e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.966384,allowed,,
gpt_4_turbo,OpenAI,,,,3.54e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.70e+25 (Medium); coding_score: 3.64e+25 (Medium); aai_score: 3.49e+25 (Medium); mmlu_pro_score: 3.33e+25 (Medium) → weighted average: 3.54e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.965005,allowed,,
gemini_1.5_flash_002,Google,,,,3.52e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.84e+25 (Medium); coding_score: 3.58e+25 (Medium); aai_score: 3.52e+25 (Medium); mmlu_pro_score: 3.16e+25 (Medium) → weighted average: 3.52e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.966747,allowed,,
mistral_large,Mistral,,,,3.52e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.65e+25 (Medium); coding_score: 3.67e+25 (Medium); aai_score: 3.40e+25 (Medium); mmlu_pro_score: 3.36e+25 (Medium) → weighted average: 3.52e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.966881,allowed,,
qwen_plus,Alibaba,,,,3.46e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.42e+25 (Medium); coding_score: 3.49e+25 (Medium) → weighted average: 3.46e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.961525,allowed,,
deepseek_v2_api,DeepSeek,,,,3.44e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.41e+25 (Medium); coding_score: 3.47e+25 (Medium) → weighted average: 3.44e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.989599,allowed,,
amazon_nova_pro,Amazon,,,,3.43e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.59e+25 (Medium); coding_score: 3.65e+25 (Medium); aai_score: 3.19e+25 (Medium); mmlu_pro_score: 3.29e+25 (Medium) → weighted average: 3.43e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.968621,allowed,,
gemini_1.5_flash_001,Google,,,,3.38e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,"Benchmark-based (lmarena_score): 1284.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.38e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T19:46:43.969346,allowed,,
pixtral_large,Mistral,,,,3.33e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: aai_score: 3.24e+25 (Medium); mmlu_pro_score: 3.41e+25 (Medium) → weighted average: 3.33e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.991222,allowed,,
gpt_4o_mini,OpenAI,,,,3.14e+25,low,Distant benchmark interpolation or single source,benchmark_based,,not_sure,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.20e+25 (Medium); superclue_math: 2.52e+25 (Low); superclue_reasoning: 1.32e+25 (Low); superclue_code: 3.27e+25 (Medium); superclue_agents: 4.59e+25 (Medium) → weighted average: 3.14e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T19:46:43.966276,allowed,,
gemini_1.5_flash,Google,,,,3.11e+25,low,Distant benchmark interpolation or single source,benchmark_based,,not_sure,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.07e+25 (Medium); superclue_math: 2.42e+25 (Low); superclue_reasoning: 9.65e+24 (Low); superclue_code: 3.39e+25 (Medium); superclue_agents: 4.75e+25 (Medium) → weighted average: 3.11e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T19:46:43.995476,allowed,,
gemini_pro_dev_api,Google,,,,3.00e+25,low,Distant benchmark interpolation or single source,benchmark_based,,not_sure,uncertain,"Benchmark-based (lmarena_score): 1234.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.00e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T19:46:43.974649,allowed,,
grok_2,xAI,,,,3.00e+25,high,Official disclosure or high-precision research estimate,epoch_estimate,Benchmark Based: 3.77e+25 (Medium),not_sure,confirmed_above_1e25,https://epoch.ai/data-insights/models-over-1e25-flop: High-precision estimate from xAI disclosure,CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T19:46:43.963714,allowed,,
claude_3.5_haiku,Anthropic,,,,2.97e+25,low,Distant benchmark interpolation or single source,benchmark_based,,not_sure,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.84e+25 (Medium); superclue_math: 1.88e+25 (Low); superclue_reasoning: 8.52e+24 (Low); superclue_code: 3.15e+25 (Medium); superclue_agents: 5.07e+25 (Medium) → weighted average: 2.97e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T19:46:43.989445,allowed,,
deepseek_v2.5,DeepSeek,,,,2.94e+25,low,Distant benchmark interpolation or single source,benchmark_based,,not_sure,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.05e+25 (Medium); superclue_math: 3.12e+25 (Medium); superclue_reasoning: 1.20e+25 (Low); superclue_code: 2.95e+25 (Low); superclue_agents: 3.81e+25 (Medium) → weighted average: 2.94e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T19:46:43.964686,allowed,,
gemini_pro,Google,,,,2.91e+25,low,Distant benchmark interpolation or single source,benchmark_based,,not_sure,uncertain,"Benchmark-based (lmarena_score): 1222.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 2.91e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T19:46:43.975663,allowed,,
amazon_nova_lite,Amazon,,,,2.86e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.35e+25 (Medium); coding_score: 3.41e+25 (Medium); aai_score: 2.45e+25 (Medium); mmlu_pro_score: 2.22e+25 (Medium) → weighted average: 2.86e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.971569,allowed,,
claude_1,Anthropic,,,,2.82e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.92e+25 (Low); coding_score: 2.71e+25 (Low) → weighted average: 2.82e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.990147,allowed,,
marco_o1,OpenAI,,,,2.72e+25,low,Distant benchmark interpolation or single source,benchmark_based,,not_sure,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.68e+25 (Low); superclue_math: 3.20e+25 (Medium); superclue_reasoning: 1.27e+25 (Low); superclue_code: 1.57e+25 (Low); superclue_agents: 4.00e+25 (Medium) → weighted average: 2.72e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T19:46:43.996180,allowed,,
gemini_1.0_pro_001,Google,,,,2.61e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.75e+25 (Low); coding_score: 2.47e+25 (Low) → weighted average: 2.61e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.990370,allowed,,
claude_3_sonnet,Anthropic,,,,2.60e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.27e+25 (Medium); coding_score: 3.24e+25 (Medium); aai_score: 1.79e+25 (Medium); mmlu_pro_score: 2.11e+25 (Medium) → weighted average: 2.60e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.969594,allowed,,
gpt_3.5_turbo,OpenAI,,,,2.59e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.63e+25 (Low); coding_score: 2.54e+25 (Low) → weighted average: 2.59e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.975795,allowed,,
claude_instant_1,Anthropic,,,,2.58e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.61e+25 (Low); coding_score: 2.54e+25 (Low) → weighted average: 2.58e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.990521,allowed,,
amazon_nova_micro,Amazon,,,,2.57e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.20e+25 (Medium); coding_score: 3.21e+25 (Medium); aai_score: 1.86e+25 (Medium); mmlu_pro_score: 1.70e+25 (Low) → weighted average: 2.57e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.973455,allowed,,
palm_2,Google,,,,2.37e+25,low,Distant benchmark interpolation or single source,benchmark_based,,not_sure,uncertain,"Benchmark-based (lmarena_score): 1141.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 2.37e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T19:46:43.983398,allowed,,
claude_3_haiku,Anthropic,,,,2.31e+25,low,Distant benchmark interpolation or single source,benchmark_based,,not_sure,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.09e+25 (Medium); coding_score: 3.06e+25 (Medium); aai_score: 1.35e+25 (Medium); mmlu_pro_score: 1.47e+25 (Low) → weighted average: 2.31e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.971286,allowed,,
gemini_1.5_pro,Google,,300000000000.0,known_specification:gemini_1.5_pro,2.16e+25,medium,Known parameters with estimated training tokens,scaling_laws,,not_sure,likely_above_1e25,"Known model specification 'gemini_1.5_pro': Chinchilla scaling law: 6 × 300,000,000,000 params × 12,000,000,000,000 tokens = 2.16e+25 FLOP",https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects),,2025-08-04T19:46:43.986740,allowed,,
gpt_4,OpenAI,,1760000000000.0,known_specification:gpt_4,2.10e+25,medium,Reliable research estimate from industry analysis,epoch_estimate,Scaling Laws: 1.37e+26 (Medium); Benchmark Based: 3.33e+25 (Medium),not_sure,likely_above_1e25,https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from scaling analysis,CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T19:46:43.969072,allowed,,
mistral_medium,Mistral,,,,1.97e+25,low,Distant benchmark interpolation or single source,benchmark_based,,not_sure,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.87e+25 (Low); coding_score: 2.79e+25 (Low); aai_score: 1.20e+25 (Medium); mmlu_pro_score: 1.40e+25 (Low) → weighted average: 1.97e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.947970,allowed,,
claude_3_opus,Anthropic,,175000000000.0,known_specification:claude_3_opus,1.60e+25,medium,Reliable research estimate from industry analysis,epoch_estimate,Scaling Laws: 8.40e+24 (Medium); Benchmark Based: 3.34e+25 (Medium),not_sure,likely_above_1e25,https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from industry analysis,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.965184,allowed,,
llama_3.1_405b_instruct_bf16,Meta,,405000000000.0,extracted_from_name,1.48e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 3.80e+25 (Medium),not_sure,likely_above_1e25,"Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 × 405,000,000,000 params × 6,075,000,000,000 tokens = 1.48e+25 FLOP (Modern large model: 405B params * 15 tokens/param)",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.963237,allowed,,
llama_3.1_405b_instruct_fp8,Meta,,405000000000.0,extracted_from_name,1.48e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 3.78e+25 (Medium),not_sure,likely_above_1e25,"Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 × 405,000,000,000 params × 6,075,000,000,000 tokens = 1.48e+25 FLOP (Modern large model: 405B params * 15 tokens/param)",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.963436,allowed,,
qwen3_235b,Alibaba,,235000000000.0,known_specification:qwen3_235b,1.41e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 5.57e+25 (Medium),not_sure,likely_above_1e25,"Known model specification 'qwen3_235b': Chinchilla scaling law: 6 × 235,000,000,000 params × 10,000,000,000,000 tokens = 1.41e+25 FLOP",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.949909,allowed,,
runway_gen_3,Runway,,,,1.10e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Threshold-based (physics_iq_score): 22.8 > sora (10.0) → assumed >1e25 FLOP (reference model assumed frontier-level),https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models),,2025-08-04T19:46:43.992345,allowed,,
stable_video_diffusion,Stability AI,,,,1.10e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Threshold-based (physics_iq_score): 14.8 > sora (10.0) → assumed >1e25 FLOP (reference model assumed frontier-level),https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models),,2025-08-04T19:46:43.992458,allowed,,
veo_2,Google,,,,1.10e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 1.10e+25 (Medium); video_quality: 1.10e+25 (Medium) → weighted average: 1.10e+25 FLOP,https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings),,2025-08-04T19:46:43.996760,allowed,,
veo_3_preview,Google,,,,1.10e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,not_sure,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 1.10e+25 (Medium); video_quality: 1.10e+25 (Medium) → weighted average: 1.10e+25 FLOP,https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings),,2025-08-04T19:46:43.996457,allowed,,
nemotron_4_340b,Nvidia,,340000000000.0,extracted_from_name,1.04e+25,low,Extracted parameters with uncertain training tokens,scaling_laws,Benchmark Based: 3.23e+25 (Medium),not_sure,uncertain,"Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 × 340,000,000,000 params × 5,100,000,000,000 tokens = 1.04e+25 FLOP (Generic estimate: 340B params * 15 tokens/param)",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:43.969845,allowed,,
doubao_pro_256k,ByteDance,,,,9.9e+24,low,Distant benchmark interpolation or single source,benchmark_based,Benchmark Based: 4.27e+25 (Medium),not_sure,uncertain,FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Limited disclosure on training methodology and data sources for frontier models. Original estimate: 4.27e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.16e+25 (Medium); superclue_math: 4.12e+25 (Medium); superclue_reasoning: 4.01e+25 (Medium); superclue_code: 4.81e+25 (Medium); superclue_agents: 4.27e+25 (Medium) → weighted average: 4.27e+25 FLOP),https://www.supercluebench.com/leaderboard (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T16:03:15.514626,capped,4.2699999999999995e+25,
sora_i2v,Unknown,,,,9.9e+24,low,Distant benchmark interpolation or single source,benchmark_based,,not_sure,uncertain,Threshold-based (physics_iq_score): 10.0 < sora (10.0) → scaled estimate 9.90e+24 FLOP (α=2.0),https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models),,2025-08-04T16:03:15.513763,allowed,,
doubao_lite_32k,ByteDance,,,,,speculative,Speculative confidence from manual research method,manual_research,,not_sure,uncertain,,https://github.com/openai/open-agent-leaderboard (Open Agent Leaderboard - Multi-task agent performance evaluation),,2025-08-04T14:10:04.589866,allowed,,
doubao_lite_4k,ByteDance,,,,,speculative,Speculative confidence from manual research method,manual_research,,not_sure,uncertain,,https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects),,2025-08-04T14:10:03.915468,allowed,,
doubao_pro_128k,ByteDance,,,,,speculative,Speculative confidence from manual research method,manual_research,,not_sure,uncertain,,https://github.com/openai/open-agent-leaderboard (Open Agent Leaderboard - Multi-task agent performance evaluation),,2025-08-04T14:10:04.589912,allowed,,
doubao_pro_32k,ByteDance,,,,,speculative,Speculative confidence from manual research method,manual_research,,not_sure,uncertain,,https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects),,2025-08-04T14:10:03.915417,allowed,,
gemini_1.0_pro,Google,,,,,speculative,Speculative confidence from manual research method,manual_research,,not_sure,uncertain,,https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects),,2025-08-04T19:46:32.159097,allowed,,
hunyuan_standard_vision,Tencent,,,,,speculative,Speculative confidence from manual research method,manual_research,,not_sure,uncertain,,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:32.611230,allowed,,
pangu_ultra,Unknown,,,,,speculative,Speculative confidence from manual research method,manual_research,,not_sure,uncertain,,https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects),,2025-08-04T14:10:03.915579,allowed,,
qwen_vl_max,Alibaba,,,,,speculative,Speculative confidence from manual research method,manual_research,,not_sure,uncertain,,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:32.610974,allowed,,
step_1o_vision_32k,StepFun,,,,,speculative,Speculative confidence from manual research method,manual_research,,not_sure,uncertain,,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:32.610906,allowed,,
step_1v_32k,StepFun,,,,,speculative,Speculative confidence from manual research method,manual_research,,not_sure,uncertain,,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:32.611077,allowed,,
yi_large_turbo,01 AI,,,,,speculative,Speculative confidence from manual research method,manual_research,,not_sure,uncertain,,https://llmbench.github.io/ChineseSimpleQA/ (ChineseSimpleQA - Question answering benchmark for Chinese LLMs),,2025-08-04T14:10:03.623272,allowed,,
yi_vision,01 AI,,,,,speculative,Speculative confidence from manual research method,manual_research,,not_sure,uncertain,,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T19:46:32.611291,allowed,,
