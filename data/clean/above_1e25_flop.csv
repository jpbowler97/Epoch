model,developer,release_date,parameters,parameter_source,training_flop,confidence,confidence_explanation,estimation_method,alternative_methods,status,reasoning,sources,verified,last_updated,blacklist_status,original_estimate,notes,threshold_classification
claude_opus_4,Anthropic,,,,1.5000000000000002e+26,low,Speculative research estimate,epoch_estimate,Benchmark Based: 5.77e+25 (Medium),uncertain,https://epoch.ai/data-insights/models-over-1e25-flop: Speculative estimate for next-gen Claude,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.514833,allowed,,,
gemini_2.5_pro,Google,,800000000000.0,known_specification:gemini_2.5_pro,1.2000000000000002e+26,low,Extracted parameters with uncertain training tokens,scaling_laws,Benchmark Based: 5.02e+25 (Low),uncertain,"Known model specification 'gemini_2.5_pro': Chinchilla scaling law: 6 × 800,000,000,000 params × 25,000,000,000,000 tokens = 1.20e+26 FLOP",https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T20:15:51.513454,allowed,,,
grok_4,xAI,,,,7.01e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.28e+25 (Low); coding_score: 5.19e+25 (Low); aai_score: 1.24e+26 (Low); mmlu_pro_score: 5.78e+25 (Medium) → weighted average: 7.01e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.514241,allowed,,,
gpt_4.5_preview,OpenAI,,,,6.4e+25,low,Speculative research estimate,epoch_estimate,Benchmark Based: 5.42e+25 (Medium),uncertain,https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate for GPT-4.5,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.514090,allowed,,,
gemini_2.0_flash,Google,,500000000000.0,,6.000000000000001e+25,low,,scaling_laws,,uncertain,"Known model specification: Chinchilla scaling law: 6 × 500,000,000,000 params × 20,000,000,000,000 tokens = 6.00e+25 FLOP",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),y,2025-07-31T18:56:49.505562,,,,high_confidence_above_1e25
deepseek_coder,DeepSeek,,671000000000.0,known_specification:deepseek,5.959999999999999e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 3.26e+25 (Medium),likely_above_1e25,"Known model specification 'deepseek': Chinchilla scaling law: 6 × 671,000,000,000 params × 14,800,000,000,000 tokens = 5.96e+25 FLOP",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-01T21:13:48.807190,,,,high_confidence_above_1e25
deepseek_v3,DeepSeek,,671000000000.0,known_specification:deepseek_v3,5.959999999999999e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 4.25e+25 (Low),likely_above_1e25,"Known model specification 'deepseek_v3': Chinchilla scaling law: 6 × 671,000,000,000 params × 14,800,000,000,000 tokens = 5.96e+25 FLOP",https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T20:15:51.516306,allowed,,,
grok_3,xAI,,,,5.669999999999999e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.15e+25 (Low); coding_score: 5.18e+25 (Low); aai_score: 7.29e+25 (Medium); mmlu_pro_score: 4.73e+25 (Medium) → weighted average: 5.67e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.514983,allowed,,,
o1,OpenAI,,,,5.559999999999999e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.55e+25 (Medium); coding_score: 4.54e+25 (Medium); aai_score: 8.88e+25 (Low); mmlu_pro_score: 5.38e+25 (Medium) → weighted average: 5.56e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),y,2025-08-04T20:23:58.155429,allowed,,Manual review - confirmed above 1e25 FLOP. Reason: Known large reasoning model,
o3,OpenAI,,,,5.33e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 5.64e+25 (Medium); superclue_math: 6.62e+25 (Medium); superclue_reasoning: 3.52e+25 (Medium); superclue_code: 5.53e+25 (Medium); superclue_agents: 5.36e+25 (Medium) → weighted average: 5.33e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T20:15:51.513732,allowed,,,
chatgpt_4o,OpenAI,,,,5.269999999999999e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.22e+25 (Low); coding_score: 5.13e+25 (Low); aai_score: 5.86e+25 (Medium); mmlu_pro_score: 4.79e+25 (Medium) → weighted average: 5.27e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.513916,allowed,,,
o3_mini,OpenAI,,,,5.229999999999999e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.09e+25 (Medium); coding_score: 4.36e+25 (Medium); aai_score: 9.17e+25 (Low); mmlu_pro_score: 4.61e+25 (Medium) → weighted average: 5.23e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.518462,allowed,,,
gpt_4.1,OpenAI,,,,5.189999999999999e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.70e+25 (Medium); coding_score: 4.74e+25 (Medium); aai_score: 6.48e+25 (Medium); mmlu_pro_score: 4.83e+25 (Medium) → weighted average: 5.19e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.515312,allowed,,,
claude_sonnet_4,Anthropic,,,,5.18e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.28e+25 (Medium); coding_score: 4.62e+25 (Medium); aai_score: 6.51e+25 (Medium); mmlu_pro_score: 5.31e+25 (Medium) → weighted average: 5.18e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.516536,allowed,,,
gemini_2.0_pro,Google,,,,5.019999999999999e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.87e+25 (Low); coding_score: 4.74e+25 (Medium); aai_score: 5.61e+25 (Medium); mmlu_pro_score: 4.82e+25 (Medium) → weighted average: 5.02e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.553954,allowed,,,
gpt_4.1_mini,OpenAI,,,,4.899999999999999e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.27e+25 (Medium); coding_score: 4.45e+25 (Medium); aai_score: 6.41e+25 (Medium); mmlu_pro_score: 4.47e+25 (Medium) → weighted average: 4.90e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.517372,allowed,,,
gemini_2.5_flash,Google,,400000000000.0,known_specification:gemini_2.5_flash,4.8e+25,low,Extracted parameters with uncertain training tokens,scaling_laws,Benchmark Based: 6.15e+25 (Medium),uncertain,"Known model specification 'gemini_2.5_flash': Chinchilla scaling law: 6 × 400,000,000,000 params × 20,000,000,000,000 tokens = 4.80e+25 FLOP",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.515166,allowed,,,
o1_mini,OpenAI,,,,4.79e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.10e+25 (Medium); coding_score: 4.42e+25 (Medium); aai_score: 6.71e+25 (Medium); mmlu_pro_score: 3.93e+25 (Medium) → weighted average: 4.79e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.522543,allowed,,,
mistral_medium_3,Mistral,,,,4.74e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.59e+25 (Medium); coding_score: 4.64e+25 (Medium); aai_score: 5.56e+25 (Medium); mmlu_pro_score: 4.17e+25 (Medium) → weighted average: 4.74e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.555023,allowed,,,
magistral_medium,Mistral,,,,4.68e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.51e+25 (Medium); coding_score: 3.87e+25 (Medium); aai_score: 7.27e+25 (Medium); mmlu_pro_score: 4.08e+25 (Medium) → weighted average: 4.68e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.529417,allowed,,,
claude_opus_4_thinking_16k,Anthropic,,,,4.58e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,"Benchmark-based (lmarena_score): 1421.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 4.58e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T20:15:51.514359,allowed,,,
claude_3.7_sonnet,Anthropic,,,,4.57e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.93e+25 (Medium); coding_score: 4.18e+25 (Medium); aai_score: 5.38e+25 (Medium); mmlu_pro_score: 4.79e+25 (Medium) → weighted average: 4.57e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.556278,allowed,,,
kimi_k2_preview,Moonshot,,,,4.55e+25,medium,,benchmark_based,,likely_above_1e25,"Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1380.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling α=3.0 = 4.55e+25 FLOP",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),y,2025-08-01T15:52:05.757110,,,Manual review - confirmed above 1e25 FLOP. Reason: Example reasoning,not_sure
qwen2.5_max,Alibaba,,,,4.5e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.56e+25 (Medium); coding_score: 4.49e+25 (Medium); aai_score: 4.75e+25 (Medium); mmlu_pro_score: 4.20e+25 (Medium) → weighted average: 4.50e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.517726,allowed,,,
claude_3_7_sonnet,Anthropic,,,,4.47e+25,medium,,benchmark_based,,likely_above_1e25,"Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1372.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling α=3.0 = 4.47e+25 FLOP",LMArena Manual Data Collection (CSV file with manually collected leaderboard data),y,2025-08-01T17:43:00.800819,,,Manual review - confirmed above 1e25 FLOP. Reason: This is a big model... pretty sure it's over 1e25 flop,not_sure
claude_sonnet_4_thinking_32k,Anthropic,,,,4.379999999999999e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,"Benchmark-based (lmarena_score): 1400.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 4.38e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T20:15:51.515417,allowed,,,
qwen3_coder_480b,Alibaba,,480000000000.0,known_specification:qwen3_coder_480b,4.32e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 5.37e+25 (Medium),likely_above_1e25,"Known model specification 'qwen3_coder_480b': Chinchilla scaling law: 6 × 480,000,000,000 params × 15,000,000,000,000 tokens = 4.32e+25 FLOP",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.516021,allowed,,,
claude_opus_4_reasoning,Anthropic,,,,4.27e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.43e+25 (Medium); superclue_math: 3.09e+25 (Medium); superclue_reasoning: 1.83e+25 (Low); superclue_code: 4.99e+25 (Medium); superclue_agents: 6.18e+25 (Medium) → weighted average: 4.27e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T20:15:51.562187,allowed,,,
claude_3_7_sonnet_thinking_32k,Anthropic,,,,4.26e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,"Benchmark-based (lmarena_score): 1387.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 4.26e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T20:15:51.516657,allowed,,,
o1_preview,OpenAI,,,,4.24e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1385.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 4.24e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T20:15:51.516761,allowed,,,
gemini_1.5_pro_002,Google,,,,4.169999999999999e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.11e+25 (Medium); coding_score: 3.91e+25 (Medium); aai_score: 4.61e+25 (Medium); mmlu_pro_score: 4.04e+25 (Medium) → weighted average: 4.17e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.519921,allowed,,,
amazon_nova_chat_05_14,Amazon,,,,4.07e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.13e+25 (Medium); coding_score: 4.12e+25 (Medium); aai_score: 4.20e+25 (Medium); mmlu_pro_score: 3.81e+25 (Medium) → weighted average: 4.07e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.555908,allowed,,,
gemini_2.0_flash_001,Google,,,,4.07e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,"Benchmark-based (lmarena_score): 1366.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 4.07e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T20:15:51.518094,allowed,,,
gemini_2.0_flash_lite,Google,,,,4.01e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.21e+25 (Medium); coding_score: 4.15e+25 (Medium); aai_score: 3.97e+25 (Medium); mmlu_pro_score: 3.70e+25 (Medium) → weighted average: 4.01e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.519674,allowed,,,
mistral_small,Mistral,,,,3.98e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.25e+25 (Medium); coding_score: 4.37e+25 (Medium); aai_score: 4.15e+25 (Medium); mmlu_pro_score: 3.17e+25 (Medium) → weighted average: 3.98e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.520226,allowed,,,
claude_3_5_sonnet,Anthropic,,,,3.84e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,"Benchmark-based (lmarena_score): 1340.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.84e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T20:15:51.517947,allowed,,,
gpt_4o,OpenAI,,1760000000000.0,known_specification:gpt_4o,3.8e+25,medium,Reliable research estimate from industry analysis,epoch_estimate,Scaling Laws: 1.37e+26 (Medium); Benchmark Based: 4.00e+25 (Low),likely_above_1e25,https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from OpenAI patterns,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T20:15:51.521790,allowed,,,
llama_3.1_405b,Meta,,405000000000.0,known_specification:llama_3.1_405b,3.8e+25,high,Official disclosure or high-precision research estimate,epoch_estimate,Scaling Laws: 3.65e+25 (High),confirmed_above_1e25,https://epoch.ai/data-insights/models-over-1e25-flop: High-precision estimate from Meta disclosure,https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects),,2025-08-04T20:15:51.553422,allowed,,,
qwen_max,Alibaba,,,,3.79e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.02e+25 (Medium); superclue_math: 3.91e+25 (Medium); superclue_reasoning: 1.98e+25 (Low); superclue_code: 4.28e+25 (Medium); superclue_agents: 4.15e+25 (Medium) → weighted average: 3.79e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T20:15:51.526591,allowed,,,
gemini_advanced,Google,,,,3.77e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,"Benchmark-based (lmarena_score): 1332.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.77e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T20:15:51.523388,allowed,,,
grok_2_08_13,xAI,,,,3.71e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.98e+25 (Medium); coding_score: 3.79e+25 (Medium); aai_score: 3.56e+25 (Medium); mmlu_pro_score: 3.51e+25 (Medium) → weighted average: 3.71e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.556731,allowed,,,
grok_2_mini_08_13,xAI,,,,3.7e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.77e+25 (Medium); coding_score: 3.63e+25 (Medium) → weighted average: 3.70e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.557359,allowed,,,
gemini_1.5_pro_001,Google,,,,3.67e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,"Benchmark-based (lmarena_score): 1320.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.67e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T20:15:51.525788,allowed,,,
amazon_nova_experimental_chat_05_14,Amazon,,,,3.66e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,"Benchmark-based (lmarena_score): 1318.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.66e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T20:15:51.525954,allowed,,,
claude_3_5_haiku,Anthropic,,,,3.65e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,"Benchmark-based (lmarena_score): 1317.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.65e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T20:15:51.527188,allowed,,,
gpt_4.1_nano,OpenAI,,,,3.63e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.81e+25 (Medium); coding_score: 3.92e+25 (Medium); aai_score: 3.89e+25 (Medium); mmlu_pro_score: 2.90e+25 (Medium) → weighted average: 3.63e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.525066,allowed,,,
qwen2.5_plus,Alibaba,,,,3.63e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,"Benchmark-based (lmarena_score): 1315.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.63e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T20:15:51.527004,allowed,,,
claude_3.5_sonnet,Anthropic,,,,3.6e+25,medium,,company_disclosure,,likely_above_1e25,Epoch AI: Low-precision estimate from benchmarks,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),y,2025-07-31T18:56:49.506343,,,,not_sure
grok_2_mini,xAI,,,,3.57e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,"Benchmark-based (lmarena_score): 1307.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.57e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T20:15:51.529792,allowed,,,
gpt_4_preview,OpenAI,,,,3.55e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.63e+25 (Medium); coding_score: 3.48e+25 (Medium) → weighted average: 3.55e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.527704,allowed,,,
gpt_4_turbo,OpenAI,,,,3.54e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.70e+25 (Medium); coding_score: 3.64e+25 (Medium); aai_score: 3.49e+25 (Medium); mmlu_pro_score: 3.33e+25 (Medium) → weighted average: 3.54e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.525315,allowed,,,
gemini_1.5_flash_002,Google,,,,3.52e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.84e+25 (Medium); coding_score: 3.58e+25 (Medium); aai_score: 3.52e+25 (Medium); mmlu_pro_score: 3.16e+25 (Medium) → weighted average: 3.52e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.528984,allowed,,,
mistral_large,Mistral,,,,3.52e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.65e+25 (Medium); coding_score: 3.67e+25 (Medium); aai_score: 3.40e+25 (Medium); mmlu_pro_score: 3.36e+25 (Medium) → weighted average: 3.52e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.529243,allowed,,,
qwen_plus,Alibaba,,,,3.4599999999999995e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.42e+25 (Medium); coding_score: 3.49e+25 (Medium) → weighted average: 3.46e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.520614,allowed,,,
deepseek_v2_api,DeepSeek,,,,3.44e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.41e+25 (Medium); coding_score: 3.47e+25 (Medium) → weighted average: 3.44e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.557854,allowed,,,
amazon_nova_pro,Amazon,,,,3.43e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.59e+25 (Medium); coding_score: 3.65e+25 (Medium); aai_score: 3.19e+25 (Medium); mmlu_pro_score: 3.29e+25 (Medium) → weighted average: 3.43e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.531366,allowed,,,
gemini_1.5_flash_001,Google,,,,3.38e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,"Benchmark-based (lmarena_score): 1284.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.38e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T20:15:51.532764,allowed,,,
pixtral_large,Mistral,,,,3.33e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: aai_score: 3.24e+25 (Medium); mmlu_pro_score: 3.41e+25 (Medium) → weighted average: 3.33e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.559918,allowed,,,
gpt_4o_mini,OpenAI,,,,3.14e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.20e+25 (Medium); superclue_math: 2.52e+25 (Low); superclue_reasoning: 1.32e+25 (Low); superclue_code: 3.27e+25 (Medium); superclue_agents: 4.59e+25 (Medium) → weighted average: 3.14e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T20:15:51.527511,allowed,,,
gemini_pro_dev_api,Google,,,,3e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1234.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 3.00e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T20:15:51.537938,allowed,,,
grok_2,xAI,,,,3e+25,high,Official disclosure or high-precision research estimate,epoch_estimate,Benchmark Based: 3.77e+25 (Medium),confirmed_above_1e25,https://epoch.ai/data-insights/models-over-1e25-flop: High-precision estimate from xAI disclosure,CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T20:15:51.523556,allowed,,,
claude_3.5_haiku,Anthropic,,,,2.97e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.84e+25 (Medium); superclue_math: 1.88e+25 (Low); superclue_reasoning: 8.52e+24 (Low); superclue_code: 3.15e+25 (Medium); superclue_agents: 5.07e+25 (Medium) → weighted average: 2.97e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T20:15:51.557637,allowed,,,
deepseek_v2.5,DeepSeek,,,,2.94e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.05e+25 (Medium); superclue_math: 3.12e+25 (Medium); superclue_reasoning: 1.20e+25 (Low); superclue_code: 2.95e+25 (Low); superclue_agents: 3.81e+25 (Medium) → weighted average: 2.94e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T20:15:51.524834,allowed,,,
gemini_pro,Google,,,,2.91e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1222.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 2.91e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T20:15:51.538803,allowed,,,
amazon_nova_lite,Amazon,,,,2.86e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.35e+25 (Medium); coding_score: 3.41e+25 (Medium); aai_score: 2.45e+25 (Medium); mmlu_pro_score: 2.22e+25 (Medium) → weighted average: 2.86e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.536260,allowed,,,
claude_1,Anthropic,,,,2.82e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.92e+25 (Low); coding_score: 2.71e+25 (Low) → weighted average: 2.82e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.558576,allowed,,,
marco_o1,OpenAI,,,,2.72e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.68e+25 (Low); superclue_math: 3.20e+25 (Medium); superclue_reasoning: 1.27e+25 (Low); superclue_code: 1.57e+25 (Low); superclue_agents: 4.00e+25 (Medium) → weighted average: 2.72e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T20:15:51.566550,allowed,,,
gemini_1.0_pro_001,Google,,,,2.61e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.75e+25 (Low); coding_score: 2.47e+25 (Low) → weighted average: 2.61e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.558889,allowed,,,
claude_3_sonnet,Anthropic,,,,2.6e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.27e+25 (Medium); coding_score: 3.24e+25 (Medium); aai_score: 1.79e+25 (Medium); mmlu_pro_score: 2.11e+25 (Medium) → weighted average: 2.60e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.533107,allowed,,,
gpt_3.5_turbo,OpenAI,,,,2.59e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.63e+25 (Low); coding_score: 2.54e+25 (Low) → weighted average: 2.59e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.538986,allowed,,,
claude_instant_1,Anthropic,,,,2.58e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.61e+25 (Low); coding_score: 2.54e+25 (Low) → weighted average: 2.58e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.559084,allowed,,,
amazon_nova_micro,Amazon,,,,2.57e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.20e+25 (Medium); coding_score: 3.21e+25 (Medium); aai_score: 1.86e+25 (Medium); mmlu_pro_score: 1.70e+25 (Low) → weighted average: 2.57e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.536964,allowed,,,
palm_2,Google,,,,2.37e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,"Benchmark-based (lmarena_score): 1141.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with α=3.0 = 2.37e+25 FLOP",CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T20:15:51.547796,allowed,,,
claude_3_haiku,Anthropic,,,,2.31e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.09e+25 (Medium); coding_score: 3.06e+25 (Medium); aai_score: 1.35e+25 (Medium); mmlu_pro_score: 1.47e+25 (Low) → weighted average: 2.31e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.535886,allowed,,,
gemini_1.5_pro,Google,,300000000000.0,known_specification:gemini_1.5_pro,2.16e+25,medium,Known parameters with estimated training tokens,scaling_laws,,likely_above_1e25,"Known model specification 'gemini_1.5_pro': Chinchilla scaling law: 6 × 300,000,000,000 params × 12,000,000,000,000 tokens = 2.16e+25 FLOP",https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects),,2025-08-04T20:15:51.553187,allowed,,,
gpt_4,OpenAI,,1760000000000.0,known_specification:gpt_4,2.1e+25,medium,Reliable research estimate from industry analysis,epoch_estimate,Scaling Laws: 1.37e+26 (Medium); Benchmark Based: 3.33e+25 (Medium),likely_above_1e25,https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from scaling analysis,CSV file with manually collected leaderboard data (LMArena Manual Data Collection),,2025-08-04T20:15:51.532486,allowed,,,
mistral_medium,Mistral,,,,1.97e+25,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.87e+25 (Low); coding_score: 2.79e+25 (Low); aai_score: 1.20e+25 (Medium); mmlu_pro_score: 1.40e+25 (Low) → weighted average: 1.97e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.516904,allowed,,,
claude_3_opus,Anthropic,,175000000000.0,known_specification:claude_3_opus,1.6e+25,medium,Reliable research estimate from industry analysis,epoch_estimate,Scaling Laws: 8.40e+24 (Medium); Benchmark Based: 3.34e+25 (Medium),likely_above_1e25,https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from industry analysis,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.525610,allowed,,,
llama_3.1_405b_instruct_bf16,Meta,,405000000000.0,extracted_from_name,1.4799999999999998e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 3.80e+25 (Medium),likely_above_1e25,"Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 × 405,000,000,000 params × 6,075,000,000,000 tokens = 1.48e+25 FLOP (Modern large model: 405B params * 15 tokens/param)",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.522869,allowed,,,
llama_3.1_405b_instruct_fp8,Meta,,405000000000.0,extracted_from_name,1.4799999999999998e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 3.78e+25 (Medium),likely_above_1e25,"Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 × 405,000,000,000 params × 6,075,000,000,000 tokens = 1.48e+25 FLOP (Modern large model: 405B params * 15 tokens/param)",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.523199,allowed,,,
qwen3_235b,Alibaba,,235000000000.0,known_specification:qwen3_235b,1.41e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 5.57e+25 (Medium),likely_above_1e25,"Known model specification 'qwen3_235b': Chinchilla scaling law: 6 × 235,000,000,000 params × 10,000,000,000,000 tokens = 1.41e+25 FLOP",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.517544,allowed,,,
runway_gen_3,Runway,,,,1.0999999999999998e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Threshold-based (physics_iq_score): 22.8 > sora (10.0) → assumed >1e25 FLOP (reference model assumed frontier-level),https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models),,2025-08-04T20:15:51.561246,allowed,,,
stable_video_diffusion,Stability AI,,,,1.0999999999999998e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Threshold-based (physics_iq_score): 14.8 > sora (10.0) → assumed >1e25 FLOP (reference model assumed frontier-level),https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models),,2025-08-04T20:15:51.561396,allowed,,,
veo_2,Google,,,,1.0999999999999998e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 1.10e+25 (Medium); video_quality: 1.10e+25 (Medium) → weighted average: 1.10e+25 FLOP,https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings),,2025-08-04T20:15:51.567195,allowed,,,
veo_3_preview,Google,,,,1.0999999999999998e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 1.10e+25 (Medium); video_quality: 1.10e+25 (Medium) → weighted average: 1.10e+25 FLOP,https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings),,2025-08-04T20:15:51.567021,allowed,,,
nemotron_4_340b,Nvidia,,340000000000.0,extracted_from_name,1.04e+25,low,Extracted parameters with uncertain training tokens,scaling_laws,Benchmark Based: 3.23e+25 (Medium),uncertain,"Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 × 340,000,000,000 params × 5,100,000,000,000 tokens = 1.04e+25 FLOP (Generic estimate: 340B params * 15 tokens/param)",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:51.533920,allowed,,,
doubao_pro_256k,ByteDance,,,,9.899999999999999e+24,low,Distant benchmark interpolation or single source,benchmark_based,Benchmark Based: 4.27e+25 (Medium),uncertain,FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Limited disclosure on training methodology and data sources for frontier models. Original estimate: 4.27e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.16e+25 (Medium); superclue_math: 4.12e+25 (Medium); superclue_reasoning: 4.01e+25 (Medium); superclue_code: 4.81e+25 (Medium); superclue_agents: 4.27e+25 (Medium) → weighted average: 4.27e+25 FLOP),https://www.supercluebench.com/leaderboard (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-04T16:03:15.514626,capped,4.27e+25,,not_sure
sora_i2v,Unknown,,,,9.899999999999999e+24,low,Distant benchmark interpolation or single source,benchmark_based,,uncertain,Threshold-based (physics_iq_score): 10.0 < sora (10.0) → scaled estimate 9.90e+24 FLOP (α=2.0),https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models),,2025-08-04T16:03:15.513763,allowed,,,not_sure
doubao_lite_32k,ByteDance,,,,,speculative,Speculative confidence from manual research method,manual_research,,uncertain,,https://github.com/openai/open-agent-leaderboard (Open Agent Leaderboard - Multi-task agent performance evaluation),,2025-08-04T14:10:04.589866,allowed,,,not_sure
doubao_lite_4k,ByteDance,,,,,speculative,Speculative confidence from manual research method,manual_research,,uncertain,,https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects),,2025-08-04T14:10:03.915468,allowed,,,not_sure
doubao_pro_128k,ByteDance,,,,,speculative,Speculative confidence from manual research method,manual_research,,uncertain,,https://github.com/openai/open-agent-leaderboard (Open Agent Leaderboard - Multi-task agent performance evaluation),,2025-08-04T14:10:04.589912,allowed,,,not_sure
doubao_pro_32k,ByteDance,,,,,speculative,Speculative confidence from manual research method,manual_research,,uncertain,,https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects),,2025-08-04T14:10:03.915417,allowed,,,not_sure
gemini_1.0_pro,Google,,,,,speculative,Speculative confidence from manual research method,manual_research,,uncertain,,https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects),,2025-08-04T20:15:44.642443,allowed,,,
hunyuan_standard_vision,Tencent,,,,,speculative,Speculative confidence from manual research method,manual_research,,uncertain,,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:45.210423,allowed,,,
pangu_ultra,Unknown,,,,,speculative,Speculative confidence from manual research method,manual_research,,uncertain,,https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects),,2025-08-04T14:10:03.915579,allowed,,,not_sure
qwen_vl_max,Alibaba,,,,,speculative,Speculative confidence from manual research method,manual_research,,uncertain,,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:45.210216,allowed,,,
step_1o_vision_32k,StepFun,,,,,speculative,Speculative confidence from manual research method,manual_research,,uncertain,,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:45.210126,allowed,,,
step_1v_32k,StepFun,,,,,speculative,Speculative confidence from manual research method,manual_research,,uncertain,,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:45.210270,allowed,,,
yi_large_turbo,01 AI,,,,,speculative,Speculative confidence from manual research method,manual_research,,uncertain,,https://llmbench.github.io/ChineseSimpleQA/ (ChineseSimpleQA - Question answering benchmark for Chinese LLMs),,2025-08-04T14:10:03.623272,allowed,,,not_sure
yi_vision,01 AI,,,,,speculative,Speculative confidence from manual research method,manual_research,,uncertain,,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:15:45.210498,allowed,,,
