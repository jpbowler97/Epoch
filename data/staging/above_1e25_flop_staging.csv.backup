model,developer,release_date,parameters,parameter_source,training_flop,confidence,confidence_explanation,estimation_method,alternative_methods,threshold_classification,status,reasoning,sources,verified,last_updated,notes,blacklist_status,original_estimate
claude_opus_4,Anthropic,,175000000000.0,known_specification:claude_opus,1.5000000000000002e+26,low,Speculative research estimate,epoch_estimate,Scaling Laws: 8.40e+24 (Medium); Benchmark Based: 4.67e+25 (Medium),high_confidence_above_1e25,uncertain,https://epoch.ai/data-insights/models-over-1e25-flop: Speculative estimate for next-gen Claude,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-01T19:50:21.102414,,,
gpt_4.5_preview,OpenAI,,1760000000000.0,known_specification:gpt_4,6.4e+25,low,Speculative research estimate,epoch_estimate,Scaling Laws: 1.37e+26 (Medium); Benchmark Based: 5.01e+25 (Medium),high_confidence_above_1e25,uncertain,https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate for GPT-4.5,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-01T19:50:21.101799,,,
gemini_2.0_flash,Google,,,,6.000000000000001e+25,low,,scaling_laws,,high_confidence_above_1e25,uncertain,Benchmark-based estimation - official parameters not disclosed,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),y,2025-07-31T18:56:49.505562,,,
gemini_2.0_flash_001,Google,,,,5.999999999999999e+25,low,Extracted parameters with uncertain training tokens,scaling_laws,Benchmark Based: 4.07e+25 (Medium),high_confidence_above_1e25,uncertain,Benchmark-based estimation - official parameters not disclosed,LMArena Manual Data Collection (CSV file with manually collected leaderboard data),,2025-08-01T19:50:21.104722,,,
gemini_2.0_pro,Google,,,,5.999999999999999e+25,low,Extracted parameters with uncertain training tokens,scaling_laws,Benchmark Based: 4.79e+25 (Medium),high_confidence_above_1e25,uncertain,Benchmark-based estimation - official parameters not disclosed,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-01T19:50:21.125808,,,
gemini_2.5_flash,Google,,,,5.999999999999999e+25,low,Extracted parameters with uncertain training tokens,scaling_laws,Benchmark Based: 5.01e+25 (Medium),high_confidence_above_1e25,uncertain,Benchmark-based estimation - official parameters not disclosed,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-01T19:50:21.102659,,,
gemini_2.5_pro,Google,,,,5.999999999999999e+25,low,Extracted parameters with uncertain training tokens,scaling_laws,Benchmark Based: 5.61e+25 (Medium),high_confidence_above_1e25,uncertain,Benchmark-based estimation - official parameters not disclosed,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-01T19:50:21.101269,,,
deepseek_v2_api,DeepSeek,,671000000000.0,known_specification:deepseek,5.959999999999999e+25,medium,Known parameters with estimated training tokens,scaling_laws,Benchmark Based: 3.44e+25 (Medium),high_confidence_above_1e25,likely_above_1e25,"Known model specification 'deepseek': Chinchilla scaling law: 6 × 671,000,000,000 params × 14,800,000,000,000 tokens = 5.96e+25 FLOP",https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-01T19:50:21.130737,,,
grok_4,xAI,,,,5.24e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,high_confidence_above_1e25,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 5.28e+25 (Low); coding_score: 5.20e+25 (Low) → weighted average: 5.24e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-01T19:50:21.101924,,,
o3,OpenAI,,,,5.22e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,high_confidence_above_1e25,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 5.20e+25 (Low); coding_score: 5.23e+25 (Low) → weighted average: 5.22e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-01T19:50:21.101479,,,
grok_3,xAI,,,,5.16e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,high_confidence_above_1e25,likely_above_1e25,Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 5.16e+25 (Low); coding_score: 5.17e+25 (Low) → weighted average: 5.16e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-01T19:50:21.102531,,,
o1,OpenAI,,,,5.559999999999999e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,,likely_above_1e25,Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.55e+25 (Medium); coding_score: 4.54e+25 (Medium); aai_score: 8.88e+25 (Low); mmlu_pro_score: 5.38e+25 (Medium) → weighted average: 5.56e+25 FLOP,https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard),,2025-08-04T20:23:58.108270,Added to staging by manual review. Reason: Known large reasoning model,allowed,
gpt_5,OpenAI,,,,5.47e+25,medium,Good benchmark match with multiple agreeing sources,benchmark_based,,,likely_above_1e25,Multi-benchmark estimation from 5 benchmarks: superclue_overall: 5.94e+25 (Medium); superclue_math: 5.70e+25 (Medium); superclue_reasoning: 2.77e+25 (Medium); superclue_code: 6.24e+25 (Medium); superclue_agents: 6.69e+25 (Medium) → weighted average: 5.47e+25 FLOP,https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark),,2025-08-11T21:33:30.575986,Added to staging by manual review. Reason: I just know GPT5 must be big!,allowed,
gemini_1.5_pro,Google DeepMind,2/15/2024,,,1.58e+25,Speculative,,Training compute imputed from benchmark scores.,,manual_entry,manual_import,,https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf,,2025-08-15T18:31:59.381865+00:00,API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models. Added via automation pipeline from published dataset on 2025-08-15 18:31 UTC,,
gpt_4,OpenAI,3/15/2023,1800000000000.0,,2.1e+25,Likely,,"90% CI: 8.2E+24 to 4.4E+25

NOTE: this is a rough estimate based on public information, much less information than most other systems in the database.

Calculation and confidence intervals here: https://colab.research.google.com/drive/1O99z9b1I5O66bT78r9ScslE_nOj5irN9?usp=sharing",,manual_entry,manual_import,,https://arxiv.org/abs/2303.08774,,2025-08-15T18:32:14.715753+00:00,Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC,,
inflection_2,Inflection AI,11/22/2023,,,1e+25,Confident,,"""Inflection-2 was trained on 5,000 NVIDIA H100 GPUs in fp8 mixed precision for ~10²⁵ FLOPs""

(the second 1 is there because of airtable being wonky, it's not a real sig fig)",,manual_entry,manual_import,,https://inflection.ai/inflection-2,,2025-08-15T18:32:19.278135+00:00,"via Pi, no API. Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC",,
llama_3.1_405b,Meta AI,7/23/2024,405000000000.0,,3.8e+25,Confident,,"Stated in paper.

Also, 6 * 405B * 15.6T training tokens = 3.8e25",,manual_entry,manual_import,,https://ai.meta.com/research/publications/the-llama-3-herd-of-models/,,2025-08-15T18:32:21.034942+00:00,"Llama 3.1 model license:

https://huggingface.co/meta-llama/Meta-Llama-3.1-8B/blob/main/LICENSE 

must seek separate license if over 700m monthly users, acceptable use restrictions

training code here: https://github.com/meta-llama/llama-recipes/blob/main/src/llama_recipes/utils/train_utils.py#L70 
. Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC",,
pangu_ultra,Huawei,4/10/2025,135000000000.0,,1.07e+25,Confident,,"When compared to Llama 3.1 405B, Pangu Ultra achieves better scores on most of the challenging benchmarks, while utilizing only about 29% of the training FLOPs required by Llama 405B.

Compute = 6 FLOP/token/param *  135e9 params *13.2e12 tokens = 1.069200e+25 FLOP
This is consistent with 29% of Llama 405B's compute: 3.8e25*0.29=1.1e25.",,manual_entry,manual_import,,https://arxiv.org/abs/2504.07866,,2025-08-15T18:32:26.490589+00:00,"""Our model and system will be available for our commercial customers"". Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC",,
grok_2,xAI,8/13/2024,,,2.9599999999999996e+25,Confident,,Estimate based on xAI statements comparing Grok-2 compute to GPT-4 and Grok-3. Full estimate here: https://docs.google.com/document/d/1C_dABuZrAqYE_ui4_GZ4bRLtq3TBjIGoBSktaPElhEU/edit?usp=sharing,,manual_entry,manual_import,,https://x.ai/blog/grok-2,,2025-08-15T18:32:27.897961+00:00,Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC,,
aramco_metabrain_ai,Saudi Aramco,3/4/2024,250000000000.0,,1.05e+25,Likely,,6*250B*7T=1.05e+25,,manual_entry,manual_import,,https://www.offshore-technology.com/news/saudi-aramco-unveils-industry-first-generative-ai-model/,,2025-08-15T18:32:28.778585+00:00,Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC,,
gemini_1.0_ultra,Google DeepMind,12/6/2023,,,4.999999999999999e+25,Speculative,,"This number is an estimate based on limited evidence. In particular, we combine information about the performance of Gemini Ultra on various benchmarks compared to other models, and guesstimates about the hardware setup used for training to arrive at our estimate. Our reasoning and calculations are detailed in this Colab notebook.
https://colab.research.google.com/drive/1sfG91UfiYpEYnj_xB5YRy07T5dv-9O_c",,manual_entry,manual_import,,https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf,,2025-08-15T18:32:29.546471+00:00,API access: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models. Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC,,
claude_3_opus,Anthropic,3/4/2024,,,1.6399999999999998e+25,Speculative,,Training compute estimated from benchmark scores.,,manual_entry,manual_import,,https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf,,2025-08-15T18:32:30.346447+00:00,Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC,,
doubao_pro,ByteDance,10/28/2024,500000000000.0,,2.5099999999999997e+25,Speculative,,6ND = 6 * 500*10^9 * 8350*10^9 = 2.505e+25,,manual_entry,manual_import,,https://www.volcengine.com/docs/6360/1264663,,2025-08-15T18:32:31.162797+00:00,Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC,,
gpt_4o,OpenAI,5/13/2024,,,3.8099999999999996e+25,Speculative,,Training compute estimated from benchmark scores.,,manual_entry,manual_import,,"https://openai.com/index/hello-gpt-4o/ 
https://openai.com/index/gpt-4o-system-card/",,2025-08-15T18:32:31.947504+00:00,Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC,,
gpt_4_turbo,OpenAI,11/6/2023,,,2.2e+25,Unknown,,Estimated using benchmark imputation,,manual_entry,manual_import,,https://openai.com/blog/new-models-and-developer-products-announced-at-devday,,2025-08-15T18:32:32.635366+00:00,Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC,,
llama_4_behemoth_preview,Meta AI,4/5/2025,2000000000000.0,,5.18e+25,Likely,,"Behemoth's training dataset is at least 30T tokens:
https://ai.meta.com/blog/llama-4-multimodal-intelligence/ 

6 FLOP / parameter / token * 288 * 10^9 activated parameters * 30 * 10^12 tokens = 5.184e+25 FLOP",,manual_entry,manual_import,,https://ai.meta.com/blog/llama-4-multimodal-intelligence/,,2025-08-15T18:32:33.307648+00:00,"""While we’re not yet releasing Llama 4 Behemoth as it is still training"". Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC",,
glm_4_plus,Zhipu AI,8/29/2024,,,3.5999999999999997e+25,Unknown,,Estimated using benchmark imputation,,manual_entry,manual_import,,https://bigmodel.cn/dev/howuse/glm-4,,2025-08-15T18:32:34.002665+00:00,Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC,,
mistral_large_2,Mistral AI,7/24/2024,123000000000.0,,2.13e+25,Likely,,"Details are sparse, but we can hazard a guess based on evidence about the training cluster they may have used, the scale up in compute they likely would have used relative to Mistral Large 1, and from the model's MMLU score. Extended reasoning given here: https://docs.google.com/document/d/1I2ZWBLFMpRZYcdMMUfKAGZFJrOJpduNDS9ZeVFIHnd8/edit?usp=sharing",,manual_entry,manual_import,,https://mistral.ai/news/mistral-large-2407/,,2025-08-15T18:32:34.634831+00:00,"""We are releasing Mistral Large 2 under the Mistral Research License, that allows usage and modification for research and non-commercial usages. For commercial usage of Mistral Large 2 requiring self-deployment, a Mistral Commercial License must be acquired by contacting us."". Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC",,
claude_3_7_sonnet,Anthropic,2/24/2025,,,3.3499999999999998e+25,Likely,,https://docs.google.com/spreadsheets/d/10bhwdVrfHI8tysVIz62ZxtvQ30L-HojYvmU18_b-WIM/edit?gid=0#gid=0,,manual_entry,manual_import,,https://www.anthropic.com/news/claude-3-7-sonnet,,2025-08-15T18:32:35.275060+00:00,Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC,,
claude_3_5_sonnet,Anthropic,6/20/2024,,,2.7e+25,Speculative,,"Blog post by Dario Amodei includes some info on 3.5 Sonnet compute: https://darioamodei.com/on-deepseek-and-export-controls
""Claude 3.5 Sonnet is a mid-sized model that cost a few $10M's to train (I won't give an exact number). Also, 3.5 Sonnet was not trained in any way that involved a larger or more expensive model (contrary to some rumors).""

Using assumptions about GPU pricing, this lets us estimate compute. https://docs.google.com/spreadsheets/d/1-p-ab6t6dkUM6T7GwnFp85ePTMpZMW7LFY7fW2t8POs/",,manual_entry,manual_import,,https://www-cdn.anthropic.com/fed9cc193a14b84131812372d8d5857f8f304c52/Model_Card_Claude_3_Addendum.pdf,,2025-08-15T18:32:35.930524+00:00,Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC,,
nemotron_4_340b,NVIDIA,6/14/2024,340000000000.0,,1.7999999999999999e+25,Confident,,"9 trillion tokens for training
6 * 340B * 9T = 1.8E25

alternatively, can do a hardware estimate with a few extra steps:

According to the technical report, Nemotron-4 340B was trained using up to 6144 H100 GPUs. Helpfully, they also report the model FLOP utilization (MFU), which was 41-42% (Table 2). This is the ratio of the actual output of their GPUs, in FLOP used for training, relative to their theoretical max of 989 teraFLOP/s per GPU. 
Unfortunately, the report omits the last ingredient, which is the duration of the training run. However, in Table 2 they report some relevant data that we can use to infer the training time. 
Nemotron-4 was trained in several stages, but the largest stage used all 6144 GPUs with a batch size of 2304 and an iteration time (time per batch) of 8.0 seconds. This stage involved 7.6T tokens, so it makes up the majority of training. 
A batch size of 2304 means that each batch consists of 2304 sequences, and they report that the sequence length used for training was 4096 tokens. This means that each batch contained 4096 * 2304 = 9,437,184 tokens. 
So, during this stage, it took 8 seconds to train the model on 9.4m tokens. Extrapolating to the entire 9T token dataset, this implies the training run would have taken 7,659,574 seconds, or 89 days. (it actually took longer because they didn't use all their GPUs for the whole run) 
Multiplying 7,659,574 seconds by 41% MFU, 989 peak teraFLOP/s for each H100, and 6144 H100s, we get ~1.9e25 FLOP. This is very close to our first estimate. 
",,manual_entry,manual_import,,https://blogs.nvidia.com/blog/nemotron-4-synthetic-data-generation-llm-training/ ,,2025-08-15T18:32:36.575276+00:00,Permissive commercial license: https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf . Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC,,
llama_nemotron_ultra_253b,NVIDIA,3/18/2025,253000000000.0,,3.91e+25,Likely,,"Total training compute: 3.8e+25 FLOP (base model) + 1.11e+24 FLOP (fine-tuning) = 3.9e25 FLOP
See calculation in the finetune compute notes.",,manual_entry,manual_import,,https://arxiv.org/abs/2505.00949,,2025-08-15T18:32:37.258355+00:00,"GOVERNING TERMS: Your use of this model is governed by the NVIDIA Open Model License. Additional Information: Llama 3.3 Community License Agreement. Built with Llama.

https://huggingface.co/nvidia/Llama-3_1-Nemotron-Ultra-253B-v1. Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC",,
mistral_large,Mistral AI,2/26/2024,,,1.1199999999999999e+25,Likely,,"https://www.wsj.com/tech/ai/the-9-month-old-ai-startup-challenging-silicon-valleys-giants-ee2e4c48

Mistral spent <20 million euro (meaning approximately 20 million?) to train Mistral Large:

https://x.com/EMostaque/status/1762152740938031484?s=20
""assuming this is on H100s with @Scaleway who are €1.9/hour => 10m H100 hours (c 30m A100 hrs), 3 months at 4k H100s :timer_clock:"" -Emad Mostaque

Assuming bf16 or fp16, H100 SXM performance is 989 TFLOPS
At 1.9 euro per H100-hour and 30% utilization, spending 20M euro produces 1.12*10^25 FLOP.
https://www.wolframalpha.com/input?i=20+million+%2F+%281.9%2Fhour%29+*+989+TFLOPS+*+0.30 ",,manual_entry,manual_import,,https://mistral.ai/news/mistral-large/,,2025-08-15T18:32:37.963119+00:00,Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC,,
glm_4_0116,Zhipu AI,1/17/2024,,,1.2e+25,Likely,,"'- 0116 has slightly worse performance than 0520
- “the GLM-4 models are pre-trained on ten trillions of tokens”
- I did not find any information about parameters or compute. Over here they speculatively estimate GLM-4 to be 200B parameters (which seems plausible to me), though no source provided. 
- “GLM-4 gets close to the state-of-the-art models (GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus)”  none of these models has parameters disclosed or compute estimation.

6 FLOP / token / parameter * 10000000000000 tokens * 200000000000 parameters = 1.2e+25 FLOP with “Likely” confidence (+/- 1 OOM)",,manual_entry,manual_import,,"https://arxiv.org/abs/2406.12793
https://zhipuai.cn/en/devday",,2025-08-15T18:32:38.596521+00:00,"GLM-4 (0116) has been made available through the GLM-4 API at
https://bigmodel.cn. Added via automation pipeline from published dataset on 2025-08-15 18:32 UTC",,
