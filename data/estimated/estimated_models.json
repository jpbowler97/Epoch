{
  "metadata": {
    "saved_at": "2025-08-04T20:15:51.567895",
    "source": "consolidated_estimated",
    "last_updated": "2025-08-04T20:15:51.567855",
    "model_count": 273,
    "stage": "estimated"
  },
  "models": [
    {
      "name": "gemini_2.5_pro",
      "developer": "Google",
      "release_date": null,
      "parameters": 800000000000,
      "parameter_source": "known_specification:gemini_2.5_pro",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.20e+26",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "5.02e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.76e+25 (Medium); superclue_math: 5.20e+25 (Medium); superclue_reasoning: 2.57e+25 (Low); superclue_code: 5.30e+25 (Medium); superclue_agents: 6.47e+25 (Medium) \u2192 weighted average: 5.02e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 68.98,
        "superclue_math": 74.05,
        "superclue_reasoning": 52.59,
        "superclue_code": 82.38,
        "superclue_agents": 82.09
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Known model specification 'gemini_2.5_pro': Chinchilla scaling law: 6 \u00d7 800,000,000,000 params \u00d7 25,000,000,000,000 tokens = 1.20e+26 FLOP",
      "last_updated": "2025-08-04T20:15:51.513454",
      "metadata": {}
    },
    {
      "name": "o3",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.33e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "superclue_overall": 73.78,
        "superclue_math": 81.6,
        "superclue_reasoning": 59.7,
        "superclue_code": 83.76,
        "superclue_agents": 76.12
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 5.64e+25 (Medium); superclue_math: 6.62e+25 (Medium); superclue_reasoning: 3.52e+25 (Medium); superclue_code: 5.53e+25 (Medium); superclue_agents: 5.36e+25 (Medium) \u2192 weighted average: 5.33e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.513732",
      "metadata": {}
    },
    {
      "name": "chatgpt_4o",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.27e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1430.0,
        "coding_score": 1436.0,
        "vision_score": 1297.0,
        "aai_score": 50.3,
        "mmlu_pro_score": 80.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.22e+25 (Low); coding_score: 5.13e+25 (Low); aai_score: 5.86e+25 (Medium); mmlu_pro_score: 4.79e+25 (Medium) \u2192 weighted average: 5.27e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.513916",
      "metadata": {}
    },
    {
      "name": "gpt_4.5_preview",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "6.40e+25",
      "training_flop_confidence": "low",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "5.42e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.06e+25 (Low); coding_score: 4.95e+25 (Low); aai_score: 6.51e+25 (Medium); mmlu_pro_score: 4.89e+25 (Medium) \u2192 weighted average: 5.42e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1415.0,
        "coding_score": 1419.0,
        "vision_score": 1239.0,
        "aai_score": 53.0,
        "mmlu_pro_score": 81.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate for GPT-4.5",
      "last_updated": "2025-08-04T20:15:51.514090",
      "metadata": {}
    },
    {
      "name": "grok_4",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "7.01e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1435.0,
        "coding_score": 1441.0,
        "vision_score": 1272.0,
        "aai_score": 73.2,
        "mmlu_pro_score": 86.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.28e+25 (Low); coding_score: 5.19e+25 (Low); aai_score: 1.24e+26 (Low); mmlu_pro_score: 5.78e+25 (Medium) \u2192 weighted average: 7.01e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.514241",
      "metadata": {}
    },
    {
      "name": "claude_opus_4_thinking_16k",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.58e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1421.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1421.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 4.58e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.514359",
      "metadata": {}
    },
    {
      "name": "kimi_k2_preview",
      "developer": "Moonshot",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "5.65e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.71e+25 (Medium); coding_score: 4.82e+25 (Low); aai_score: 7.69e+25 (Medium); mmlu_pro_score: 5.11e+25 (Medium) \u2192 weighted average: 5.65e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1381.0,
        "coding_score": 1406.0,
        "aai_score": 57.6,
        "mmlu_pro_score": 82.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI startup with limited public disclosure on model training details - Minimal transparency on training compute, data sources, and methodology. Original estimate: 5.65e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.71e+25 (Medium); coding_score: 4.82e+25 (Low); aai_score: 7.69e+25 (Medium); mmlu_pro_score: 5.11e+25 (Medium) \u2192 weighted average: 5.65e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.514596",
      "metadata": {}
    },
    {
      "name": "deepseek_r1",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": 671000000000,
      "parameter_source": "known_specification:deepseek_r1",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.00e+24",
      "training_flop_confidence": "high",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "6.04e+25",
          "confidence": "low",
          "method": "scaling_laws",
          "reasoning": "Known model specification 'deepseek_r1': Chinchilla scaling law: 6 \u00d7 671,000,000,000 params \u00d7 15,000,000,000,000 tokens = 6.04e+25 FLOP"
        },
        {
          "flop": "5.76e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.62e+25 (Medium); coding_score: 4.58e+25 (Medium); aai_score: 8.40e+25 (Medium); mmlu_pro_score: 5.42e+25 (Medium) \u2192 weighted average: 5.76e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1373.0,
        "coding_score": 1382.0,
        "aai_score": 60.2,
        "mmlu_pro_score": 84.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1",
      "last_updated": "2025-08-04T20:15:51.514741",
      "metadata": {}
    },
    {
      "name": "claude_opus_4",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.50e+26",
      "training_flop_confidence": "low",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "5.77e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.55e+25 (Medium); coding_score: 4.81e+25 (Low); aai_score: 7.71e+25 (Medium); mmlu_pro_score: 5.69e+25 (Medium) \u2192 weighted average: 5.77e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1366.0,
        "coding_score": 1405.0,
        "vision_score": 1212.0,
        "aai_score": 57.7,
        "mmlu_pro_score": 86.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: Speculative estimate for next-gen Claude",
      "last_updated": "2025-08-04T20:15:51.514833",
      "metadata": {}
    },
    {
      "name": "grok_3",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.67e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1423.0,
        "coding_score": 1440.0,
        "aai_score": 56.1,
        "mmlu_pro_score": 79.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.15e+25 (Low); coding_score: 5.18e+25 (Low); aai_score: 7.29e+25 (Medium); mmlu_pro_score: 4.73e+25 (Medium) \u2192 weighted average: 5.67e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.514983",
      "metadata": {}
    },
    {
      "name": "gemini_2.5_flash",
      "developer": "Google",
      "release_date": null,
      "parameters": 400000000000,
      "parameter_source": "known_specification:gemini_2.5_flash",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.80e+25",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "6.15e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.03e+25 (Low); coding_score: 4.97e+25 (Low); aai_score: 9.82e+25 (Low); mmlu_pro_score: 5.23e+25 (Medium) \u2192 weighted average: 6.15e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1412.0,
        "coding_score": 1421.0,
        "vision_score": 1280.0,
        "aai_score": 65.1,
        "mmlu_pro_score": 83.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'gemini_2.5_flash': Chinchilla scaling law: 6 \u00d7 400,000,000,000 params \u00d7 20,000,000,000,000 tokens = 4.80e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.515166",
      "metadata": {}
    },
    {
      "name": "gpt_4.1",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.19e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1380.0,
        "coding_score": 1398.0,
        "vision_score": 1255.0,
        "aai_score": 52.9,
        "mmlu_pro_score": 80.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.70e+25 (Medium); coding_score: 4.74e+25 (Medium); aai_score: 6.48e+25 (Medium); mmlu_pro_score: 4.83e+25 (Medium) \u2192 weighted average: 5.19e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.515312",
      "metadata": {}
    },
    {
      "name": "claude_sonnet_4_thinking_32k",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.38e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1400.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1400.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 4.38e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.515417",
      "metadata": {}
    },
    {
      "name": "o1",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.56e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1366.0,
        "coding_score": 1378.0,
        "vision_score": 1216.0,
        "aai_score": 61.9,
        "mmlu_pro_score": 84.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.55e+25 (Medium); coding_score: 4.54e+25 (Medium); aai_score: 8.88e+25 (Low); mmlu_pro_score: 5.38e+25 (Medium) \u2192 weighted average: 5.56e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.515557",
      "metadata": {}
    },
    {
      "name": "qwen3_235b_a22b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 235000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.97e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "6.02e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.81e+25 (Low); coding_score: 4.88e+25 (Low); aai_score: 8.45e+25 (Medium); mmlu_pro_score: 5.17e+25 (Medium) \u2192 weighted average: 6.02e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1391.0,
        "coding_score": 1412.0,
        "aai_score": 60.4,
        "mmlu_pro_score": 82.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 235,000,000,000 params \u00d7 3,525,000,000,000 tokens = 4.97e+24 FLOP (Modern large model: 235B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.515733",
      "metadata": {}
    },
    {
      "name": "o4_mini",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "5.47e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 5.55e+25 (Medium); superclue_math: 5.61e+25 (Medium); superclue_reasoning: 2.94e+25 (Medium); superclue_code: 5.93e+25 (Medium); superclue_agents: 7.34e+25 (Medium) \u2192 weighted average: 5.47e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 73.32,
        "superclue_math": 76.34,
        "superclue_reasoning": 55.56,
        "superclue_code": 86.14,
        "superclue_agents": 86.36
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Models with unidentified developers cannot be properly evaluated - Cannot verify training methodology or model provenance. Original estimate: 5.47e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 5.55e+25 (Medium); superclue_math: 5.61e+25 (Medium); superclue_reasoning: 2.94e+25 (Medium); superclue_code: 5.93e+25 (Medium); superclue_agents: 7.34e+25 (Medium) \u2192 weighted average: 5.47e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.515908",
      "metadata": {}
    },
    {
      "name": "qwen3_coder_480b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 480000000000,
      "parameter_source": "known_specification:qwen3_coder_480b",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.32e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "5.37e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.47e+25 (Medium); coding_score: 4.80e+25 (Low); aai_score: 7.45e+25 (Medium); mmlu_pro_score: 4.57e+25 (Medium) \u2192 weighted average: 5.37e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1358.0,
        "coding_score": 1404.0,
        "aai_score": 56.7,
        "mmlu_pro_score": 78.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'qwen3_coder_480b': Chinchilla scaling law: 6 \u00d7 480,000,000,000 params \u00d7 15,000,000,000,000 tokens = 4.32e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.516021",
      "metadata": {}
    },
    {
      "name": "deepseek_v3",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": 671000000000,
      "parameter_source": "known_specification:deepseek_v3",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.96e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.25e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.35e+25 (Medium); superclue_math: 3.64e+25 (Medium); superclue_reasoning: 1.47e+25 (Low); superclue_code: 5.76e+25 (Medium); superclue_agents: 5.11e+25 (Medium) \u2192 weighted average: 4.25e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "superclue_overall": 66.54,
        "superclue_math": 64.22,
        "superclue_reasoning": 42.1,
        "superclue_code": 85.15,
        "superclue_agents": 74.7
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Known model specification 'deepseek_v3': Chinchilla scaling law: 6 \u00d7 671,000,000,000 params \u00d7 14,800,000,000,000 tokens = 5.96e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.516306",
      "metadata": {}
    },
    {
      "name": "claude_sonnet_4",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.18e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1338.0,
        "coding_score": 1386.0,
        "vision_score": 1212.0,
        "aai_score": 53.0,
        "mmlu_pro_score": 83.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.28e+25 (Medium); coding_score: 4.62e+25 (Medium); aai_score: 6.51e+25 (Medium); mmlu_pro_score: 5.31e+25 (Medium) \u2192 weighted average: 5.18e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.516536",
      "metadata": {}
    },
    {
      "name": "claude_3_7_sonnet_thinking_32k",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.26e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1387.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1387.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 4.26e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.516657",
      "metadata": {}
    },
    {
      "name": "o1_preview",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.24e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1385.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1385.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 4.24e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.516761",
      "metadata": {}
    },
    {
      "name": "mistral_medium",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.97e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1171.0,
        "coding_score": 1172.0,
        "aai_score": 22.8,
        "mmlu_pro_score": 49.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.87e+25 (Low); coding_score: 2.79e+25 (Low); aai_score: 1.20e+25 (Medium); mmlu_pro_score: 1.40e+25 (Low) \u2192 weighted average: 1.97e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.516904",
      "metadata": {}
    },
    {
      "name": "hunyuan_turbos",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "4.58e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 3 benchmarks: openlm_arena_elo: 4.67e+25 (Medium); coding_score: 4.64e+25 (Medium); mmlu_pro_score: 4.45e+25 (Medium) \u2192 weighted average: 4.58e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1377.0,
        "coding_score": 1388.0,
        "mmlu_pro_score": 78.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Major Chinese tech company but with limited transparency on AI model training specifics - Limited disclosure on training methodology and compute for frontier AI models. Original estimate: 4.58e+25 FLOP (Multi-benchmark estimation from 3 benchmarks: openlm_arena_elo: 4.67e+25 (Medium); coding_score: 4.64e+25 (Medium); mmlu_pro_score: 4.45e+25 (Medium) \u2192 weighted average: 4.58e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.517141",
      "metadata": {}
    },
    {
      "name": "minimax_m1",
      "developer": "MiniMax",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "5.45e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.44e+25 (Medium); coding_score: 4.45e+25 (Medium); aai_score: 9.20e+25 (Low); mmlu_pro_score: 4.99e+25 (Medium) \u2192 weighted average: 5.45e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1354.0,
        "coding_score": 1369.0,
        "aai_score": 63.0,
        "mmlu_pro_score": 81.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company with very limited transparency on model specifications - Insufficient disclosure on training methodology, data sources, and model architecture. Original estimate: 5.45e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.44e+25 (Medium); coding_score: 4.45e+25 (Medium); aai_score: 9.20e+25 (Low); mmlu_pro_score: 4.99e+25 (Medium) \u2192 weighted average: 5.45e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.517288",
      "metadata": {}
    },
    {
      "name": "gpt_4.1_mini",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.90e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1337.0,
        "coding_score": 1369.0,
        "vision_score": 1230.0,
        "aai_score": 52.6,
        "mmlu_pro_score": 78.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.27e+25 (Medium); coding_score: 4.45e+25 (Medium); aai_score: 6.41e+25 (Medium); mmlu_pro_score: 4.47e+25 (Medium) \u2192 weighted average: 4.90e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.517372",
      "metadata": {}
    },
    {
      "name": "qwen3_235b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 235000000000,
      "parameter_source": "known_specification:qwen3_235b",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.41e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "5.57e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.55e+25 (Medium); coding_score: 4.70e+25 (Medium); aai_score: 8.99e+25 (Low); mmlu_pro_score: 5.17e+25 (Medium) \u2192 weighted average: 5.57e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1366.0,
        "coding_score": 1394.0,
        "aai_score": 62.3,
        "mmlu_pro_score": 82.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'qwen3_235b': Chinchilla scaling law: 6 \u00d7 235,000,000,000 params \u00d7 10,000,000,000,000 tokens = 1.41e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.517544",
      "metadata": {}
    },
    {
      "name": "qwen2.5_max",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.50e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1367.0,
        "coding_score": 1373.0,
        "aai_score": 45.3,
        "mmlu_pro_score": 76.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.56e+25 (Medium); coding_score: 4.49e+25 (Medium); aai_score: 4.75e+25 (Medium); mmlu_pro_score: 4.20e+25 (Medium) \u2192 weighted average: 4.50e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.517726",
      "metadata": {}
    },
    {
      "name": "claude_3_7_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.12e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1372.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1372.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 4.12e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.517815",
      "metadata": {}
    },
    {
      "name": "claude_3_5_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.84e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1340.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1340.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.84e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.517947",
      "metadata": {}
    },
    {
      "name": "gemini_2.0_flash_001",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.07e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1366.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1366.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 4.07e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.518094",
      "metadata": {}
    },
    {
      "name": "o3_mini",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.23e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1318.0,
        "coding_score": 1360.0,
        "aai_score": 62.9,
        "mmlu_pro_score": 79.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.09e+25 (Medium); coding_score: 4.36e+25 (Medium); aai_score: 9.17e+25 (Low); mmlu_pro_score: 4.61e+25 (Medium) \u2192 weighted average: 5.23e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.518462",
      "metadata": {}
    },
    {
      "name": "gemma_3_27b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 27000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.75e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.76e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.46e+25 (Medium); coding_score: 4.25e+25 (Medium); aai_score: 3.28e+25 (Medium); mmlu_pro_score: 3.03e+25 (Medium) \u2192 weighted average: 3.76e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1357.0,
        "coding_score": 1348.0,
        "vision_score": 1214.0,
        "aai_score": 37.6,
        "mmlu_pro_score": 66.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 27,000,000,000 params \u00d7 540,000,000,000 tokens = 8.75e+22 FLOP (Modern model: 27B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.519022",
      "metadata": {}
    },
    {
      "name": "grok_3_mini",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.75e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1361.0,
        "coding_score": 1377.0,
        "aai_score": 66.7,
        "mmlu_pro_score": 82.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.50e+25 (Medium); coding_score: 4.53e+25 (Medium); aai_score: 1.03e+26 (Low); mmlu_pro_score: 5.17e+25 (Medium) \u2192 weighted average: 5.75e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.519184",
      "metadata": {}
    },
    {
      "name": "llama_3.1_nemotron_ultra_253b_v1",
      "developer": "Meta",
      "release_date": null,
      "parameters": 253000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "5.23e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.12e+25 (Medium); coding_score: 4.22e+25 (Medium); aai_score: 8.56e+25 (Low); mmlu_pro_score: 5.12e+25 (Medium) \u2192 weighted average: 5.23e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1321.0,
        "coding_score": 1345.0,
        "aai_score": 60.8,
        "mmlu_pro_score": 82.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 253,000,000,000 params \u00d7 3,795,000,000,000 tokens = 5.76e+24 FLOP (Modern large model: 253B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.519511",
      "metadata": {}
    },
    {
      "name": "gemini_2.0_flash_lite",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.01e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1331.0,
        "coding_score": 1338.0,
        "vision_score": 1147.0,
        "aai_score": 41.4,
        "mmlu_pro_score": 72.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.21e+25 (Medium); coding_score: 4.15e+25 (Medium); aai_score: 3.97e+25 (Medium); mmlu_pro_score: 3.70e+25 (Medium) \u2192 weighted average: 4.01e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.519674",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_pro_002",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.17e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1320.0,
        "coding_score": 1311.0,
        "vision_score": 1208.0,
        "aai_score": 44.6,
        "mmlu_pro_score": 75.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.11e+25 (Medium); coding_score: 3.91e+25 (Medium); aai_score: 4.61e+25 (Medium); mmlu_pro_score: 4.04e+25 (Medium) \u2192 weighted average: 4.17e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.519921",
      "metadata": {}
    },
    {
      "name": "mistral_small",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.98e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1335.0,
        "coding_score": 1361.0,
        "vision_score": 1183.0,
        "aai_score": 42.3,
        "mmlu_pro_score": 68.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.25e+25 (Medium); coding_score: 4.37e+25 (Medium); aai_score: 4.15e+25 (Medium); mmlu_pro_score: 3.17e+25 (Medium) \u2192 weighted average: 3.98e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.520226",
      "metadata": {}
    },
    {
      "name": "command_a_03",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.90e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Benchmark-based (lmarena_score): 1347.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.90e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1347.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Canadian AI company with reasonable disclosure standards and research focus. Original estimate: 3.90e+25 FLOP (Benchmark-based (lmarena_score): 1347.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.90e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.520503",
      "metadata": {}
    },
    {
      "name": "qwen_plus",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.46e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1242.0,
        "coding_score": 1263.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.42e+25 (Medium); coding_score: 3.49e+25 (Medium) \u2192 weighted average: 3.46e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.520614",
      "metadata": {}
    },
    {
      "name": "qwen3_32b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.23e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "5.42e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.32e+25 (Medium); coding_score: 4.52e+25 (Medium); aai_score: 8.12e+25 (Medium); mmlu_pro_score: 4.72e+25 (Medium) \u2192 weighted average: 5.42e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1342.0,
        "coding_score": 1376.0,
        "aai_score": 59.2,
        "mmlu_pro_score": 79.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 640,000,000,000 tokens = 1.23e+23 FLOP (Modern model: 32B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.520879",
      "metadata": {}
    },
    {
      "name": "glm_4_plus",
      "developer": "Zhipu AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.88e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.90e+25 (Medium); superclue_math: 3.48e+25 (Medium); superclue_reasoning: 1.07e+25 (Low); superclue_code: 5.27e+25 (Medium); superclue_agents: 4.73e+25 (Medium) \u2192 weighted average: 3.88e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 63.67,
        "superclue_math": 63.04,
        "superclue_reasoning": 37.04,
        "superclue_code": 82.18,
        "superclue_agents": 72.41
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company (same as Zhipu) with limited transparency on model training details - Insufficient disclosure on training compute and methodology. Original estimate: 3.88e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.90e+25 (Medium); superclue_math: 3.48e+25 (Medium); superclue_reasoning: 1.07e+25 (Low); superclue_code: 5.27e+25 (Medium); superclue_agents: 4.73e+25 (Medium) \u2192 weighted average: 3.88e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.521147",
      "metadata": {}
    },
    {
      "name": "hunyuan_turbo",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "4.09e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 4.05e+25 (Medium); coding_score: 4.12e+25 (Medium) \u2192 weighted average: 4.09e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1314.0,
        "coding_score": 1335.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Major Chinese tech company but with limited transparency on AI model training specifics - Limited disclosure on training methodology and compute for frontier AI models. Original estimate: 4.09e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 4.05e+25 (Medium); coding_score: 4.12e+25 (Medium) \u2192 weighted average: 4.09e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.521388",
      "metadata": {}
    },
    {
      "name": "gemma_3_12b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 12000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.73e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.26e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.25e+25 (Medium); coding_score: 3.90e+25 (Medium); aai_score: 2.65e+25 (Medium); mmlu_pro_score: 2.26e+25 (Medium) \u2192 weighted average: 3.26e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1335.0,
        "coding_score": 1310.0,
        "aai_score": 33.8,
        "mmlu_pro_score": 59.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 12,000,000,000 params \u00d7 240,000,000,000 tokens = 1.73e+22 FLOP (Modern model: 12B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.521583",
      "metadata": {}
    },
    {
      "name": "gpt_4o",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": 1760000000000,
      "parameter_source": "known_specification:gpt_4o",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.80e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "1.37e+26",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Known model specification 'gpt_4o': Chinchilla scaling law: 6 \u00d7 1,760,000,000,000 params \u00d7 13,000,000,000,000 tokens = 1.37e+26 FLOP"
        },
        {
          "flop": "4.00e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.25e+25 (Medium); superclue_math: 3.71e+25 (Medium); superclue_reasoning: 2.10e+25 (Low); superclue_code: 3.96e+25 (Medium); superclue_agents: 5.36e+25 (Medium) \u2192 weighted average: 4.00e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "superclue_overall": 65.92,
        "superclue_math": 64.74,
        "superclue_reasoning": 48.52,
        "superclue_code": 73.27,
        "superclue_agents": 76.12
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from OpenAI patterns",
      "last_updated": "2025-08-04T20:15:51.521790",
      "metadata": {}
    },
    {
      "name": "qwq_32b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "5.21e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.21e+25 (Medium); coding_score: 4.26e+25 (Low); aai_score: 7.82e+25 (Medium); mmlu_pro_score: 4.23e+25 (Medium) \u2192 weighted average: 5.21e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1331.0,
        "coding_score": 1349.0,
        "aai_score": 58.1,
        "mmlu_pro_score": 76.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.522093",
      "metadata": {}
    },
    {
      "name": "step_2",
      "developer": "StepFun",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "4.02e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 4.12e+25 (Medium); coding_score: 3.92e+25 (Medium) \u2192 weighted average: 4.02e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1321.0,
        "coding_score": 1313.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company with very limited public information on model training - Extremely limited disclosure on training compute, methodology, and specifications. Original estimate: 4.02e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 4.12e+25 (Medium); coding_score: 3.92e+25 (Medium) \u2192 weighted average: 4.02e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.522357",
      "metadata": {}
    },
    {
      "name": "o1_mini",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.79e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1319.0,
        "coding_score": 1366.0,
        "aai_score": 53.8,
        "mmlu_pro_score": 74.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.10e+25 (Medium); coding_score: 4.42e+25 (Medium); aai_score: 6.71e+25 (Medium); mmlu_pro_score: 3.93e+25 (Medium) \u2192 weighted average: 4.79e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.522543",
      "metadata": {}
    },
    {
      "name": "llama_3.1_405b_instruct_bf16",
      "developer": "Meta",
      "release_date": null,
      "parameters": 405000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.48e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.80e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.80e+25 (Medium); coding_score: 3.80e+25 (Medium); aai_score: 3.80e+25 (Medium); mmlu_pro_score: 3.80e+25 (Medium) \u2192 weighted average: 3.80e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1286.0,
        "coding_score": 1299.0,
        "aai_score": 40.5,
        "mmlu_pro_score": 73.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 405,000,000,000 params \u00d7 6,075,000,000,000 tokens = 1.48e+25 FLOP (Modern large model: 405B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.522869",
      "metadata": {}
    },
    {
      "name": "llama_3.1_405b_instruct_fp8",
      "developer": "Meta",
      "release_date": null,
      "parameters": 405000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.48e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.78e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.78e+25 (Medium); coding_score: 3.74e+25 (Medium); aai_score: 3.80e+25 (Medium); mmlu_pro_score: 3.80e+25 (Medium) \u2192 weighted average: 3.78e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1284.0,
        "coding_score": 1292.0,
        "aai_score": 40.5,
        "mmlu_pro_score": 73.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 405,000,000,000 params \u00d7 6,075,000,000,000 tokens = 1.48e+25 FLOP (Modern large model: 405B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.523199",
      "metadata": {}
    },
    {
      "name": "gemini_advanced",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.77e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1332.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1332.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.77e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.523388",
      "metadata": {}
    },
    {
      "name": "grok_2",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.00e+25",
      "training_flop_confidence": "high",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "3.77e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1332.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.77e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_above_1e25",
      "benchmarks": {
        "lmarena_score": 1332.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: High-precision estimate from xAI disclosure",
      "last_updated": "2025-08-04T20:15:51.523556",
      "metadata": {}
    },
    {
      "name": "qwen3_30b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 30000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.08e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.98e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.11e+25 (Medium); coding_score: 4.23e+25 (Medium); aai_score: 7.16e+25 (Medium); mmlu_pro_score: 4.41e+25 (Medium) \u2192 weighted average: 4.98e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1320.0,
        "coding_score": 1346.0,
        "aai_score": 55.6,
        "mmlu_pro_score": 77.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 30,000,000,000 params \u00d7 600,000,000,000 tokens = 1.08e+23 FLOP (Modern model: 30B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.523827",
      "metadata": {}
    },
    {
      "name": "llama_4_maverick_17b_128e",
      "developer": "Meta",
      "release_date": null,
      "parameters": 17000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.47e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.64e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.85e+25 (Medium); coding_score: 3.92e+25 (Medium); aai_score: 5.91e+25 (Medium); mmlu_pro_score: 4.88e+25 (Medium) \u2192 weighted average: 4.64e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1292.0,
        "coding_score": 1312.0,
        "vision_score": 1184.0,
        "aai_score": 50.5,
        "mmlu_pro_score": 80.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 17,000,000,000 params \u00d7 340,000,000,000 tokens = 3.47e+22 FLOP (Modern model: 17B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.524004",
      "metadata": {}
    },
    {
      "name": "llama_3.3_nemotron_49b_super_v1",
      "developer": "Nvidia",
      "release_date": null,
      "parameters": 49000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.72e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1325.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.72e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1325.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 49,000,000,000 params \u00d7 980,000,000,000 tokens = 2.88e+23 FLOP (Modern model: 49B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.524192",
      "metadata": {}
    },
    {
      "name": "yi_lightning",
      "developer": "01 AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.84e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.93e+25 (Medium); superclue_math: 3.65e+25 (Medium); superclue_reasoning: 1.32e+25 (Low); superclue_code: 4.51e+25 (Medium); superclue_agents: 4.93e+25 (Medium) \u2192 weighted average: 3.84e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 63.88,
        "superclue_math": 64.3,
        "superclue_reasoning": 40.37,
        "superclue_code": 77.23,
        "superclue_agents": 73.61
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company founded by Kai-Fu Lee, relatively new but has some disclosure. Original estimate: 3.84e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.93e+25 (Medium); superclue_math: 3.65e+25 (Medium); superclue_reasoning: 1.32e+25 (Low); superclue_code: 4.51e+25 (Medium); superclue_agents: 4.93e+25 (Medium) \u2192 weighted average: 3.84e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.524509",
      "metadata": {}
    },
    {
      "name": "hunyuan_large",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.88e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.84e+25 (Medium); coding_score: 3.91e+25 (Medium) \u2192 weighted average: 3.88e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1291.0,
        "coding_score": 1311.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Major Chinese tech company but with limited transparency on AI model training specifics - Limited disclosure on training methodology and compute for frontier AI models. Original estimate: 3.88e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.84e+25 (Medium); coding_score: 3.91e+25 (Medium) \u2192 weighted average: 3.88e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.524690",
      "metadata": {}
    },
    {
      "name": "deepseek_v2.5",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.94e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 57.67,
        "superclue_math": 60.34,
        "superclue_reasoning": 38.77,
        "superclue_code": 65.15,
        "superclue_agents": 66.42
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.05e+25 (Medium); superclue_math: 3.12e+25 (Medium); superclue_reasoning: 1.20e+25 (Low); superclue_code: 2.95e+25 (Low); superclue_agents: 3.81e+25 (Medium) \u2192 weighted average: 2.94e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.524834",
      "metadata": {}
    },
    {
      "name": "gpt_4.1_nano",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.63e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1287.0,
        "coding_score": 1312.0,
        "vision_score": 1110.0,
        "aai_score": 41.0,
        "mmlu_pro_score": 65.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.81e+25 (Medium); coding_score: 3.92e+25 (Medium); aai_score: 3.89e+25 (Medium); mmlu_pro_score: 2.90e+25 (Medium) \u2192 weighted average: 3.63e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.525066",
      "metadata": {}
    },
    {
      "name": "gpt_4_turbo",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.54e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1275.0,
        "coding_score": 1280.0,
        "vision_score": 1137.0,
        "aai_score": 38.8,
        "mmlu_pro_score": 69.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.70e+25 (Medium); coding_score: 3.64e+25 (Medium); aai_score: 3.49e+25 (Medium); mmlu_pro_score: 3.33e+25 (Medium) \u2192 weighted average: 3.54e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.525315",
      "metadata": {}
    },
    {
      "name": "claude_3_opus",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": 175000000000,
      "parameter_source": "known_specification:claude_3_opus",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.60e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "8.40e+24",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Known model specification 'claude_3_opus': Chinchilla scaling law: 6 \u00d7 175,000,000,000 params \u00d7 8,000,000,000,000 tokens = 8.40e+24 FLOP"
        },
        {
          "flop": "3.34e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.62e+25 (Medium); coding_score: 3.54e+25 (Medium); aai_score: 2.85e+25 (Medium); mmlu_pro_score: 3.35e+25 (Medium) \u2192 weighted average: 3.34e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1265.0,
        "coding_score": 1269.0,
        "vision_score": 1070.0,
        "aai_score": 35.1,
        "mmlu_pro_score": 69.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from industry analysis",
      "last_updated": "2025-08-04T20:15:51.525610",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_pro_001",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.67e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1320.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1320.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.67e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.525788",
      "metadata": {}
    },
    {
      "name": "amazon_nova_experimental_chat_05_14",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.66e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1318.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1318.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.66e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.525954",
      "metadata": {}
    },
    {
      "name": "llama_4_scout_17b_16e",
      "developer": "Meta",
      "release_date": null,
      "parameters": 17000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.47e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.95e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.71e+25 (Medium); coding_score: 3.72e+25 (Medium); aai_score: 4.28e+25 (Medium); mmlu_pro_score: 4.06e+25 (Medium) \u2192 weighted average: 3.95e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1276.0,
        "coding_score": 1290.0,
        "vision_score": 1171.0,
        "aai_score": 43.0,
        "mmlu_pro_score": 75.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 17,000,000,000 params \u00d7 340,000,000,000 tokens = 3.47e+22 FLOP (Modern model: 17B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.526241",
      "metadata": {}
    },
    {
      "name": "gemma_3n_e4b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 4000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.92e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.86e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.97e+25 (Medium); coding_score: 3.78e+25 (Medium); aai_score: 1.82e+25 (Medium); mmlu_pro_score: 1.38e+25 (Low) \u2192 weighted average: 2.86e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1305.0,
        "coding_score": 1297.0,
        "aai_score": 28.0,
        "mmlu_pro_score": 48.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 4,000,000,000 params \u00d7 80,000,000,000 tokens = 1.92e+21 FLOP (Modern model: 4B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.526415",
      "metadata": {}
    },
    {
      "name": "qwen_max",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.79e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 64.47,
        "superclue_math": 66.07,
        "superclue_reasoning": 47.41,
        "superclue_code": 75.64,
        "superclue_agents": 68.76
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.02e+25 (Medium); superclue_math: 3.91e+25 (Medium); superclue_reasoning: 1.98e+25 (Low); superclue_code: 4.28e+25 (Medium); superclue_agents: 4.15e+25 (Medium) \u2192 weighted average: 3.79e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.526591",
      "metadata": {}
    },
    {
      "name": "llama_3.3_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.78e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.81e+25 (Medium); superclue_math: 2.94e+25 (Medium); superclue_reasoning: 1.11e+25 (Low); superclue_code: 2.30e+25 (Low); superclue_agents: 4.01e+25 (Medium) \u2192 weighted average: 2.78e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "superclue_overall": 55.84,
        "superclue_math": 58.92,
        "superclue_reasoning": 37.65,
        "superclue_code": 59.01,
        "superclue_agents": 67.8
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,400,000,000,000 tokens = 5.88e+23 FLOP (Modern model: 70B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.526927",
      "metadata": {}
    },
    {
      "name": "qwen2.5_plus",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.63e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1315.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1315.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.63e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.527004",
      "metadata": {}
    },
    {
      "name": "claude_3_5_haiku",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.65e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1317.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1317.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.65e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.527188",
      "metadata": {}
    },
    {
      "name": "gpt_4o_mini",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.14e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 58.82,
        "superclue_math": 55.45,
        "superclue_reasoning": 40.37,
        "superclue_code": 67.92,
        "superclue_agents": 71.56
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.20e+25 (Medium); superclue_math: 2.52e+25 (Low); superclue_reasoning: 1.32e+25 (Low); superclue_code: 3.27e+25 (Medium); superclue_agents: 4.59e+25 (Medium) \u2192 weighted average: 3.14e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.527511",
      "metadata": {}
    },
    {
      "name": "gpt_4_preview",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.55e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1266.0,
        "coding_score": 1261.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.63e+25 (Medium); coding_score: 3.48e+25 (Medium) \u2192 weighted average: 3.55e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.527704",
      "metadata": {}
    },
    {
      "name": "athene",
      "developer": "NexusFlow",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.63e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Benchmark-based (lmarena_score): 1315.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.63e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1315.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: AI company with focus on open models and reasonable disclosure practices. Original estimate: 3.63e+25 FLOP (Benchmark-based (lmarena_score): 1315.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.63e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.528133",
      "metadata": {}
    },
    {
      "name": "hunyuan_standard",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.71e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.71e+25 (Medium); coding_score: 3.71e+25 (Medium) \u2192 weighted average: 3.71e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1276.0,
        "coding_score": 1289.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Major Chinese tech company but with limited transparency on AI model training specifics - Limited disclosure on training methodology and compute for frontier AI models. Original estimate: 3.71e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.71e+25 (Medium); coding_score: 3.71e+25 (Medium) \u2192 weighted average: 3.71e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.528378",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_flash_002",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.52e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1290.0,
        "coding_score": 1273.0,
        "vision_score": 1187.0,
        "aai_score": 39.0,
        "mmlu_pro_score": 68.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.84e+25 (Medium); coding_score: 3.58e+25 (Medium); aai_score: 3.52e+25 (Medium); mmlu_pro_score: 3.16e+25 (Medium) \u2192 weighted average: 3.52e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.528984",
      "metadata": {}
    },
    {
      "name": "mistral_large",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.52e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1269.0,
        "coding_score": 1284.0,
        "aai_score": 38.3,
        "mmlu_pro_score": 69.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.65e+25 (Medium); coding_score: 3.67e+25 (Medium); aai_score: 3.40e+25 (Medium); mmlu_pro_score: 3.36e+25 (Medium) \u2192 weighted average: 3.52e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.529243",
      "metadata": {}
    },
    {
      "name": "magistral_medium",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.68e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1253.0,
        "coding_score": 1307.0,
        "aai_score": 56.0,
        "mmlu_pro_score": 75.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.51e+25 (Medium); coding_score: 3.87e+25 (Medium); aai_score: 7.27e+25 (Medium); mmlu_pro_score: 4.08e+25 (Medium) \u2192 weighted average: 4.68e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.529417",
      "metadata": {}
    },
    {
      "name": "gemma_3_4b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 4000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.92e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.59e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.86e+25 (Medium); coding_score: 3.51e+25 (Medium); aai_score: 1.49e+25 (Medium); mmlu_pro_score: 9.31e+24 (Low) \u2192 weighted average: 2.59e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1293.0,
        "coding_score": 1265.0,
        "aai_score": 25.4,
        "mmlu_pro_score": 41.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 4,000,000,000 params \u00d7 80,000,000,000 tokens = 1.92e+21 FLOP (Modern model: 4B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.529697",
      "metadata": {}
    },
    {
      "name": "grok_2_mini",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.57e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1307.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1307.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.57e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.529792",
      "metadata": {}
    },
    {
      "name": "athene_70b",
      "developer": "NexusFlow",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.61e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.64e+25 (Medium); coding_score: 3.58e+25 (Medium) \u2192 weighted average: 3.61e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1268.0,
        "coding_score": 1274.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.530109",
      "metadata": {}
    },
    {
      "name": "qwen2.5_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.73e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.89e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.94e+25 (Medium); superclue_math: 3.35e+25 (Medium); superclue_reasoning: 1.14e+25 (Low); superclue_code: 2.48e+25 (Low); superclue_agents: 3.81e+25 (Medium) \u2192 weighted average: 2.89e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "superclue_overall": 56.84,
        "superclue_math": 62.11,
        "superclue_reasoning": 38.02,
        "superclue_code": 60.79,
        "superclue_agents": 66.42
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 864,000,000,000 tokens = 3.73e+23 FLOP (Mid-era model: 72B params * 12 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.530319",
      "metadata": {}
    },
    {
      "name": "llama_3.1_nemotron_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.50e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.79e+25 (Medium); coding_score: 3.71e+25 (Medium); aai_score: 3.22e+25 (Medium); mmlu_pro_score: 3.28e+25 (Medium) \u2192 weighted average: 3.50e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1285.0,
        "coding_score": 1289.0,
        "aai_score": 37.3,
        "mmlu_pro_score": 69.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,400,000,000,000 tokens = 5.88e+23 FLOP (Modern model: 70B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.530508",
      "metadata": {}
    },
    {
      "name": "hunyuan_large_vision",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.73e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.66e+25 (Medium); coding_score: 3.79e+25 (Medium) \u2192 weighted average: 3.73e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1270.0,
        "coding_score": 1298.0,
        "vision_score": 1238.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Major Chinese tech company but with limited transparency on AI model training specifics - Limited disclosure on training methodology and compute for frontier AI models. Original estimate: 3.73e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.66e+25 (Medium); coding_score: 3.79e+25 (Medium) \u2192 weighted average: 3.73e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.530733",
      "metadata": {}
    },
    {
      "name": "mistral_small_3.1_24b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 24000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.15e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.31e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.65e+25 (Medium); coding_score: 3.77e+25 (Medium); aai_score: 2.89e+25 (Medium); mmlu_pro_score: 2.92e+25 (Medium) \u2192 weighted average: 3.31e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1269.0,
        "coding_score": 1295.0,
        "vision_score": 1162.0,
        "aai_score": 35.3,
        "mmlu_pro_score": 65.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 24,000,000,000 params \u00d7 288,000,000,000 tokens = 4.15e+22 FLOP (Mid-era model: 24B params * 12 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.530895",
      "metadata": {}
    },
    {
      "name": "llama_3.1_tulu_3_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.48e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.57e+25 (Medium); coding_score: 3.39e+25 (Medium) \u2192 weighted average: 3.48e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1260.0,
        "coding_score": 1251.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,400,000,000,000 tokens = 5.88e+23 FLOP (Modern model: 70B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.530997",
      "metadata": {}
    },
    {
      "name": "llama_3.1_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "known_specification:llama_3.1_70b",
      "context_length": null,
      "architecture": null,
      "training_flop": "6.30e+24",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.23e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.32e+25 (Low); superclue_math: 2.72e+25 (Low); superclue_reasoning: 1.10e+25 (Low); superclue_code: 1.71e+25 (Low); superclue_agents: 2.96e+25 (Medium) \u2192 weighted average: 2.23e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "superclue_overall": 51.74,
        "superclue_math": 57.15,
        "superclue_reasoning": 37.41,
        "superclue_code": 52.38,
        "superclue_agents": 60.02
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Known model specification 'llama_3.1_70b': Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 15,000,000,000,000 tokens = 6.30e+24 FLOP",
      "last_updated": "2025-08-04T20:15:51.531175",
      "metadata": {}
    },
    {
      "name": "llama_3.1_nemotron_51b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 51000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.12e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.27e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.33e+25 (Medium); coding_score: 3.20e+25 (Medium) \u2192 weighted average: 3.27e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1231.0,
        "coding_score": 1227.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 51,000,000,000 params \u00d7 1,020,000,000,000 tokens = 3.12e+23 FLOP (Modern model: 51B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.531268",
      "metadata": {}
    },
    {
      "name": "amazon_nova_pro",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.43e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1262.0,
        "coding_score": 1282.0,
        "vision_score": 1029.0,
        "aai_score": 37.1,
        "mmlu_pro_score": 69.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.59e+25 (Medium); coding_score: 3.65e+25 (Medium); aai_score: 3.19e+25 (Medium); mmlu_pro_score: 3.29e+25 (Medium) \u2192 weighted average: 3.43e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.531366",
      "metadata": {}
    },
    {
      "name": "jamba_1.5_large",
      "developer": "AI21 Labs",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.70e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.42e+25 (Medium); coding_score: 3.34e+25 (Medium); aai_score: 1.99e+25 (Medium); mmlu_pro_score: 2.05e+25 (Medium) \u2192 weighted average: 2.70e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1242.0,
        "coding_score": 1244.0,
        "aai_score": 29.3,
        "mmlu_pro_score": 57.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Israeli AI company with reasonable disclosure standards, academic backing. Original estimate: 2.70e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.42e+25 (Medium); coding_score: 3.34e+25 (Medium); aai_score: 1.99e+25 (Medium); mmlu_pro_score: 2.05e+25 (Medium) \u2192 weighted average: 2.70e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.531831",
      "metadata": {}
    },
    {
      "name": "reka_core",
      "developer": "Reka AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.34e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.49e+25 (Medium); coding_score: 3.19e+25 (Medium) \u2192 weighted average: 3.34e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1250.0,
        "coding_score": 1226.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: AI company with research focus and reasonable disclosure practices. Original estimate: 3.34e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.49e+25 (Medium); coding_score: 3.19e+25 (Medium) \u2192 weighted average: 3.34e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.532388",
      "metadata": {}
    },
    {
      "name": "gpt_4",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": 1760000000000,
      "parameter_source": "known_specification:gpt_4",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.10e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "1.37e+26",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Known model specification 'gpt_4': Chinchilla scaling law: 6 \u00d7 1,760,000,000,000 params \u00d7 13,000,000,000,000 tokens = 1.37e+26 FLOP"
        },
        {
          "flop": "3.33e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1277.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.33e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1277.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from scaling analysis",
      "last_updated": "2025-08-04T20:15:51.532486",
      "metadata": {}
    },
    {
      "name": "gemma_2_27b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 27000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.25e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.65e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.61e+25 (Low); superclue_math: 2.90e+25 (Medium); superclue_reasoning: 9.90e+24 (Low); superclue_code: 1.87e+25 (Low); superclue_agents: 4.05e+25 (Medium) \u2192 weighted average: 2.65e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "superclue_overall": 54.21,
        "superclue_math": 58.63,
        "superclue_reasoning": 35.93,
        "superclue_code": 54.26,
        "superclue_agents": 68.04
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 27,000,000,000 params \u00d7 324,000,000,000 tokens = 5.25e+22 FLOP (Mid-era model: 27B params * 12 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.532711",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_flash_001",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.38e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1284.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1284.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.38e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.532764",
      "metadata": {}
    },
    {
      "name": "gemma_2_9b_it_simpo",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 9000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.83e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.21e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.35e+25 (Medium); coding_score: 3.08e+25 (Medium) \u2192 weighted average: 3.21e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1233.0,
        "coding_score": 1211.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 9,000,000,000 params \u00d7 108,000,000,000 tokens = 5.83e+21 FLOP (Mid-era model: 9B params * 12 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.532951",
      "metadata": {}
    },
    {
      "name": "claude_3_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.60e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1223.0,
        "coding_score": 1232.0,
        "vision_score": 1033.0,
        "aai_score": 27.8,
        "mmlu_pro_score": 57.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.27e+25 (Medium); coding_score: 3.24e+25 (Medium); aai_score: 1.79e+25 (Medium); mmlu_pro_score: 2.11e+25 (Medium) \u2192 weighted average: 2.60e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.533107",
      "metadata": {}
    },
    {
      "name": "command_r_plus_08",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.36e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Benchmark-based (lmarena_score): 1281.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.36e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1281.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Canadian AI company with reasonable disclosure standards and research focus. Original estimate: 3.36e+25 FLOP (Benchmark-based (lmarena_score): 1281.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.36e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.533601",
      "metadata": {}
    },
    {
      "name": "nemotron_4_340b",
      "developer": "Nvidia",
      "release_date": null,
      "parameters": 340000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.04e+25",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.23e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.32e+25 (Medium); coding_score: 3.15e+25 (Medium) \u2192 weighted average: 3.23e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1229.0,
        "coding_score": 1220.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 340,000,000,000 params \u00d7 5,100,000,000,000 tokens = 1.04e+25 FLOP (Generic estimate: 340B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.533920",
      "metadata": {}
    },
    {
      "name": "reka_flash",
      "developer": "Reka AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.12e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.23e+25 (Medium); coding_score: 3.00e+25 (Medium) \u2192 weighted average: 3.12e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1218.0,
        "coding_score": 1201.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: AI company with research focus and reasonable disclosure practices. Original estimate: 3.12e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.23e+25 (Medium); coding_score: 3.00e+25 (Medium) \u2192 weighted average: 3.12e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.534348",
      "metadata": {}
    },
    {
      "name": "glm_4",
      "developer": "Zhipu AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.07e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.07e+25 (Medium); coding_score: 3.06e+25 (Medium) \u2192 weighted average: 3.07e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1198.0,
        "coding_score": 1209.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company (same as Zhipu) with limited transparency on model training details - Insufficient disclosure on training compute and methodology. Original estimate: 3.07e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.07e+25 (Medium); coding_score: 3.06e+25 (Medium) \u2192 weighted average: 3.07e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.534552",
      "metadata": {}
    },
    {
      "name": "llama_3_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "known_specification:llama_3_70b",
      "context_length": null,
      "architecture": null,
      "training_flop": "6.30e+24",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.55e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.28e+25 (Medium); coding_score: 3.12e+25 (Medium); aai_score: 1.75e+25 (Medium); mmlu_pro_score: 2.07e+25 (Medium) \u2192 weighted average: 2.55e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1224.0,
        "coding_score": 1216.0,
        "aai_score": 27.5,
        "mmlu_pro_score": 57.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'llama_3_70b': Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 15,000,000,000,000 tokens = 6.30e+24 FLOP",
      "last_updated": "2025-08-04T20:15:51.534745",
      "metadata": {}
    },
    {
      "name": "mistral_small_24b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 24000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.15e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.13e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.39e+25 (Medium); coding_score: 3.39e+25 (Medium); aai_score: 2.89e+25 (Medium); mmlu_pro_score: 2.85e+25 (Medium) \u2192 weighted average: 3.13e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1238.0,
        "coding_score": 1251.0,
        "aai_score": 35.3,
        "mmlu_pro_score": 65.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 24,000,000,000 params \u00d7 288,000,000,000 tokens = 4.15e+22 FLOP (Mid-era model: 24B params * 12 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.534885",
      "metadata": {}
    },
    {
      "name": "qwen2.5_coder_32b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.37e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.18e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.37e+25 (Medium); coding_score: 3.63e+25 (Medium); aai_score: 3.05e+25 (Medium); mmlu_pro_score: 2.66e+25 (Medium) \u2192 weighted average: 3.18e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1235.0,
        "coding_score": 1279.0,
        "aai_score": 36.3,
        "mmlu_pro_score": 63.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 384,000,000,000 tokens = 7.37e+22 FLOP (Mid-era model: 32B params * 12 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.535015",
      "metadata": {}
    },
    {
      "name": "c4ai_aya_expanse_32b",
      "developer": "Cohere",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.26e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1269.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.26e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1269.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.535089",
      "metadata": {}
    },
    {
      "name": "olmo_2_32b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.19e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.27e+25 (Medium); coding_score: 3.11e+25 (Medium) \u2192 weighted average: 3.19e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1223.0,
        "coding_score": 1215.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.535187",
      "metadata": {}
    },
    {
      "name": "qwen2_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.73e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.80e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.15e+25 (Medium); coding_score: 3.04e+25 (Medium); aai_score: 2.46e+25 (Medium); mmlu_pro_score: 2.53e+25 (Medium) \u2192 weighted average: 2.80e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1208.0,
        "coding_score": 1206.0,
        "aai_score": 32.6,
        "mmlu_pro_score": 62.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 864,000,000,000 tokens = 3.73e+23 FLOP (Mid-era model: 72B params * 12 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.535317",
      "metadata": {}
    },
    {
      "name": "command_r_plus",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.24e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Benchmark-based (lmarena_score): 1266.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.24e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1266.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Canadian AI company with reasonable disclosure standards and research focus. Original estimate: 3.24e+25 FLOP (Benchmark-based (lmarena_score): 1266.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.24e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.535494",
      "metadata": {}
    },
    {
      "name": "gemma_2_9b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 9000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.83e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.18e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.19e+25 (Medium); coding_score: 2.95e+25 (Low); aai_score: 1.14e+25 (Medium); mmlu_pro_score: 1.43e+25 (Low) \u2192 weighted average: 2.18e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1213.0,
        "coding_score": 1194.0,
        "aai_score": 22.2,
        "mmlu_pro_score": 49.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 9,000,000,000 params \u00d7 108,000,000,000 tokens = 5.83e+21 FLOP (Mid-era model: 9B params * 12 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.535657",
      "metadata": {}
    },
    {
      "name": "deepseek_coder_v2",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": 236000000000,
      "parameter_source": "known_specification:deepseek_coder_v2",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.50e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.26e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.06e+25 (Medium); coding_score: 3.46e+25 (Medium) \u2192 weighted average: 3.26e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1196.0,
        "coding_score": 1259.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'deepseek_coder_v2': Chinchilla scaling law: 6 \u00d7 236,000,000,000 params \u00d7 6,000,000,000,000 tokens = 8.50e+24 FLOP",
      "last_updated": "2025-08-04T20:15:51.535758",
      "metadata": {}
    },
    {
      "name": "claude_3_haiku",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.31e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1200.0,
        "coding_score": 1208.0,
        "vision_score": 1000.0,
        "aai_score": 24.1,
        "mmlu_pro_score": 50.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.09e+25 (Medium); coding_score: 3.06e+25 (Medium); aai_score: 1.35e+25 (Medium); mmlu_pro_score: 1.47e+25 (Low) \u2192 weighted average: 2.31e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.535886",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_flash_8b_001",
      "developer": "Google",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.69e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.33e+25 (Medium); coding_score: 3.21e+25 (Medium); aai_score: 2.20e+25 (Medium); mmlu_pro_score: 2.02e+25 (Medium) \u2192 weighted average: 2.69e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1231.0,
        "coding_score": 1228.0,
        "vision_score": 1092.0,
        "aai_score": 30.8,
        "mmlu_pro_score": 56.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.536160",
      "metadata": {}
    },
    {
      "name": "amazon_nova_lite",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.86e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1233.0,
        "coding_score": 1253.0,
        "vision_score": 1039.0,
        "aai_score": 32.5,
        "mmlu_pro_score": 59.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.35e+25 (Medium); coding_score: 3.41e+25 (Medium); aai_score: 2.45e+25 (Medium); mmlu_pro_score: 2.22e+25 (Medium) \u2192 weighted average: 2.86e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.536260",
      "metadata": {}
    },
    {
      "name": "phi_4",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.47e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.26e+25 (Medium); coding_score: 3.32e+25 (Medium); aai_score: 3.74e+25 (Medium); mmlu_pro_score: 3.57e+25 (Medium) \u2192 weighted average: 3.47e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1222.0,
        "coding_score": 1242.0,
        "aai_score": 40.2,
        "mmlu_pro_score": 71.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: So far they are not generating large proprietary models. Original estimate: 3.47e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.26e+25 (Medium); coding_score: 3.32e+25 (Medium); aai_score: 3.74e+25 (Medium); mmlu_pro_score: 3.57e+25 (Medium) \u2192 weighted average: 3.47e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.536640",
      "metadata": {}
    },
    {
      "name": "command_r_08",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.14e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Benchmark-based (lmarena_score): 1253.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.14e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1253.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Canadian AI company with reasonable disclosure standards and research focus. Original estimate: 3.14e+25 FLOP (Benchmark-based (lmarena_score): 1253.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.14e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.536840",
      "metadata": {}
    },
    {
      "name": "amazon_nova_micro",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.57e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1215.0,
        "coding_score": 1228.0,
        "aai_score": 28.3,
        "mmlu_pro_score": 53.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.20e+25 (Medium); coding_score: 3.21e+25 (Medium); aai_score: 1.86e+25 (Medium); mmlu_pro_score: 1.70e+25 (Low) \u2192 weighted average: 2.57e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.536964",
      "metadata": {}
    },
    {
      "name": "jamba_1.5_mini",
      "developer": "AI21 Labs",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.01e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.03e+25 (Medium); coding_score: 2.97e+25 (Low) \u2192 weighted average: 3.01e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1193.0,
        "coding_score": 1197.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Israeli AI company with reasonable disclosure standards, academic backing. Original estimate: 3.01e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.03e+25 (Medium); coding_score: 2.97e+25 (Low) \u2192 weighted average: 3.01e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.537210",
      "metadata": {}
    },
    {
      "name": "hunyuan_standard_256k",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.25e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.16e+25 (Medium); coding_score: 3.34e+25 (Medium) \u2192 weighted average: 3.25e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1209.0,
        "coding_score": 1244.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Major Chinese tech company but with limited transparency on AI model training specifics - Limited disclosure on training methodology and compute for frontier AI models. Original estimate: 3.25e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.16e+25 (Medium); coding_score: 3.34e+25 (Medium) \u2192 weighted average: 3.25e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.537391",
      "metadata": {}
    },
    {
      "name": "ministral_8b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.15e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.07e+25 (Medium); coding_score: 3.14e+25 (Medium); aai_score: 1.15e+25 (Medium); mmlu_pro_score: 7.82e+24 (Low) \u2192 weighted average: 2.15e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1198.0,
        "coding_score": 1219.0,
        "aai_score": 22.3,
        "mmlu_pro_score": 38.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.537529",
      "metadata": {}
    },
    {
      "name": "qwen1.5_110b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 110000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.16e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.30e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 3 benchmarks: openlm_arena_elo: 2.94e+25 (Low); coding_score: 2.94e+25 (Low); aai_score: 1.45e+25 (Medium) \u2192 weighted average: 2.30e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1180.0,
        "coding_score": 1192.0,
        "aai_score": 25.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 110,000,000,000 params \u00d7 1,760,000,000,000 tokens = 1.16e+24 FLOP (Chinese model: 110B params * 16 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.537635",
      "metadata": {}
    },
    {
      "name": "qwen1.5_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.98e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.84e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.88e+25 (Low); coding_score: 2.81e+25 (Low) \u2192 weighted average: 2.84e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1172.0,
        "coding_score": 1175.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 1,152,000,000,000 tokens = 4.98e+23 FLOP (Chinese model: 72B params * 16 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.537746",
      "metadata": {}
    },
    {
      "name": "reka_flash_21b_online",
      "developer": "Reka AI",
      "release_date": null,
      "parameters": 21000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.97e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.01e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1235.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.01e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1235.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 21,000,000,000 params \u00d7 315,000,000,000 tokens = 3.97e+22 FLOP (Generic estimate: 21B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.537849",
      "metadata": {}
    },
    {
      "name": "gemini_pro_dev_api",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.00e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1234.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1234.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.00e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.537938",
      "metadata": {}
    },
    {
      "name": "mixtral_8x22b_instruct",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 22000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.23e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.13e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.85e+25 (Low); coding_score: 2.81e+25 (Low); aai_score: 1.59e+25 (Medium); mmlu_pro_score: 1.75e+25 (Medium) \u2192 weighted average: 2.13e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1168.0,
        "coding_score": 1175.0,
        "aai_score": 26.2,
        "mmlu_pro_score": 53.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 22,000,000,000 params \u00d7 396,000,000,000 tokens = 5.23e+22 FLOP (Specialized model: 22B params * 18 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.538247",
      "metadata": {}
    },
    {
      "name": "command_r",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "1.62e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.85e+25 (Low); coding_score: 2.58e+25 (Low); aai_score: 5.01e+24 (Low); mmlu_pro_score: 5.46e+24 (Low) \u2192 weighted average: 1.62e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1169.0,
        "coding_score": 1141.0,
        "aai_score": 14.7,
        "mmlu_pro_score": 33.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Canadian AI company with reasonable disclosure standards and research focus. Original estimate: 1.62e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.85e+25 (Low); coding_score: 2.58e+25 (Low); aai_score: 5.01e+24 (Low); mmlu_pro_score: 5.46e+24 (Low) \u2192 weighted average: 1.62e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.538492",
      "metadata": {}
    },
    {
      "name": "reka_flash_21b",
      "developer": "Reka AI",
      "release_date": null,
      "parameters": 21000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.97e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.97e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1230.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.97e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1230.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 21,000,000,000 params \u00d7 315,000,000,000 tokens = 3.97e+22 FLOP (Generic estimate: 21B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.538603",
      "metadata": {}
    },
    {
      "name": "llama_3.1_tulu_3_8b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.68e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.04e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.09e+25 (Medium); coding_score: 2.97e+25 (Low) \u2192 weighted average: 3.04e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1200.0,
        "coding_score": 1197.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 160,000,000,000 tokens = 7.68e+21 FLOP (Modern model: 8B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.538734",
      "metadata": {}
    },
    {
      "name": "gemini_pro",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.91e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1222.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1222.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.91e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.538803",
      "metadata": {}
    },
    {
      "name": "gpt_3.5_turbo",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.59e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1138.0,
        "coding_score": 1136.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.63e+25 (Low); coding_score: 2.54e+25 (Low) \u2192 weighted average: 2.59e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.538986",
      "metadata": {}
    },
    {
      "name": "c4ai_aya_expanse_8b",
      "developer": "Cohere",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.93e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1224.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.93e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1224.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.539178",
      "metadata": {}
    },
    {
      "name": "llama_3_8b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "known_specification:llama_3_8b",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.20e+23",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.79e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.87e+25 (Low); coding_score: 2.73e+25 (Low); aai_score: 1.07e+25 (Medium); mmlu_pro_score: 8.65e+24 (Low) \u2192 weighted average: 1.79e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1171.0,
        "coding_score": 1164.0,
        "aai_score": 21.5,
        "mmlu_pro_score": 40.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'llama_3_8b': Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 15,000,000,000,000 tokens = 7.20e+23 FLOP",
      "last_updated": "2025-08-04T20:15:51.539307",
      "metadata": {}
    },
    {
      "name": "zephyr_orpo_141b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 141000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.79e+24",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.66e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.72e+25 (Low); coding_score: 2.60e+25 (Low) \u2192 weighted average: 2.66e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1150.0,
        "coding_score": 1144.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 141,000,000,000 params \u00d7 2,115,000,000,000 tokens = 1.79e+24 FLOP (Generic estimate: 141B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.539406",
      "metadata": {}
    },
    {
      "name": "yi_1.5_34b",
      "developer": "01 AI",
      "release_date": null,
      "parameters": 34000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.11e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.33e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.43e+25 (Low); superclue_math: 2.63e+25 (Low); superclue_reasoning: 1.02e+25 (Low); superclue_code: 2.11e+25 (Low); superclue_agents: 3.09e+25 (Medium) \u2192 weighted average: 2.33e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "superclue_overall": 52.69,
        "superclue_math": 56.35,
        "superclue_reasoning": 36.3,
        "superclue_code": 57.03,
        "superclue_agents": 61.08
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 34,000,000,000 params \u00d7 544,000,000,000 tokens = 1.11e+23 FLOP (Chinese model: 34B params * 16 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.539538",
      "metadata": {}
    },
    {
      "name": "granite_3.1_8b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.85e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.77e+25 (Low); coding_score: 2.93e+25 (Low) \u2192 weighted average: 2.85e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1158.0,
        "coding_score": 1191.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.539636",
      "metadata": {}
    },
    {
      "name": "llama_3.1_8b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "known_specification:llama_3.1_8b",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.20e+23",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.24e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.03e+25 (Medium); coding_score: 3.02e+25 (Medium); aai_score: 1.30e+25 (Medium); mmlu_pro_score: 1.30e+25 (Low) \u2192 weighted average: 2.24e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1193.0,
        "coding_score": 1203.0,
        "aai_score": 23.7,
        "mmlu_pro_score": 47.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'llama_3.1_8b': Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 15,000,000,000,000 tokens = 7.20e+23 FLOP",
      "last_updated": "2025-08-04T20:15:51.539743",
      "metadata": {}
    },
    {
      "name": "qwen1.5_32b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.83e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.70e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.68e+25 (Low); coding_score: 2.73e+25 (Low) \u2192 weighted average: 2.70e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1144.0,
        "coding_score": 1163.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 512,000,000,000 tokens = 9.83e+22 FLOP (Chinese model: 32B params * 16 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.539840",
      "metadata": {}
    },
    {
      "name": "phi_3_medium_4k",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.01e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.68e+25 (Low); coding_score: 2.61e+25 (Low); aai_score: 1.39e+25 (Medium); mmlu_pro_score: 1.80e+25 (Medium) \u2192 weighted average: 2.01e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1144.0,
        "coding_score": 1146.0,
        "aai_score": 24.5,
        "mmlu_pro_score": 54.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: So far they are not generating large proprietary models. Original estimate: 2.01e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.68e+25 (Low); coding_score: 2.61e+25 (Low); aai_score: 1.39e+25 (Medium); mmlu_pro_score: 1.80e+25 (Medium) \u2192 weighted average: 2.01e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.540145",
      "metadata": {}
    },
    {
      "name": "mixtral_8x7b_instruct",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.29e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.65e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.63e+25 (Low); coding_score: 2.54e+25 (Low); aai_score: 6.70e+24 (Low); mmlu_pro_score: 7.72e+24 (Low) \u2192 weighted average: 1.65e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1138.0,
        "coding_score": 1136.0,
        "aai_score": 17.0,
        "mmlu_pro_score": 38.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 126,000,000,000 tokens = 5.29e+21 FLOP (Specialized model: 7B params * 18 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.540339",
      "metadata": {}
    },
    {
      "name": "gemma_2_2b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.88e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.66e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.81e+25 (Low); coding_score: 2.50e+25 (Low) \u2192 weighted average: 2.66e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1163.0,
        "coding_score": 1130.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 24,000,000,000 tokens = 2.88e+20 FLOP (Mid-era model: 2B params * 12 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.540487",
      "metadata": {}
    },
    {
      "name": "internlm2_5_20b",
      "developer": "InternLM",
      "release_date": null,
      "parameters": 20000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.76e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1200.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.76e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1200.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 20,000,000,000 params \u00d7 300,000,000,000 tokens = 3.60e+22 FLOP (Generic estimate: 20B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.540575",
      "metadata": {}
    },
    {
      "name": "qwen1.5_14b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 14000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.88e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.60e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.61e+25 (Low); coding_score: 2.60e+25 (Low) \u2192 weighted average: 2.60e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1135.0,
        "coding_score": 1144.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 14,000,000,000 params \u00d7 224,000,000,000 tokens = 1.88e+22 FLOP (Chinese model: 14B params * 16 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.540704",
      "metadata": {}
    },
    {
      "name": "dbrx_instruct_preview",
      "developer": "Databricks",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.56e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.55e+25 (Low); coding_score: 2.58e+25 (Low) \u2192 weighted average: 2.56e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1126.0,
        "coding_score": 1141.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Open source focused company with transparent model development practices. Original estimate: 2.56e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.55e+25 (Low); coding_score: 2.58e+25 (Low) \u2192 weighted average: 2.56e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.540953",
      "metadata": {}
    },
    {
      "name": "wizardlm_70b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.41e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.55e+25 (Low); coding_score: 2.26e+25 (Low) \u2192 weighted average: 2.41e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1126.0,
        "coding_score": 1093.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.541110",
      "metadata": {}
    },
    {
      "name": "granite_3.0_8b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.42e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.43e+25 (Low); coding_score: 2.40e+25 (Low) \u2192 weighted average: 2.42e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1108.0,
        "coding_score": 1115.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.541251",
      "metadata": {}
    },
    {
      "name": "deepseek_llm_67b",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": 67000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.04e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.91e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 3 benchmarks: openlm_arena_elo: 2.45e+25 (Low); coding_score: 2.35e+25 (Low); aai_score: 9.27e+24 (Low) \u2192 weighted average: 1.91e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1111.0,
        "coding_score": 1106.0,
        "aai_score": 20.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 67,000,000,000 params \u00d7 1,005,000,000,000 tokens = 4.04e+23 FLOP (Generic estimate: 67B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.541400",
      "metadata": {}
    },
    {
      "name": "yi_34b",
      "developer": "01 AI",
      "release_date": null,
      "parameters": 34000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.11e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.55e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.61e+25 (Low); coding_score: 2.49e+25 (Low) \u2192 weighted average: 2.55e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1134.0,
        "coding_score": 1129.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 34,000,000,000 params \u00d7 544,000,000,000 tokens = 1.11e+23 FLOP (Chinese model: 34B params * 16 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.541534",
      "metadata": {}
    },
    {
      "name": "openchat",
      "developer": "OpenChat",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.28e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.40e+25 (Low); coding_score: 2.17e+25 (Low) \u2192 weighted average: 2.28e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1103.0,
        "coding_score": 1077.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Open source project with transparent development practices. Original estimate: 2.28e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.40e+25 (Low); coding_score: 2.17e+25 (Low) \u2192 weighted average: 2.28e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.541813",
      "metadata": {}
    },
    {
      "name": "granite_3.1_2b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+20",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.68e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.62e+25 (Low); coding_score: 2.75e+25 (Low) \u2192 weighted average: 2.68e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1136.0,
        "coding_score": 1166.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 30,000,000,000 tokens = 3.60e+20 FLOP (Generic estimate: 2B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.541970",
      "metadata": {}
    },
    {
      "name": "tulu_2_dpo_70b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.50e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.56e+25 (Low); coding_score: 2.44e+25 (Low) \u2192 weighted average: 2.50e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1127.0,
        "coding_score": 1120.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.542104",
      "metadata": {}
    },
    {
      "name": "openhermes_2.5_mistral_7b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.29e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.38e+25 (Low); coding_score: 2.20e+25 (Low) \u2192 weighted average: 2.29e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1100.0,
        "coding_score": 1083.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.542477",
      "metadata": {}
    },
    {
      "name": "snowflake_arctic",
      "developer": "Snowflake",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.38e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.44e+25 (Low); coding_score: 2.31e+25 (Low) \u2192 weighted average: 2.38e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1109.0,
        "coding_score": 1101.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Data platform company with transparent model development practices. Original estimate: 2.38e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.44e+25 (Low); coding_score: 2.31e+25 (Low) \u2192 weighted average: 2.38e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.542772",
      "metadata": {}
    },
    {
      "name": "gemma_1.1_7b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.37e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.39e+25 (Low); coding_score: 2.34e+25 (Low) \u2192 weighted average: 2.37e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1102.0,
        "coding_score": 1105.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.542933",
      "metadata": {}
    },
    {
      "name": "vicuna_33b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 33000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.80e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.36e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.46e+25 (Low); coding_score: 2.25e+25 (Low) \u2192 weighted average: 2.36e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1113.0,
        "coding_score": 1091.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 33,000,000,000 params \u00d7 495,000,000,000 tokens = 9.80e+22 FLOP (Generic estimate: 33B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.543075",
      "metadata": {}
    },
    {
      "name": "phi_3_small_8k",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.47e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.49e+25 (Low); coding_score: 2.46e+25 (Low) \u2192 weighted average: 2.47e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1117.0,
        "coding_score": 1123.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: So far they are not generating large proprietary models. Original estimate: 2.47e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.49e+25 (Low); coding_score: 2.46e+25 (Low) \u2192 weighted average: 2.47e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.543320",
      "metadata": {}
    },
    {
      "name": "starling_lm_7b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.64e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.64e+25 (Low); coding_score: 2.64e+25 (Low) \u2192 weighted average: 2.64e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1139.0,
        "coding_score": 1151.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.543480",
      "metadata": {}
    },
    {
      "name": "llama_2_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "known_specification:llama_2_70b",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.40e+23",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.66e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.52e+25 (Low); coding_score: 2.30e+25 (Low); aai_score: 9.27e+24 (Low); mmlu_pro_score: 8.76e+24 (Low) \u2192 weighted average: 1.66e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1122.0,
        "coding_score": 1099.0,
        "aai_score": 20.0,
        "mmlu_pro_score": 40.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'llama_2_70b': Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 2,000,000,000,000 tokens = 8.40e+23 FLOP",
      "last_updated": "2025-08-04T20:15:51.543657",
      "metadata": {}
    },
    {
      "name": "nous_hermes_2_mixtral_8x7b_dpo",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.41e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.50e+25 (Low); coding_score: 2.33e+25 (Low) \u2192 weighted average: 2.41e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1119.0,
        "coding_score": 1103.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.543800",
      "metadata": {}
    },
    {
      "name": "qwq_32b_preview",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.58e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1174.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.58e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1174.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.543885",
      "metadata": {}
    },
    {
      "name": "llama_3.2_3b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 3000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.08e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.56e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.50e+25 (Low); coding_score: 2.29e+25 (Low); aai_score: 8.81e+24 (Low); mmlu_pro_score: 5.88e+24 (Low) \u2192 weighted average: 1.56e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1118.0,
        "coding_score": 1097.0,
        "aai_score": 19.5,
        "mmlu_pro_score": 34.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 3,000,000,000 params \u00d7 60,000,000,000 tokens = 1.08e+21 FLOP (Modern model: 3B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.544107",
      "metadata": {}
    },
    {
      "name": "starling_lm_7b_alpha",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.40e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.47e+25 (Low); coding_score: 2.33e+25 (Low) \u2192 weighted average: 2.40e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1114.0,
        "coding_score": 1104.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.544287",
      "metadata": {}
    },
    {
      "name": "llama2_70b_steerlm",
      "developer": "Nvidia",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.52e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1164.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.52e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1164.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.544403",
      "metadata": {}
    },
    {
      "name": "solar_10.7b_instruct",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 10700000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.24e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.23e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.32e+25 (Low); coding_score: 2.14e+25 (Low) \u2192 weighted average: 2.23e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1091.0,
        "coding_score": 1073.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 10,700,000,000 params \u00d7 192,600,000,000 tokens = 1.24e+22 FLOP (Specialized model: 11B params * 18 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.544539",
      "metadata": {}
    },
    {
      "name": "dolphin_2.2.1_mistral_7b",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.15e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.30e+25 (Low); coding_score: 2.00e+25 (Low) \u2192 weighted average: 2.15e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1088.0,
        "coding_score": 1049.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.544650",
      "metadata": {}
    },
    {
      "name": "granite_3.0_2b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+20",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.33e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.32e+25 (Low); coding_score: 2.33e+25 (Low) \u2192 weighted average: 2.33e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1091.0,
        "coding_score": 1104.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 30,000,000,000 tokens = 3.60e+20 FLOP (Generic estimate: 2B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.544742",
      "metadata": {}
    },
    {
      "name": "mpt_30b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 30000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.10e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.13e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.23e+25 (Low); coding_score: 2.04e+25 (Low) \u2192 weighted average: 2.13e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1076.0,
        "coding_score": 1055.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 30,000,000,000 params \u00d7 450,000,000,000 tokens = 8.10e+22 FLOP (Generic estimate: 30B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.544857",
      "metadata": {}
    },
    {
      "name": "mistral_7b_instruct",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.28e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.36e+25 (Low); coding_score: 2.27e+25 (Low); aai_score: 2.36e+24 (Low); mmlu_pro_score: 2.46e+24 (Low) \u2192 weighted average: 1.28e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1097.0,
        "coding_score": 1094.0,
        "aai_score": 10.1,
        "mmlu_pro_score": 24.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.544991",
      "metadata": {}
    },
    {
      "name": "falcon_180b",
      "developer": "TII",
      "release_date": null,
      "parameters": 180000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.92e+24",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.40e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1145.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.40e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1145.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 180,000,000,000 params \u00d7 2,700,000,000,000 tokens = 2.92e+24 FLOP (Generic estimate: 180B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.545093",
      "metadata": {}
    },
    {
      "name": "wizardlm_13b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.14e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.28e+25 (Low); coding_score: 2.00e+25 (Low) \u2192 weighted average: 2.14e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1084.0,
        "coding_score": 1048.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.545238",
      "metadata": {}
    },
    {
      "name": "phi_3_mini_4k_instruct_june",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.42e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Benchmark-based (lmarena_score): 1149.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.42e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1149.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: So far they are not generating large proprietary models. Original estimate: 2.42e+25 FLOP (Benchmark-based (lmarena_score): 1149.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.42e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.545607",
      "metadata": {}
    },
    {
      "name": "llama_2_13b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.22e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.57e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.33e+25 (Low); coding_score: 2.17e+25 (Low); aai_score: 9.17e+24 (Low); mmlu_pro_score: 8.71e+24 (Low) \u2192 weighted average: 1.57e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1093.0,
        "coding_score": 1077.0,
        "aai_score": 19.9,
        "mmlu_pro_score": 40.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 156,000,000,000 tokens = 1.22e+22 FLOP (Mid-era model: 13B params * 12 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.545872",
      "metadata": {}
    },
    {
      "name": "qwen1.5_7b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.70e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.34e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.31e+25 (Low); coding_score: 2.37e+25 (Low) \u2192 weighted average: 2.34e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1090.0,
        "coding_score": 1110.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 112,000,000,000 tokens = 4.70e+21 FLOP (Chinese model: 7B params * 16 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.546015",
      "metadata": {}
    },
    {
      "name": "vicuna_13b",
      "developer": "LMSYS",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.40e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1146.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.40e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1146.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.546125",
      "metadata": {}
    },
    {
      "name": "codellama_34b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 34000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.39e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.15e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.21e+25 (Low); coding_score: 2.09e+25 (Low) \u2192 weighted average: 2.15e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1073.0,
        "coding_score": 1065.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 34,000,000,000 params \u00d7 680,000,000,000 tokens = 1.39e+23 FLOP (Modern model: 34B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.547696",
      "metadata": {}
    },
    {
      "name": "palm_2",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.37e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1141.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1141.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.37e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.547796",
      "metadata": {}
    },
    {
      "name": "qwen_14b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 14000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.88e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.37e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1140.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.37e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1140.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 14,000,000,000 params \u00d7 224,000,000,000 tokens = 1.88e+22 FLOP (Chinese model: 14B params * 16 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.548455",
      "metadata": {}
    },
    {
      "name": "gemma_7b",
      "developer": "Google",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.36e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1139.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.36e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1139.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.548553",
      "metadata": {}
    },
    {
      "name": "codellama_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.29e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.31e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1131.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.31e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1131.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,260,000,000,000 tokens = 5.29e+23 FLOP (Specialized model: 70B params * 18 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.548632",
      "metadata": {}
    },
    {
      "name": "smollm2_1.7b",
      "developer": "HuggingFace",
      "release_date": null,
      "parameters": 1700000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.60e+20",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.32e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1132.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.32e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1132.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 1,700,000,000 params \u00d7 25,500,000,000 tokens = 2.60e+20 FLOP (Generic estimate: 2B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.548710",
      "metadata": {}
    },
    {
      "name": "zephyr_7b_alpha",
      "developer": "HuggingFace",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.32e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1132.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.32e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1132.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.548828",
      "metadata": {}
    },
    {
      "name": "phi_3_mini_128k",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.35e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Benchmark-based (lmarena_score): 1138.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.35e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1138.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: So far they are not generating large proprietary models. Original estimate: 2.35e+25 FLOP (Benchmark-based (lmarena_score): 1138.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.35e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.549272",
      "metadata": {}
    },
    {
      "name": "zephyr_7b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.13e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.23e+25 (Low); coding_score: 2.02e+25 (Low) \u2192 weighted average: 2.13e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1076.0,
        "coding_score": 1053.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.549891",
      "metadata": {}
    },
    {
      "name": "phi_3_mini_4k",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.29e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.26e+25 (Low); coding_score: 2.32e+25 (Low) \u2192 weighted average: 2.29e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1082.0,
        "coding_score": 1102.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: So far they are not generating large proprietary models. Original estimate: 2.29e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.26e+25 (Low); coding_score: 2.32e+25 (Low) \u2192 weighted average: 2.29e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.550204",
      "metadata": {}
    },
    {
      "name": "guanaco_33b",
      "developer": "UW",
      "release_date": null,
      "parameters": 33000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.80e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.30e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1129.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.30e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1129.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 33,000,000,000 params \u00d7 495,000,000,000 tokens = 9.80e+22 FLOP (Generic estimate: 33B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.550448",
      "metadata": {}
    },
    {
      "name": "stripedhyena_nous_7b",
      "developer": "Together AI",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.27e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1124.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.27e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1124.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.550587",
      "metadata": {}
    },
    {
      "name": "vicuna_7b",
      "developer": "LMSYS",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.24e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1119.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.24e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1119.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.550693",
      "metadata": {}
    },
    {
      "name": "llama_3.2_1b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 1000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.20e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.15e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.17e+25 (Low); coding_score: 2.08e+25 (Low); aai_score: 2.18e+24 (Low); mmlu_pro_score: 1.48e+24 (Low) \u2192 weighted average: 1.15e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1067.0,
        "coding_score": 1063.0,
        "aai_score": 9.7,
        "mmlu_pro_score": 20.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 1,000,000,000 params \u00d7 20,000,000,000 tokens = 1.20e+20 FLOP (Modern model: 1B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.550881",
      "metadata": {}
    },
    {
      "name": "mistral_7b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.21e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1114.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.21e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1114.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.551035",
      "metadata": {}
    },
    {
      "name": "llama_2_7b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "known_specification:llama_2_7b",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.40e+22",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.20e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1113.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.20e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "lmarena_score": 1113.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Known model specification 'llama_2_7b': Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 2,000,000,000,000 tokens = 8.40e+22 FLOP",
      "last_updated": "2025-08-04T20:15:51.551149",
      "metadata": {}
    },
    {
      "name": "gemma_1.1_2b",
      "developer": "Google",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+20",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.20e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1112.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.20e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1112.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 30,000,000,000 tokens = 3.60e+20 FLOP (Generic estimate: 2B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.551284",
      "metadata": {}
    },
    {
      "name": "gemma_2b",
      "developer": "Google",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.88e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.08e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1092.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.08e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1092.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 24,000,000,000 tokens = 2.88e+20 FLOP (Mid-era model: 2B params * 12 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.551395",
      "metadata": {}
    },
    {
      "name": "qwen1.5_4b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 4000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.54e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.07e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1091.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.07e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1091.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 4,000,000,000 params \u00d7 64,000,000,000 tokens = 1.54e+21 FLOP (Chinese model: 4B params * 16 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.551520",
      "metadata": {}
    },
    {
      "name": "olmo_7b",
      "developer": "Allen AI",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.02e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1082.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.02e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1082.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.551639",
      "metadata": {}
    },
    {
      "name": "koala_13b",
      "developer": "UC Berkeley",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.97e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1072.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.97e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1072.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.551845",
      "metadata": {}
    },
    {
      "name": "gpt4all_13b_snoozy",
      "developer": "Nomic AI",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.95e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1068.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.95e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1068.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.551942",
      "metadata": {}
    },
    {
      "name": "alpaca_13b",
      "developer": "Stanford",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.93e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1066.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.93e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1066.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.552009",
      "metadata": {}
    },
    {
      "name": "mpt_7b",
      "developer": "MosaicML",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.92e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1063.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.92e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1063.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.552156",
      "metadata": {}
    },
    {
      "name": "chatglm3_6b",
      "developer": "Tsinghua",
      "release_date": null,
      "parameters": 6000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.89e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.88e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1055.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.88e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1055.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 6,000,000,000 params \u00d7 108,000,000,000 tokens = 3.89e+21 FLOP (Specialized model: 6B params * 18 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.552242",
      "metadata": {}
    },
    {
      "name": "rwkv_4_raven_14b",
      "developer": "RWKV",
      "release_date": null,
      "parameters": 14000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.76e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.82e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1044.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.82e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1044.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 14,000,000,000 params \u00d7 210,000,000,000 tokens = 1.76e+22 FLOP (Generic estimate: 14B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.552314",
      "metadata": {}
    },
    {
      "name": "chatglm2_6b",
      "developer": "Tsinghua",
      "release_date": null,
      "parameters": 6000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.89e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.76e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1033.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.76e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1033.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 6,000,000,000 params \u00d7 108,000,000,000 tokens = 3.89e+21 FLOP (Specialized model: 6B params * 18 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.552380",
      "metadata": {}
    },
    {
      "name": "oasst_pythia_12b",
      "developer": "OpenAssistant",
      "release_date": null,
      "parameters": 12000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.30e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.70e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1021.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.70e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1021.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 12,000,000,000 params \u00d7 180,000,000,000 tokens = 1.30e+22 FLOP (Generic estimate: 12B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.552453",
      "metadata": {}
    },
    {
      "name": "chatglm_6b",
      "developer": "Tsinghua",
      "release_date": null,
      "parameters": 6000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.89e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.62e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1004.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.62e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1004.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 6,000,000,000 params \u00d7 108,000,000,000 tokens = 3.89e+21 FLOP (Specialized model: 6B params * 18 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.552519",
      "metadata": {}
    },
    {
      "name": "fastchat_t5_3b",
      "developer": "LMSYS",
      "release_date": null,
      "parameters": 3000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.72e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.57e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 995.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.57e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 995.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 3,000,000,000 params \u00d7 54,000,000,000 tokens = 9.72e+20 FLOP (Specialized model: 3B params * 18 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.552589",
      "metadata": {}
    },
    {
      "name": "llama_13b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.11e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.49e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 978.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.49e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 978.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 104,000,000,000 tokens = 8.11e+21 FLOP (Early era model: 13B params * 8 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.552658",
      "metadata": {}
    },
    {
      "name": "dolly_v2_12b",
      "developer": "Databricks",
      "release_date": null,
      "parameters": 12000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.30e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.48e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 976.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.48e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 976.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 12,000,000,000 params \u00d7 180,000,000,000 tokens = 1.30e+22 FLOP (Generic estimate: 12B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.552725",
      "metadata": {}
    },
    {
      "name": "stablelm_tuned_alpha_7b",
      "developer": "Stability AI",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.40e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 956.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.40e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 956.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.552790",
      "metadata": {}
    },
    {
      "name": "claude_3.5_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": 250000000000,
      "parameter_source": "known_specification:claude_3.5_sonnet",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "1.50e+25",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Known model specification 'claude_3.5_sonnet': Chinchilla scaling law: 6 \u00d7 250,000,000,000 params \u00d7 10,000,000,000,000 tokens = 1.50e+25 FLOP"
        },
        {
          "flop": "3.96e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.11e+25 (Medium); superclue_math: 2.88e+25 (Medium); superclue_reasoning: 1.79e+25 (Low); superclue_code: 4.72e+25 (Medium); superclue_agents: 5.57e+25 (Medium) \u2192 weighted average: 3.96e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "superclue_overall": 65.01,
        "superclue_math": 58.45,
        "superclue_reasoning": 45.56,
        "superclue_code": 78.61,
        "superclue_agents": 77.31
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from benchmarks",
      "last_updated": "2025-08-04T20:15:51.552948",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_pro",
      "developer": "Google",
      "release_date": null,
      "parameters": 300000000000,
      "parameter_source": "known_specification:gemini_1.5_pro",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.16e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "olympic_overall": 35.09,
        "olympic_math": 21.05,
        "olympic_physics": 23.16,
        "olympic_chemistry": 39.74,
        "olympic_biology": 50.0
      },
      "sources": [
        "https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects)"
      ],
      "reasoning": "Known model specification 'gemini_1.5_pro': Chinchilla scaling law: 6 \u00d7 300,000,000,000 params \u00d7 12,000,000,000,000 tokens = 2.16e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.553187",
      "metadata": {}
    },
    {
      "name": "llama_3.1_405b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 405000000000,
      "parameter_source": "known_specification:llama_3.1_405b",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.80e+25",
      "training_flop_confidence": "high",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "3.65e+25",
          "confidence": "high",
          "method": "scaling_laws",
          "reasoning": "Known model specification 'llama_3.1_405b': Chinchilla scaling law: 6 \u00d7 405,000,000,000 params \u00d7 15,000,000,000,000 tokens = 3.65e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_above_1e25",
      "benchmarks": {
        "olympic_overall": 28.67,
        "olympic_math": 17.58,
        "olympic_physics": 18.56,
        "olympic_chemistry": 32.47,
        "olympic_biology": 40.32
      },
      "sources": [
        "https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: High-precision estimate from Meta disclosure",
      "last_updated": "2025-08-04T20:15:51.553422",
      "metadata": {}
    },
    {
      "name": "gemini_1.0_pro",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "olympic_overall": 24.12,
        "olympic_math": 13.28,
        "olympic_physics": 14.09,
        "olympic_chemistry": 26.62,
        "olympic_biology": 32.26
      },
      "sources": [
        "https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-04T20:15:44.642443",
      "metadata": {}
    },
    {
      "name": "glm",
      "developer": "Zhipu AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "6.32e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.28e+25 (Low); coding_score: 5.24e+25 (Low); aai_score: 1.00e+26 (Low); mmlu_pro_score: 5.28e+25 (Medium) \u2192 weighted average: 6.32e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1435.0,
        "coding_score": 1446.0,
        "aai_score": 65.7,
        "mmlu_pro_score": 83.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company (same as Zhipu) with limited transparency on model training details - Insufficient disclosure on training compute and methodology. Original estimate: 6.32e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.28e+25 (Low); coding_score: 5.24e+25 (Low); aai_score: 1.00e+26 (Low); mmlu_pro_score: 5.28e+25 (Medium) \u2192 weighted average: 6.32e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.553801",
      "metadata": {}
    },
    {
      "name": "gemini_2.0_pro",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.02e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1397.0,
        "coding_score": 1398.0,
        "vision_score": 1220.0,
        "aai_score": 49.2,
        "mmlu_pro_score": 80.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.87e+25 (Low); coding_score: 4.74e+25 (Medium); aai_score: 5.61e+25 (Medium); mmlu_pro_score: 4.82e+25 (Medium) \u2192 weighted average: 5.02e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.553954",
      "metadata": {}
    },
    {
      "name": "gemini_2.0_flash",
      "developer": "Google",
      "release_date": null,
      "parameters": 300000000000,
      "parameter_source": "known_specification:gemini_2.0_flash",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.24e+25",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.73e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.59e+25 (Medium); coding_score: 4.47e+25 (Medium); aai_score: 5.36e+25 (Medium); mmlu_pro_score: 4.48e+25 (Medium) \u2192 weighted average: 4.73e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1370.0,
        "coding_score": 1371.0,
        "vision_score": 1241.0,
        "aai_score": 48.1,
        "mmlu_pro_score": 78.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'gemini_2.0_flash': Chinchilla scaling law: 6 \u00d7 300,000,000,000 params \u00d7 18,000,000,000,000 tokens = 3.24e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.554431",
      "metadata": {}
    },
    {
      "name": "glm_4.5_air",
      "developer": "Zhipu AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "5.94e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.84e+25 (Low); coding_score: 4.97e+25 (Low); aai_score: 8.28e+25 (Medium); mmlu_pro_score: 4.97e+25 (Medium) \u2192 weighted average: 5.94e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1394.0,
        "coding_score": 1421.0,
        "aai_score": 59.8,
        "mmlu_pro_score": 81.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company (same as Zhipu) with limited transparency on model training details - Insufficient disclosure on training compute and methodology. Original estimate: 5.94e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.84e+25 (Low); coding_score: 4.97e+25 (Low); aai_score: 8.28e+25 (Medium); mmlu_pro_score: 4.97e+25 (Medium) \u2192 weighted average: 5.94e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.554860",
      "metadata": {}
    },
    {
      "name": "mistral_medium_3",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.74e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1370.0,
        "coding_score": 1388.0,
        "vision_score": 1196.0,
        "aai_score": 49.0,
        "mmlu_pro_score": 76.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.59e+25 (Medium); coding_score: 4.64e+25 (Medium); aai_score: 5.56e+25 (Medium); mmlu_pro_score: 4.17e+25 (Medium) \u2192 weighted average: 4.74e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.555023",
      "metadata": {}
    },
    {
      "name": "llama_3.3_nemotron_super_49b_v1.5",
      "developer": "Meta",
      "release_date": null,
      "parameters": 49000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "5.40e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.29e+25 (Medium); coding_score: 4.28e+25 (Medium); aai_score: 9.43e+25 (Low); mmlu_pro_score: 4.96e+25 (Medium) \u2192 weighted average: 5.40e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1339.0,
        "coding_score": 1352.0,
        "aai_score": 63.8,
        "mmlu_pro_score": 81.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 49,000,000,000 params \u00d7 980,000,000,000 tokens = 2.88e+23 FLOP (Modern model: 49B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.555326",
      "metadata": {}
    },
    {
      "name": "command_a",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.88e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.17e+25 (Medium); coding_score: 4.12e+25 (Medium); aai_score: 3.71e+25 (Medium); mmlu_pro_score: 3.55e+25 (Medium) \u2192 weighted average: 3.88e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1326.0,
        "coding_score": 1334.0,
        "aai_score": 40.0,
        "mmlu_pro_score": 71.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Canadian AI company with reasonable disclosure standards and research focus. Original estimate: 3.88e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.17e+25 (Medium); coding_score: 4.12e+25 (Medium); aai_score: 3.71e+25 (Medium); mmlu_pro_score: 3.55e+25 (Medium) \u2192 weighted average: 3.88e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.555621",
      "metadata": {}
    },
    {
      "name": "amazon_nova_chat_05_14",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.07e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1322.0,
        "coding_score": 1335.0,
        "aai_score": 42.6,
        "mmlu_pro_score": 73.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.13e+25 (Medium); coding_score: 4.12e+25 (Medium); aai_score: 4.20e+25 (Medium); mmlu_pro_score: 3.81e+25 (Medium) \u2192 weighted average: 4.07e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.555908",
      "metadata": {}
    },
    {
      "name": "claude_3.7_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.57e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1301.0,
        "coding_score": 1341.0,
        "vision_score": 1195.0,
        "aai_score": 48.2,
        "mmlu_pro_score": 80.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.93e+25 (Medium); coding_score: 4.18e+25 (Medium); aai_score: 5.38e+25 (Medium); mmlu_pro_score: 4.79e+25 (Medium) \u2192 weighted average: 4.57e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.556278",
      "metadata": {}
    },
    {
      "name": "llama_3.3_nemotron_super_49b_v1",
      "developer": "Meta",
      "release_date": null,
      "parameters": 49000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.65e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.02e+25 (Medium); coding_score: 3.99e+25 (Medium); aai_score: 6.07e+25 (Medium); mmlu_pro_score: 4.53e+25 (Medium) \u2192 weighted average: 4.65e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1310.0,
        "coding_score": 1320.0,
        "aai_score": 51.2,
        "mmlu_pro_score": 78.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 49,000,000,000 params \u00d7 980,000,000,000 tokens = 2.88e+23 FLOP (Modern model: 49B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.556587",
      "metadata": {}
    },
    {
      "name": "grok_2_08_13",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.71e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1306.0,
        "coding_score": 1298.0,
        "aai_score": 39.2,
        "mmlu_pro_score": 70.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.98e+25 (Medium); coding_score: 3.79e+25 (Medium); aai_score: 3.56e+25 (Medium); mmlu_pro_score: 3.51e+25 (Medium) \u2192 weighted average: 3.71e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.556731",
      "metadata": {}
    },
    {
      "name": "athene_v2_chat_72b",
      "developer": "NexusFlow",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.60e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.93e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.87e+25 (Medium); coding_score: 3.99e+25 (Medium) \u2192 weighted average: 3.93e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1294.0,
        "coding_score": 1320.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 1,296,000,000,000 tokens = 5.60e+23 FLOP (Specialized model: 72B params * 18 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.556998",
      "metadata": {}
    },
    {
      "name": "yi_lightning_lite",
      "developer": "01 AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.73e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.78e+25 (Medium); coding_score: 3.69e+25 (Medium) \u2192 weighted average: 3.73e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1284.0,
        "coding_score": 1286.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company founded by Kai-Fu Lee, relatively new but has some disclosure. Original estimate: 3.73e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.78e+25 (Medium); coding_score: 3.69e+25 (Medium) \u2192 weighted average: 3.73e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.557232",
      "metadata": {}
    },
    {
      "name": "grok_2_mini_08_13",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.70e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1283.0,
        "coding_score": 1279.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.77e+25 (Medium); coding_score: 3.63e+25 (Medium) \u2192 weighted average: 3.70e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.557359",
      "metadata": {}
    },
    {
      "name": "claude_3.5_haiku",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.97e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 56.12,
        "superclue_math": 49.26,
        "superclue_reasoning": 33.83,
        "superclue_code": 66.93,
        "superclue_agents": 74.48
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.84e+25 (Medium); superclue_math: 1.88e+25 (Low); superclue_reasoning: 8.52e+24 (Low); superclue_code: 3.15e+25 (Medium); superclue_agents: 5.07e+25 (Medium) \u2192 weighted average: 2.97e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.557637",
      "metadata": {}
    },
    {
      "name": "deepseek_v2_api",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.44e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1240.0,
        "coding_score": 1260.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.41e+25 (Medium); coding_score: 3.47e+25 (Medium) \u2192 weighted average: 3.44e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.557854",
      "metadata": {}
    },
    {
      "name": "yi_large",
      "developer": "01 AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.60e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.37e+25 (Low); coding_score: 3.29e+25 (Medium); aai_score: 1.80e+25 (Medium); mmlu_pro_score: 2.18e+25 (Medium) \u2192 weighted average: 2.60e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1236.0,
        "coding_score": 1238.0,
        "aai_score": 27.9,
        "mmlu_pro_score": 58.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company founded by Kai-Fu Lee, relatively new but has some disclosure. Original estimate: 2.60e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.37e+25 (Low); coding_score: 3.29e+25 (Medium); aai_score: 1.80e+25 (Medium); mmlu_pro_score: 2.18e+25 (Medium) \u2192 weighted average: 2.60e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.558231",
      "metadata": {}
    },
    {
      "name": "aya_expanse_32b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.25e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.32e+25 (Medium); coding_score: 3.08e+25 (Medium); aai_score: 9.36e+24 (Low); mmlu_pro_score: 7.23e+24 (Low) \u2192 weighted average: 2.25e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1229.0,
        "coding_score": 1211.0,
        "aai_score": 20.1,
        "mmlu_pro_score": 37.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.558367",
      "metadata": {}
    },
    {
      "name": "aya_expanse_8b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.88e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.03e+25 (Medium); coding_score: 2.88e+25 (Low); aai_score: 5.93e+24 (Low); mmlu_pro_score: 4.51e+24 (Low) \u2192 weighted average: 1.88e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1193.0,
        "coding_score": 1184.0,
        "aai_score": 16.0,
        "mmlu_pro_score": 31.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.558489",
      "metadata": {}
    },
    {
      "name": "claude_1",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.82e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1178.0,
        "coding_score": 1161.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.92e+25 (Low); coding_score: 2.71e+25 (Low) \u2192 weighted average: 2.82e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.558576",
      "metadata": {}
    },
    {
      "name": "internlm2.5_20b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 20000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.84e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.85e+25 (Low); coding_score: 2.84e+25 (Low) \u2192 weighted average: 2.84e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1168.0,
        "coding_score": 1179.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 20,000,000,000 params \u00d7 300,000,000,000 tokens = 3.60e+22 FLOP (Generic estimate: 20B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.558804",
      "metadata": {}
    },
    {
      "name": "gemini_1.0_pro_001",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.61e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1155.0,
        "coding_score": 1125.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.75e+25 (Low); coding_score: 2.47e+25 (Low) \u2192 weighted average: 2.61e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.558889",
      "metadata": {}
    },
    {
      "name": "claude_instant_1",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.58e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1135.0,
        "coding_score": 1136.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.61e+25 (Low); coding_score: 2.54e+25 (Low) \u2192 weighted average: 2.58e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.559084",
      "metadata": {}
    },
    {
      "name": "nv_llama2_70b_steerlm",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.20e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.42e+25 (Low); coding_score: 1.99e+25 (Low) \u2192 weighted average: 2.20e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1106.0,
        "coding_score": 1047.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.559348",
      "metadata": {}
    },
    {
      "name": "pplx_70b_online",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.20e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.37e+25 (Low); coding_score: 2.04e+25 (Low) \u2192 weighted average: 2.20e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1099.0,
        "coding_score": 1055.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.559456",
      "metadata": {}
    },
    {
      "name": "phi_3_mini_4k_instruct_june_24",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.30e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.30e+25 (Low); coding_score: 2.29e+25 (Low) \u2192 weighted average: 2.30e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1088.0,
        "coding_score": 1098.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: So far they are not generating large proprietary models. Original estimate: 2.30e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.30e+25 (Low); coding_score: 2.29e+25 (Low) \u2192 weighted average: 2.30e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.559638",
      "metadata": {}
    },
    {
      "name": "qwen2.5_vl_32b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.37e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1198.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 384,000,000,000 tokens = 7.37e+22 FLOP (Mid-era model: 32B params * 12 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.559729",
      "metadata": {}
    },
    {
      "name": "step_1o_vision_32k",
      "developer": "StepFun",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1169.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-04T20:15:45.210126",
      "metadata": {}
    },
    {
      "name": "qwen2.5_vl_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.73e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1154.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 864,000,000,000 tokens = 3.73e+23 FLOP (Mid-era model: 72B params * 12 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.559814",
      "metadata": {}
    },
    {
      "name": "pixtral_large",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.33e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "vision_score": 1138.0,
        "aai_score": 37.4,
        "mmlu_pro_score": 70.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: aai_score: 3.24e+25 (Medium); mmlu_pro_score: 3.41e+25 (Medium) \u2192 weighted average: 3.33e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.559918",
      "metadata": {}
    },
    {
      "name": "qwen_vl_max",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1106.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-04T20:15:45.210216",
      "metadata": {}
    },
    {
      "name": "qwen2_vl_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.73e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1094.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 864,000,000,000 tokens = 3.73e+23 FLOP (Mid-era model: 72B params * 12 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.560172",
      "metadata": {}
    },
    {
      "name": "step_1v_32k",
      "developer": "StepFun",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1093.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-04T20:15:45.210270",
      "metadata": {}
    },
    {
      "name": "molmo_72b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.67e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1060.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 1,080,000,000,000 tokens = 4.67e+23 FLOP (Generic estimate: 72B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.560244",
      "metadata": {}
    },
    {
      "name": "pixtral_12b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 12000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.30e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.27e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: aai_score: 1.27e+25 (Medium); mmlu_pro_score: 1.28e+25 (Low) \u2192 weighted average: 1.27e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1056.0,
        "aai_score": 23.4,
        "mmlu_pro_score": 47.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 12,000,000,000 params \u00d7 180,000,000,000 tokens = 1.30e+22 FLOP (Generic estimate: 12B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.560340",
      "metadata": {}
    },
    {
      "name": "internvl2_26b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 26000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "6.08e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1053.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 26,000,000,000 params \u00d7 390,000,000,000 tokens = 6.08e+22 FLOP (Generic estimate: 26B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.560392",
      "metadata": {}
    },
    {
      "name": "llama_3.2_90b_vision",
      "developer": "Meta",
      "release_date": null,
      "parameters": 90000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.72e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.82e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: aai_score: 2.58e+25 (Medium); mmlu_pro_score: 3.06e+25 (Medium) \u2192 weighted average: 2.82e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1047.0,
        "aai_score": 33.4,
        "mmlu_pro_score": 67.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 90,000,000,000 params \u00d7 1,800,000,000,000 tokens = 9.72e+23 FLOP (Modern model: 90B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.560477",
      "metadata": {}
    },
    {
      "name": "hunyuan_standard_vision",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1046.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-04T20:15:45.210423",
      "metadata": {}
    },
    {
      "name": "aya_vision_32b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1042.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.560553",
      "metadata": {}
    },
    {
      "name": "qwen2_vl_7b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1038.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.560598",
      "metadata": {}
    },
    {
      "name": "yi_vision",
      "developer": "01 AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1028.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-04T20:15:45.210498",
      "metadata": {}
    },
    {
      "name": "llama_3.2_11b_vision",
      "developer": "Meta",
      "release_date": null,
      "parameters": 11000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.45e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.36e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: aai_score: 1.45e+25 (Medium); mmlu_pro_score: 1.22e+25 (Low) \u2192 weighted average: 1.36e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1014.0,
        "aai_score": 25.0,
        "mmlu_pro_score": 46.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 11,000,000,000 params \u00d7 220,000,000,000 tokens = 1.45e+22 FLOP (Modern model: 11B params * 20 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.560726",
      "metadata": {}
    },
    {
      "name": "molmo_7b_d",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1007.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.560822",
      "metadata": {}
    },
    {
      "name": "videopoet",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "1.10e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Threshold-based (physics_iq_score): 20.3 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level)"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "physics_iq_score": 20.3
      },
      "sources": [
        "https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Models with unidentified developers cannot be properly evaluated - Cannot verify training methodology or model provenance. Original estimate: 1.10e+25 FLOP (Threshold-based (physics_iq_score): 20.3 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level))",
      "last_updated": "2025-08-04T20:15:51.561005",
      "metadata": {}
    },
    {
      "name": "lumiere",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "1.10e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Threshold-based (physics_iq_score): 19.0 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level)"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "physics_iq_score": 19.0
      },
      "sources": [
        "https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Models with unidentified developers cannot be properly evaluated - Cannot verify training methodology or model provenance. Original estimate: 1.10e+25 FLOP (Threshold-based (physics_iq_score): 19.0 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level))",
      "last_updated": "2025-08-04T20:15:51.561177",
      "metadata": {}
    },
    {
      "name": "runway_gen_3",
      "developer": "Runway",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.10e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "physics_iq_score": 22.8
      },
      "sources": [
        "https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models)"
      ],
      "reasoning": "Threshold-based (physics_iq_score): 22.8 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level)",
      "last_updated": "2025-08-04T20:15:51.561246",
      "metadata": {}
    },
    {
      "name": "stable_video_diffusion",
      "developer": "Stability AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.10e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "physics_iq_score": 14.8
      },
      "sources": [
        "https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models)"
      ],
      "reasoning": "Threshold-based (physics_iq_score): 14.8 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level)",
      "last_updated": "2025-08-04T20:15:51.561396",
      "metadata": {}
    },
    {
      "name": "pika",
      "developer": "Pika Labs",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "1.10e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Threshold-based (physics_iq_score): 13.0 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level)"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "physics_iq_score": 13.0
      },
      "sources": [
        "https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Insufficient disclosure on training methodology and specifications. Original estimate: 1.10e+25 FLOP (Threshold-based (physics_iq_score): 13.0 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level))",
      "last_updated": "2025-08-04T20:15:51.561623",
      "metadata": {}
    },
    {
      "name": "sora",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.00e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "video_arena_elo": 1234.0,
        "video_quality": 84.3
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 1.00e+25 (Medium); video_quality: 1.00e+25 (Medium) \u2192 weighted average: 1.00e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.561719",
      "metadata": {}
    },
    {
      "name": "doubao_seed_1.6",
      "developer": "ByteDance",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "5.15e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.60e+25 (Medium); superclue_math: 4.64e+25 (Medium); superclue_reasoning: 1.33e+25 (Low); superclue_code: 5.63e+25 (Medium); superclue_agents: 8.29e+25 (Medium) \u2192 weighted average: 5.15e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 68.04,
        "superclue_math": 70.77,
        "superclue_reasoning": 40.49,
        "superclue_code": 84.36,
        "superclue_agents": 90.67
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese tech company with limited transparency on AI model training specifics - Limited disclosure on training methodology and data sources for frontier models. Original estimate: 5.15e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.60e+25 (Medium); superclue_math: 4.64e+25 (Medium); superclue_reasoning: 1.33e+25 (Low); superclue_code: 5.63e+25 (Medium); superclue_agents: 8.29e+25 (Medium) \u2192 weighted average: 5.15e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.562020",
      "metadata": {}
    },
    {
      "name": "claude_opus_4_reasoning",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.27e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 67.02,
        "superclue_math": 60.16,
        "superclue_reasoning": 45.93,
        "superclue_code": 80.4,
        "superclue_agents": 80.6
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.43e+25 (Medium); superclue_math: 3.09e+25 (Medium); superclue_reasoning: 1.83e+25 (Low); superclue_code: 4.99e+25 (Medium); superclue_agents: 6.18e+25 (Medium) \u2192 weighted average: 4.27e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.562187",
      "metadata": {}
    },
    {
      "name": "hunyuan_pro",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.78e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.78e+25 (Medium); superclue_math: 2.88e+25 (Medium); superclue_reasoning: 1.06e+25 (Low); superclue_code: 5.24e+25 (Medium); superclue_agents: 5.03e+25 (Medium) \u2192 weighted average: 3.78e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 62.91,
        "superclue_math": 58.51,
        "superclue_reasoning": 36.91,
        "superclue_code": 81.98,
        "superclue_agents": 74.23
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Major Chinese tech company but with limited transparency on AI model training specifics - Limited disclosure on training methodology and compute for frontier AI models. Original estimate: 3.78e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.78e+25 (Medium); superclue_math: 2.88e+25 (Medium); superclue_reasoning: 1.06e+25 (Low); superclue_code: 5.24e+25 (Medium); superclue_agents: 5.03e+25 (Medium) \u2192 weighted average: 3.78e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.562534",
      "metadata": {}
    },
    {
      "name": "doubao_pro",
      "developer": "ByteDance",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.63e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.73e+25 (Medium); superclue_math: 3.13e+25 (Medium); superclue_reasoning: 1.55e+25 (Low); superclue_code: 3.89e+25 (Medium); superclue_agents: 5.15e+25 (Medium) \u2192 weighted average: 3.63e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 62.53,
        "superclue_math": 60.45,
        "superclue_reasoning": 42.96,
        "superclue_code": 72.77,
        "superclue_agents": 74.93
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese tech company with limited transparency on AI model training specifics - Limited disclosure on training methodology and data sources for frontier models. Original estimate: 3.63e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.73e+25 (Medium); superclue_math: 3.13e+25 (Medium); superclue_reasoning: 1.55e+25 (Low); superclue_code: 3.89e+25 (Medium); superclue_agents: 5.15e+25 (Medium) \u2192 weighted average: 3.63e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.563529",
      "metadata": {}
    },
    {
      "name": "step_2_16k",
      "developer": "StepFun",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.57e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.61e+25 (Medium); superclue_math: 3.99e+25 (Medium); superclue_reasoning: 1.10e+25 (Low); superclue_code: 3.52e+25 (Medium); superclue_agents: 4.83e+25 (Medium) \u2192 weighted average: 3.57e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 61.74,
        "superclue_math": 66.61,
        "superclue_reasoning": 37.41,
        "superclue_code": 69.9,
        "superclue_agents": 73.04
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company with very limited public information on model training - Extremely limited disclosure on training compute, methodology, and specifications. Original estimate: 3.57e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.61e+25 (Medium); superclue_math: 3.99e+25 (Medium); superclue_reasoning: 1.10e+25 (Low); superclue_code: 3.52e+25 (Medium); superclue_agents: 4.83e+25 (Medium) \u2192 weighted average: 3.57e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.563960",
      "metadata": {}
    },
    {
      "name": "minimax_01",
      "developer": "MiniMax",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.60e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.55e+25 (Medium); superclue_math: 2.48e+25 (Low); superclue_reasoning: 1.18e+25 (Low); superclue_code: 4.23e+25 (Medium); superclue_agents: 5.39e+25 (Medium) \u2192 weighted average: 3.60e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 61.29,
        "superclue_math": 55.09,
        "superclue_reasoning": 38.52,
        "superclue_code": 75.25,
        "superclue_agents": 76.31
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company with very limited transparency on model specifications - Insufficient disclosure on training methodology, data sources, and model architecture. Original estimate: 3.60e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.55e+25 (Medium); superclue_math: 2.48e+25 (Low); superclue_reasoning: 1.18e+25 (Low); superclue_code: 4.23e+25 (Medium); superclue_agents: 5.39e+25 (Medium) \u2192 weighted average: 3.60e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.564399",
      "metadata": {}
    },
    {
      "name": "moonshot",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.44e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.49e+25 (Medium); superclue_math: 2.90e+25 (Medium); superclue_reasoning: 1.24e+25 (Low); superclue_code: 3.42e+25 (Medium); superclue_agents: 5.44e+25 (Medium) \u2192 weighted average: 3.44e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 60.89,
        "superclue_math": 58.63,
        "superclue_reasoning": 39.26,
        "superclue_code": 69.11,
        "superclue_agents": 76.57
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Models with unidentified developers cannot be properly evaluated - Cannot verify training methodology or model provenance. Original estimate: 3.44e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.49e+25 (Medium); superclue_math: 2.90e+25 (Medium); superclue_reasoning: 1.24e+25 (Low); superclue_code: 3.42e+25 (Medium); superclue_agents: 5.44e+25 (Medium) \u2192 weighted average: 3.44e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.565133",
      "metadata": {}
    },
    {
      "name": "baichuan_4",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.25e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.30e+25 (Medium); superclue_math: 2.91e+25 (Medium); superclue_reasoning: 1.06e+25 (Low); superclue_code: 3.67e+25 (Medium); superclue_agents: 4.58e+25 (Medium) \u2192 weighted average: 3.25e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 59.54,
        "superclue_math": 58.68,
        "superclue_reasoning": 36.91,
        "superclue_code": 71.09,
        "superclue_agents": 71.48
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Models with unidentified developers cannot be properly evaluated - Cannot verify training methodology or model provenance. Original estimate: 3.25e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.30e+25 (Medium); superclue_math: 2.91e+25 (Medium); superclue_reasoning: 1.06e+25 (Low); superclue_code: 3.67e+25 (Medium); superclue_agents: 4.58e+25 (Medium) \u2192 weighted average: 3.25e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.565496",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_flash",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.11e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 57.89,
        "superclue_math": 54.55,
        "superclue_reasoning": 35.56,
        "superclue_code": 68.91,
        "superclue_agents": 72.56
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.07e+25 (Medium); superclue_math: 2.42e+25 (Low); superclue_reasoning: 9.65e+24 (Low); superclue_code: 3.39e+25 (Medium); superclue_agents: 4.75e+25 (Medium) \u2192 weighted average: 3.11e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.565680",
      "metadata": {}
    },
    {
      "name": "internlm_3",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.80e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.82e+25 (Medium); superclue_math: 3.10e+25 (Medium); superclue_reasoning: 1.18e+25 (Low); superclue_code: 2.11e+25 (Low); superclue_agents: 4.04e+25 (Medium) \u2192 weighted average: 2.80e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 55.93,
        "superclue_math": 60.22,
        "superclue_reasoning": 38.52,
        "superclue_code": 57.03,
        "superclue_agents": 67.97
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Models with unidentified developers cannot be properly evaluated - Cannot verify training methodology or model provenance. Original estimate: 2.80e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.82e+25 (Medium); superclue_math: 3.10e+25 (Medium); superclue_reasoning: 1.18e+25 (Low); superclue_code: 2.11e+25 (Low); superclue_agents: 4.04e+25 (Medium) \u2192 weighted average: 2.80e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.566045",
      "metadata": {}
    },
    {
      "name": "glm_4_flash",
      "developer": "Zhipu AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.80e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.70e+25 (Low); superclue_math: 1.93e+25 (Low); superclue_reasoning: 7.05e+24 (Low); superclue_code: 3.67e+25 (Medium); superclue_agents: 3.98e+25 (Medium) \u2192 weighted average: 2.80e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 54.97,
        "superclue_math": 49.85,
        "superclue_reasoning": 31.36,
        "superclue_code": 71.09,
        "superclue_agents": 67.56
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company (same as Zhipu) with limited transparency on model training details - Insufficient disclosure on training compute and methodology. Original estimate: 2.80e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.70e+25 (Low); superclue_math: 1.93e+25 (Low); superclue_reasoning: 7.05e+24 (Low); superclue_code: 3.67e+25 (Medium); superclue_agents: 3.98e+25 (Medium) \u2192 weighted average: 2.80e+25 FLOP)",
      "last_updated": "2025-08-04T20:15:51.566311",
      "metadata": {}
    },
    {
      "name": "marco_o1",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.72e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 54.76,
        "superclue_math": 60.97,
        "superclue_reasoning": 39.75,
        "superclue_code": 50.59,
        "superclue_agents": 67.73
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.68e+25 (Low); superclue_math: 3.20e+25 (Medium); superclue_reasoning: 1.27e+25 (Low); superclue_code: 1.57e+25 (Low); superclue_agents: 4.00e+25 (Medium) \u2192 weighted average: 2.72e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.566550",
      "metadata": {}
    },
    {
      "name": "mixtral_8x22b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 22000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.36e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.20e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.22e+25 (Low); superclue_math: 1.98e+25 (Low); superclue_reasoning: 9.73e+24 (Low); superclue_code: 1.84e+25 (Low); superclue_agents: 3.40e+25 (Medium) \u2192 weighted average: 2.20e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 50.86,
        "superclue_math": 50.33,
        "superclue_reasoning": 35.68,
        "superclue_code": 53.96,
        "superclue_agents": 63.46
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 22,000,000,000 params \u00d7 330,000,000,000 tokens = 4.36e+22 FLOP (Generic estimate: 22B params * 15 tokens/param)",
      "last_updated": "2025-08-04T20:15:51.566923",
      "metadata": {}
    },
    {
      "name": "veo_3_preview",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.10e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "video_arena_elo": 1298.0,
        "video_quality": 89.2
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 1.10e+25 (Medium); video_quality: 1.10e+25 (Medium) \u2192 weighted average: 1.10e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.567021",
      "metadata": {}
    },
    {
      "name": "veo_2",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.10e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "video_arena_elo": 1267.0,
        "video_quality": 86.7
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 1.10e+25 (Medium); video_quality: 1.10e+25 (Medium) \u2192 weighted average: 1.10e+25 FLOP",
      "last_updated": "2025-08-04T20:15:51.567195",
      "metadata": {}
    },
    {
      "name": "seedance",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.25e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "video_arena_elo": 1205.0,
        "video_quality": 81.5
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 9.31e+24 (Low); video_quality: 9.19e+24 (Low) \u2192 weighted average: 9.25e+24 FLOP",
      "last_updated": "2025-08-04T20:15:51.567458",
      "metadata": {}
    },
    {
      "name": "waver",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "8.81e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "video_arena_elo": 1187.0,
        "video_quality": 79.8
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 8.90e+24 (Low); video_quality: 8.72e+24 (Low) \u2192 weighted average: 8.81e+24 FLOP",
      "last_updated": "2025-08-04T20:15:51.567551",
      "metadata": {}
    },
    {
      "name": "runway_gen3",
      "developer": "Runway",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "7.87e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "video_arena_elo": 1156.0,
        "video_quality": 75.2
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 8.22e+24 (Low); video_quality: 7.52e+24 (Low) \u2192 weighted average: 7.87e+24 FLOP",
      "last_updated": "2025-08-04T20:15:51.567633",
      "metadata": {}
    },
    {
      "name": "kling",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "7.35e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "video_arena_elo": 1134.0,
        "video_quality": 72.8
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 7.76e+24 (Low); video_quality: 6.93e+24 (Low) \u2192 weighted average: 7.35e+24 FLOP",
      "last_updated": "2025-08-04T20:15:51.567716",
      "metadata": {}
    }
  ]
}