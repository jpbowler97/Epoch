{
  "metadata": {
    "saved_at": "2025-08-15T18:13:34.279849",
    "source": "consolidated_estimated",
    "last_updated": "2025-08-15T18:13:34.279819",
    "model_count": 282,
    "stage": "estimated"
  },
  "models": [
    {
      "name": "gemini_2.5_pro",
      "developer": "Google",
      "release_date": null,
      "parameters": 800000000000,
      "parameter_source": "known_specification:gemini_2.5_pro",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.20e+26",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.86e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.76e+25 (Low); superclue_math: 5.20e+25 (Low); superclue_reasoning: 2.57e+25 (Low); superclue_code: 5.30e+25 (Low); superclue_agents: 6.47e+25 (Low) \u2192 weighted average: 4.86e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 68.98,
        "superclue_math": 74.05,
        "superclue_reasoning": 52.59,
        "superclue_code": 82.38,
        "superclue_agents": 82.09
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Known model specification 'gemini_2.5_pro': Chinchilla scaling law: 6 \u00d7 800,000,000,000 params \u00d7 25,000,000,000,000 tokens = 1.20e+26 FLOP",
      "last_updated": "2025-08-15T18:13:34.218862",
      "metadata": {}
    },
    {
      "name": "o3",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.33e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 73.78,
        "superclue_math": 81.6,
        "superclue_reasoning": 59.7,
        "superclue_code": 83.76,
        "superclue_agents": 76.12
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 5.64e+25 (Low); superclue_math: 6.62e+25 (Low); superclue_reasoning: 3.52e+25 (Low); superclue_code: 5.53e+25 (Low); superclue_agents: 5.36e+25 (Low) \u2192 weighted average: 5.33e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.219065",
      "metadata": {}
    },
    {
      "name": "chatgpt_4o",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.70e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1427.0,
        "coding_score": 1434.0,
        "vision_score": 1296.0,
        "aai_score": 40.0,
        "mmlu_pro_score": 80.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.19e+25 (Low); coding_score: 5.11e+25 (Low); aai_score: 3.71e+25 (Low); mmlu_pro_score: 4.79e+25 (Low) \u2192 weighted average: 4.70e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.219207",
      "metadata": {}
    },
    {
      "name": "gpt_4.5_preview",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "6.40e+25",
      "training_flop_confidence": "low",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "4.75e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.07e+25 (Low); coding_score: 4.95e+25 (Low); aai_score: 4.09e+25 (Low); mmlu_pro_score: 4.89e+25 (Low) \u2192 weighted average: 4.75e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1416.0,
        "coding_score": 1419.0,
        "vision_score": 1239.0,
        "aai_score": 42.0,
        "mmlu_pro_score": 81.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate for GPT-4.5",
      "last_updated": "2025-08-15T18:13:34.219346",
      "metadata": {}
    },
    {
      "name": "grok_4",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "6.75e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1437.0,
        "coding_score": 1442.0,
        "vision_score": 1273.0,
        "aai_score": 68.0,
        "mmlu_pro_score": 86.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.30e+25 (Low); coding_score: 5.20e+25 (Low); aai_score: 1.07e+26 (Low); mmlu_pro_score: 5.78e+25 (Low) \u2192 weighted average: 6.75e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.219466",
      "metadata": {}
    },
    {
      "name": "claude_opus_4_thinking_16k",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.58e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1421.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1421.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 4.58e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.219559",
      "metadata": {}
    },
    {
      "name": "kimi_k2_preview",
      "developer": "Moonshot",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "5.05e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.71e+25 (Low); coding_score: 4.81e+25 (Low); aai_score: 5.56e+25 (Low); mmlu_pro_score: 5.11e+25 (Low) \u2192 weighted average: 5.05e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1381.0,
        "coding_score": 1405.0,
        "aai_score": 49.0,
        "mmlu_pro_score": 82.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Startup with limited computational budget for frontier model training",
      "last_updated": "2025-08-15T18:13:34.219744",
      "metadata": {}
    },
    {
      "name": "deepseek_r1",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": 671000000000,
      "parameter_source": "known_specification:deepseek_r1",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.00e+24",
      "training_flop_confidence": "high",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "6.04e+25",
          "confidence": "low",
          "method": "scaling_laws",
          "reasoning": "Known model specification 'deepseek_r1': Chinchilla scaling law: 6 \u00d7 671,000,000,000 params \u00d7 15,000,000,000,000 tokens = 6.04e+25 FLOP"
        },
        {
          "flop": "5.10e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.62e+25 (Low); coding_score: 4.58e+25 (Low); aai_score: 5.79e+25 (Low); mmlu_pro_score: 5.42e+25 (Low) \u2192 weighted average: 5.10e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1373.0,
        "coding_score": 1382.0,
        "aai_score": 50.0,
        "mmlu_pro_score": 84.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1",
      "last_updated": "2025-08-15T18:13:34.219858",
      "metadata": {}
    },
    {
      "name": "claude_opus_4",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.50e+26",
      "training_flop_confidence": "low",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "5.04e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.55e+25 (Low); coding_score: 4.80e+25 (Low); aai_score: 5.12e+25 (Low); mmlu_pro_score: 5.69e+25 (Low) \u2192 weighted average: 5.04e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1366.0,
        "coding_score": 1404.0,
        "vision_score": 1213.0,
        "aai_score": 47.0,
        "mmlu_pro_score": 86.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: Speculative estimate for next-gen Claude",
      "last_updated": "2025-08-15T18:13:34.219934",
      "metadata": {}
    },
    {
      "name": "grok_3",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.99e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1423.0,
        "coding_score": 1439.0,
        "aai_score": 46.0,
        "mmlu_pro_score": 79.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.15e+25 (Low); coding_score: 5.17e+25 (Low); aai_score: 4.90e+25 (Low); mmlu_pro_score: 4.73e+25 (Low) \u2192 weighted average: 4.99e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.220059",
      "metadata": {}
    },
    {
      "name": "gemini_2.5_flash",
      "developer": "Google",
      "release_date": null,
      "parameters": 400000000000,
      "parameter_source": "known_specification:gemini_2.5_flash",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.80e+25",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "5.76e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.04e+25 (Low); coding_score: 4.95e+25 (Low); aai_score: 7.79e+25 (Low); mmlu_pro_score: 5.23e+25 (Low) \u2192 weighted average: 5.76e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1413.0,
        "coding_score": 1419.0,
        "vision_score": 1278.0,
        "aai_score": 58.0,
        "mmlu_pro_score": 83.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'gemini_2.5_flash': Chinchilla scaling law: 6 \u00d7 400,000,000,000 params \u00d7 20,000,000,000,000 tokens = 4.80e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.220219",
      "metadata": {}
    },
    {
      "name": "gpt_4.1",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.84e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1378.0,
        "coding_score": 1396.0,
        "vision_score": 1255.0,
        "aai_score": 47.0,
        "mmlu_pro_score": 80.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.68e+25 (Low); coding_score: 4.72e+25 (Low); aai_score: 5.12e+25 (Low); mmlu_pro_score: 4.83e+25 (Low) \u2192 weighted average: 4.84e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.220338",
      "metadata": {}
    },
    {
      "name": "claude_sonnet_4_thinking_32k",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.38e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1400.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1400.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 4.38e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.220423",
      "metadata": {}
    },
    {
      "name": "o1",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.18e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1366.0,
        "coding_score": 1378.0,
        "vision_score": 1216.0,
        "aai_score": 52.0,
        "mmlu_pro_score": 84.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.55e+25 (Low); coding_score: 4.54e+25 (Low); aai_score: 6.26e+25 (Low); mmlu_pro_score: 5.38e+25 (Low) \u2192 weighted average: 5.18e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.220536",
      "metadata": {}
    },
    {
      "name": "qwen3_235b_a22b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 235000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.97e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "6.30e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.07e+25 (Low); coding_score: 5.24e+25 (Low); aai_score: 9.49e+25 (Low); mmlu_pro_score: 5.41e+25 (Low) \u2192 weighted average: 6.30e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1416.0,
        "coding_score": 1446.0,
        "aai_score": 64.0,
        "mmlu_pro_score": 84.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 235,000,000,000 params \u00d7 3,525,000,000,000 tokens = 4.97e+24 FLOP (Modern large model: 235B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.220678",
      "metadata": {}
    },
    {
      "name": "o4_mini",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "5.47e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 5.55e+25 (Low); superclue_math: 5.61e+25 (Low); superclue_reasoning: 2.94e+25 (Low); superclue_code: 5.93e+25 (Low); superclue_agents: 7.34e+25 (Low) \u2192 weighted average: 5.47e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 73.32,
        "superclue_math": 76.34,
        "superclue_reasoning": 55.56,
        "superclue_code": 86.14,
        "superclue_agents": 86.36
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Models with unidentified developers cannot be evaluated for resource availability",
      "last_updated": "2025-08-15T18:13:34.220847",
      "metadata": {}
    },
    {
      "name": "qwen3_coder_480b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 480000000000,
      "parameter_source": "known_specification:qwen3_coder_480b",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.32e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.63e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.47e+25 (Low); coding_score: 4.80e+25 (Low); aai_score: 4.69e+25 (Low); mmlu_pro_score: 4.57e+25 (Low) \u2192 weighted average: 4.63e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1358.0,
        "coding_score": 1404.0,
        "aai_score": 45.0,
        "mmlu_pro_score": 78.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'qwen3_coder_480b': Chinchilla scaling law: 6 \u00d7 480,000,000,000 params \u00d7 15,000,000,000,000 tokens = 4.32e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.220993",
      "metadata": {}
    },
    {
      "name": "deepseek_v3",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": 671000000000,
      "parameter_source": "known_specification:deepseek_v3",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.96e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.07e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.35e+25 (Low); superclue_math: 3.64e+25 (Low); superclue_reasoning: 1.47e+25 (Low); superclue_code: 5.76e+25 (Low); superclue_agents: 5.11e+25 (Low) \u2192 weighted average: 4.07e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "superclue_overall": 66.54,
        "superclue_math": 64.22,
        "superclue_reasoning": 42.1,
        "superclue_code": 85.15,
        "superclue_agents": 74.7
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Known model specification 'deepseek_v3': Chinchilla scaling law: 6 \u00d7 671,000,000,000 params \u00d7 14,800,000,000,000 tokens = 5.96e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.221217",
      "metadata": {}
    },
    {
      "name": "claude_sonnet_4",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.77e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1338.0,
        "coding_score": 1384.0,
        "vision_score": 1212.0,
        "aai_score": 46.0,
        "mmlu_pro_score": 83.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.28e+25 (Low); coding_score: 4.60e+25 (Low); aai_score: 4.90e+25 (Low); mmlu_pro_score: 5.31e+25 (Low) \u2192 weighted average: 4.77e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.221397",
      "metadata": {}
    },
    {
      "name": "claude_3_7_sonnet_thinking_32k",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.26e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1387.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1387.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 4.26e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.221520",
      "metadata": {}
    },
    {
      "name": "o1_preview",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.24e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1385.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1385.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 4.24e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.221640",
      "metadata": {}
    },
    {
      "name": "mistral_medium",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.84e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1171.0,
        "coding_score": 1172.0,
        "aai_score": 11.0,
        "mmlu_pro_score": 49.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.87e+25 (Low); coding_score: 2.79e+25 (Low); aai_score: 2.80e+24 (Low); mmlu_pro_score: 1.40e+25 (Low) \u2192 weighted average: 1.84e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.221812",
      "metadata": {}
    },
    {
      "name": "hunyuan_turbos",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "4.59e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 3 benchmarks: openlm_arena_elo: 4.67e+25 (Low); coding_score: 4.66e+25 (Low); mmlu_pro_score: 4.45e+25 (Low) \u2192 weighted average: 4.59e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1377.0,
        "coding_score": 1390.0,
        "mmlu_pro_score": 78.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited evidence of computational budget allocation for frontier AI models",
      "last_updated": "2025-08-15T18:13:34.222074",
      "metadata": {}
    },
    {
      "name": "minimax_m1",
      "developer": "MiniMax",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "5.09e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.43e+25 (Low); coding_score: 4.45e+25 (Low); aai_score: 6.51e+25 (Low); mmlu_pro_score: 4.99e+25 (Low) \u2192 weighted average: 5.09e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1353.0,
        "coding_score": 1369.0,
        "aai_score": 53.0,
        "mmlu_pro_score": 81.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited computational resources for 1e25+ FLOP training runs",
      "last_updated": "2025-08-15T18:13:34.222346",
      "metadata": {}
    },
    {
      "name": "gpt_4.1_mini",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.32e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1337.0,
        "coding_score": 1369.0,
        "vision_score": 1231.0,
        "aai_score": 42.0,
        "mmlu_pro_score": 78.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.27e+25 (Low); coding_score: 4.45e+25 (Low); aai_score: 4.09e+25 (Low); mmlu_pro_score: 4.47e+25 (Low) \u2192 weighted average: 4.32e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.222469",
      "metadata": {}
    },
    {
      "name": "qwen3_235b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 235000000000,
      "parameter_source": "known_specification:qwen3_235b",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.41e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.94e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.55e+25 (Low); coding_score: 4.70e+25 (Low); aai_score: 5.34e+25 (Low); mmlu_pro_score: 5.17e+25 (Low) \u2192 weighted average: 4.94e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1366.0,
        "coding_score": 1394.0,
        "aai_score": 48.0,
        "mmlu_pro_score": 82.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'qwen3_235b': Chinchilla scaling law: 6 \u00d7 235,000,000,000 params \u00d7 10,000,000,000,000 tokens = 1.41e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.222715",
      "metadata": {}
    },
    {
      "name": "qwen2.5_max",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.98e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1367.0,
        "coding_score": 1373.0,
        "aai_score": 34.0,
        "mmlu_pro_score": 76.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.56e+25 (Low); coding_score: 4.49e+25 (Low); aai_score: 2.68e+25 (Low); mmlu_pro_score: 4.20e+25 (Low) \u2192 weighted average: 3.98e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.222848",
      "metadata": {}
    },
    {
      "name": "claude_3_7_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.12e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1372.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1372.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 4.12e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.222937",
      "metadata": {}
    },
    {
      "name": "claude_3_5_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.84e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1340.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1340.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.84e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.223022",
      "metadata": {}
    },
    {
      "name": "gemini_2.0_flash_001",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.07e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1366.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1366.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 4.07e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.223122",
      "metadata": {}
    },
    {
      "name": "o3_mini",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.89e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1318.0,
        "coding_score": 1360.0,
        "aai_score": 53.0,
        "mmlu_pro_score": 79.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.09e+25 (Low); coding_score: 4.36e+25 (Low); aai_score: 6.51e+25 (Low); mmlu_pro_score: 4.61e+25 (Low) \u2192 weighted average: 4.89e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.223242",
      "metadata": {}
    },
    {
      "name": "gemma_3_27b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 27000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.75e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.30e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.46e+25 (Low); coding_score: 4.25e+25 (Low); aai_score: 1.45e+25 (Low); mmlu_pro_score: 3.03e+25 (Low) \u2192 weighted average: 3.30e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1357.0,
        "coding_score": 1348.0,
        "vision_score": 1216.0,
        "aai_score": 25.0,
        "mmlu_pro_score": 66.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 27,000,000,000 params \u00d7 540,000,000,000 tokens = 8.75e+22 FLOP (Modern model: 27B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.223386",
      "metadata": {}
    },
    {
      "name": "grok_3_mini",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.50e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1361.0,
        "coding_score": 1377.0,
        "aai_score": 58.0,
        "mmlu_pro_score": 82.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.50e+25 (Low); coding_score: 4.53e+25 (Low); aai_score: 7.79e+25 (Low); mmlu_pro_score: 5.17e+25 (Low) \u2192 weighted average: 5.50e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.223461",
      "metadata": {}
    },
    {
      "name": "llama_3.1_nemotron_ultra_253b_v1",
      "developer": "Meta",
      "release_date": null,
      "parameters": 253000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.59e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.12e+25 (Low); coding_score: 4.22e+25 (Low); aai_score: 4.90e+25 (Low); mmlu_pro_score: 5.12e+25 (Low) \u2192 weighted average: 4.59e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1321.0,
        "coding_score": 1345.0,
        "aai_score": 46.0,
        "mmlu_pro_score": 82.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 253,000,000,000 params \u00d7 3,795,000,000,000 tokens = 5.76e+24 FLOP (Modern large model: 253B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.223722",
      "metadata": {}
    },
    {
      "name": "gemini_2.0_flash_lite",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.54e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1331.0,
        "coding_score": 1338.0,
        "vision_score": 1147.0,
        "aai_score": 30.0,
        "mmlu_pro_score": 72.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.21e+25 (Low); coding_score: 4.15e+25 (Low); aai_score: 2.09e+25 (Low); mmlu_pro_score: 3.70e+25 (Low) \u2192 weighted average: 3.54e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.223823",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_pro_002",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.68e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1320.0,
        "coding_score": 1311.0,
        "vision_score": 1208.0,
        "aai_score": 34.0,
        "mmlu_pro_score": 75.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.11e+25 (Low); coding_score: 3.91e+25 (Low); aai_score: 2.68e+25 (Low); mmlu_pro_score: 4.04e+25 (Low) \u2192 weighted average: 3.68e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.224025",
      "metadata": {}
    },
    {
      "name": "mistral_small",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.54e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1335.0,
        "coding_score": 1361.0,
        "vision_score": 1186.0,
        "aai_score": 32.0,
        "mmlu_pro_score": 68.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.25e+25 (Low); coding_score: 4.37e+25 (Low); aai_score: 2.37e+25 (Low); mmlu_pro_score: 3.17e+25 (Low) \u2192 weighted average: 3.54e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.224326",
      "metadata": {}
    },
    {
      "name": "command_a_03",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.90e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Benchmark-based (lmarena_score): 1347.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.90e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1347.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Smaller AI company with limited computational budget for frontier models",
      "last_updated": "2025-08-15T18:13:34.224919",
      "metadata": {}
    },
    {
      "name": "qwen_plus",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.46e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1242.0,
        "coding_score": 1263.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.42e+25 (Low); coding_score: 3.49e+25 (Low) \u2192 weighted average: 3.46e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.225024",
      "metadata": {}
    },
    {
      "name": "qwen3_32b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.23e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.51e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.32e+25 (Low); coding_score: 4.52e+25 (Low); aai_score: 4.49e+25 (Low); mmlu_pro_score: 4.72e+25 (Low) \u2192 weighted average: 4.51e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1342.0,
        "coding_score": 1376.0,
        "aai_score": 44.0,
        "mmlu_pro_score": 79.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 640,000,000,000 tokens = 1.23e+23 FLOP (Modern model: 32B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.225442",
      "metadata": {}
    },
    {
      "name": "glm_4_plus",
      "developer": "Zhipu AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.69e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.90e+25 (Low); superclue_math: 3.48e+25 (Low); superclue_reasoning: 1.07e+25 (Low); superclue_code: 5.27e+25 (Low); superclue_agents: 4.73e+25 (Low) \u2192 weighted average: 3.69e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 63.67,
        "superclue_math": 63.04,
        "superclue_reasoning": 37.04,
        "superclue_code": 82.18,
        "superclue_agents": 72.41
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited computational resources for 1e25+ FLOP training runs",
      "last_updated": "2025-08-15T18:13:34.226172",
      "metadata": {}
    },
    {
      "name": "hunyuan_turbo",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "4.08e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 4.04e+25 (Low); coding_score: 4.12e+25 (Low) \u2192 weighted average: 4.08e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1313.0,
        "coding_score": 1335.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited evidence of computational budget allocation for frontier AI models",
      "last_updated": "2025-08-15T18:13:34.226724",
      "metadata": {}
    },
    {
      "name": "gemma_3_12b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 12000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.73e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.94e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.25e+25 (Low); coding_score: 3.90e+25 (Low); aai_score: 1.33e+25 (Low); mmlu_pro_score: 2.26e+25 (Low) \u2192 weighted average: 2.94e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1335.0,
        "coding_score": 1310.0,
        "aai_score": 24.0,
        "mmlu_pro_score": 59.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 12,000,000,000 params \u00d7 240,000,000,000 tokens = 1.73e+22 FLOP (Modern model: 12B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.226904",
      "metadata": {}
    },
    {
      "name": "gpt_4o",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": 1760000000000,
      "parameter_source": "known_specification:gpt_4o",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.80e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "1.37e+26",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Known model specification 'gpt_4o': Chinchilla scaling law: 6 \u00d7 1,760,000,000,000 params \u00d7 13,000,000,000,000 tokens = 1.37e+26 FLOP"
        },
        {
          "flop": "3.88e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.25e+25 (Low); superclue_math: 3.71e+25 (Low); superclue_reasoning: 2.10e+25 (Low); superclue_code: 3.96e+25 (Low); superclue_agents: 5.36e+25 (Low) \u2192 weighted average: 3.88e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "superclue_overall": 65.92,
        "superclue_math": 64.74,
        "superclue_reasoning": 48.52,
        "superclue_code": 73.27,
        "superclue_agents": 76.12
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from OpenAI patterns",
      "last_updated": "2025-08-15T18:13:34.227215",
      "metadata": {}
    },
    {
      "name": "qwq_32b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.51e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.21e+25 (Low); coding_score: 4.26e+25 (Low); aai_score: 5.34e+25 (Low); mmlu_pro_score: 4.23e+25 (Low) \u2192 weighted average: 4.51e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1331.0,
        "coding_score": 1349.0,
        "aai_score": 48.0,
        "mmlu_pro_score": 76.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.227623",
      "metadata": {}
    },
    {
      "name": "step_2",
      "developer": "StepFun",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "4.02e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 4.12e+25 (Low); coding_score: 3.92e+25 (Low) \u2192 weighted average: 4.02e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1321.0,
        "coding_score": 1313.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited computational resources for 1e25+ FLOP training runs",
      "last_updated": "2025-08-15T18:13:34.228138",
      "metadata": {}
    },
    {
      "name": "o1_mini",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.18e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1319.0,
        "coding_score": 1366.0,
        "aai_score": 43.0,
        "mmlu_pro_score": 74.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.10e+25 (Low); coding_score: 4.42e+25 (Low); aai_score: 4.28e+25 (Low); mmlu_pro_score: 3.93e+25 (Low) \u2192 weighted average: 4.18e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.228244",
      "metadata": {}
    },
    {
      "name": "llama_3.1_405b_instruct_bf16",
      "developer": "Meta",
      "release_date": null,
      "parameters": 405000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.48e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.34e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.80e+25 (Low); coding_score: 3.80e+25 (Low); aai_score: 1.95e+25 (Low); mmlu_pro_score: 3.80e+25 (Low) \u2192 weighted average: 3.34e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1286.0,
        "coding_score": 1299.0,
        "aai_score": 29.0,
        "mmlu_pro_score": 73.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 405,000,000,000 params \u00d7 6,075,000,000,000 tokens = 1.48e+25 FLOP (Modern large model: 405B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.228510",
      "metadata": {}
    },
    {
      "name": "llama_3.1_405b_instruct_fp8",
      "developer": "Meta",
      "release_date": null,
      "parameters": 405000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.48e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.32e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.78e+25 (Low); coding_score: 3.74e+25 (Low); aai_score: 1.95e+25 (Low); mmlu_pro_score: 3.80e+25 (Low) \u2192 weighted average: 3.32e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1284.0,
        "coding_score": 1292.0,
        "aai_score": 29.0,
        "mmlu_pro_score": 73.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 405,000,000,000 params \u00d7 6,075,000,000,000 tokens = 1.48e+25 FLOP (Modern large model: 405B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.228772",
      "metadata": {}
    },
    {
      "name": "gemini_advanced",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.77e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1332.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1332.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.77e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.228948",
      "metadata": {}
    },
    {
      "name": "grok_2",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.00e+25",
      "training_flop_confidence": "high",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "3.77e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1332.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.77e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_above_1e25",
      "benchmarks": {
        "lmarena_score": 1332.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: High-precision estimate from xAI disclosure",
      "last_updated": "2025-08-15T18:13:34.229082",
      "metadata": {}
    },
    {
      "name": "qwen3_30b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 30000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.08e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.21e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.11e+25 (Low); coding_score: 4.23e+25 (Low); aai_score: 4.09e+25 (Low); mmlu_pro_score: 4.41e+25 (Low) \u2192 weighted average: 4.21e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1320.0,
        "coding_score": 1346.0,
        "aai_score": 42.0,
        "mmlu_pro_score": 77.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 30,000,000,000 params \u00d7 600,000,000,000 tokens = 1.08e+23 FLOP (Modern model: 30B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.229285",
      "metadata": {}
    },
    {
      "name": "llama_4_maverick_17b_128e",
      "developer": "Meta",
      "release_date": null,
      "parameters": 17000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.47e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.18e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.85e+25 (Low); coding_score: 3.92e+25 (Low); aai_score: 4.09e+25 (Low); mmlu_pro_score: 4.88e+25 (Low) \u2192 weighted average: 4.18e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1292.0,
        "coding_score": 1312.0,
        "vision_score": 1185.0,
        "aai_score": 42.0,
        "mmlu_pro_score": 80.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 17,000,000,000 params \u00d7 340,000,000,000 tokens = 3.47e+22 FLOP (Modern model: 17B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.229390",
      "metadata": {}
    },
    {
      "name": "llama_3.3_nemotron_49b_super_v1",
      "developer": "Nvidia",
      "release_date": null,
      "parameters": 49000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.72e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1325.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.72e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1325.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 49,000,000,000 params \u00d7 980,000,000,000 tokens = 2.88e+23 FLOP (Modern model: 49B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.229464",
      "metadata": {}
    },
    {
      "name": "yi_lightning",
      "developer": "01 AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.67e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.93e+25 (Low); superclue_math: 3.65e+25 (Low); superclue_reasoning: 1.32e+25 (Low); superclue_code: 4.51e+25 (Low); superclue_agents: 4.93e+25 (Low) \u2192 weighted average: 3.67e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 63.88,
        "superclue_math": 64.3,
        "superclue_reasoning": 40.37,
        "superclue_code": 77.23,
        "superclue_agents": 73.61
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited computational resources for 1e25 FLOP training runs",
      "last_updated": "2025-08-15T18:13:34.229778",
      "metadata": {}
    },
    {
      "name": "hunyuan_large",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.88e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.84e+25 (Low); coding_score: 3.91e+25 (Low) \u2192 weighted average: 3.88e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1291.0,
        "coding_score": 1311.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited evidence of computational budget allocation for frontier AI models",
      "last_updated": "2025-08-15T18:13:34.229947",
      "metadata": {}
    },
    {
      "name": "deepseek_v2.5",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.82e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 57.67,
        "superclue_math": 60.34,
        "superclue_reasoning": 38.77,
        "superclue_code": 65.15,
        "superclue_agents": 66.42
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.05e+25 (Low); superclue_math: 3.12e+25 (Low); superclue_reasoning: 1.20e+25 (Low); superclue_code: 2.95e+25 (Low); superclue_agents: 3.81e+25 (Low) \u2192 weighted average: 2.82e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.230046",
      "metadata": {}
    },
    {
      "name": "gpt_4.1_nano",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.18e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1287.0,
        "coding_score": 1312.0,
        "vision_score": 1110.0,
        "aai_score": 30.0,
        "mmlu_pro_score": 65.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.81e+25 (Low); coding_score: 3.92e+25 (Low); aai_score: 2.09e+25 (Low); mmlu_pro_score: 2.90e+25 (Low) \u2192 weighted average: 3.18e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.230250",
      "metadata": {}
    },
    {
      "name": "gpt_4_turbo",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.12e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1275.0,
        "coding_score": 1280.0,
        "vision_score": 1137.0,
        "aai_score": 28.0,
        "mmlu_pro_score": 69.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.70e+25 (Low); coding_score: 3.64e+25 (Low); aai_score: 1.82e+25 (Low); mmlu_pro_score: 3.33e+25 (Low) \u2192 weighted average: 3.12e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.230447",
      "metadata": {}
    },
    {
      "name": "claude_3_opus",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": 175000000000,
      "parameter_source": "known_specification:claude_3_opus",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.60e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "8.40e+24",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Known model specification 'claude_3_opus': Chinchilla scaling law: 6 \u00d7 175,000,000,000 params \u00d7 8,000,000,000,000 tokens = 8.40e+24 FLOP"
        },
        {
          "flop": "2.96e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.62e+25 (Low); coding_score: 3.54e+25 (Low); aai_score: 1.33e+25 (Low); mmlu_pro_score: 3.35e+25 (Low) \u2192 weighted average: 2.96e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1265.0,
        "coding_score": 1269.0,
        "vision_score": 1070.0,
        "aai_score": 24.0,
        "mmlu_pro_score": 69.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from industry analysis",
      "last_updated": "2025-08-15T18:13:34.230639",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_pro_001",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.67e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1320.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1320.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.67e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.230751",
      "metadata": {}
    },
    {
      "name": "amazon_nova_experimental_chat_05_14",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.66e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1318.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1318.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.66e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.230873",
      "metadata": {}
    },
    {
      "name": "llama_4_scout_17b_16e",
      "developer": "Meta",
      "release_date": null,
      "parameters": 17000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.47e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.51e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.71e+25 (Low); coding_score: 3.72e+25 (Low); aai_score: 2.52e+25 (Low); mmlu_pro_score: 4.06e+25 (Low) \u2192 weighted average: 3.51e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1276.0,
        "coding_score": 1290.0,
        "vision_score": 1174.0,
        "aai_score": 33.0,
        "mmlu_pro_score": 75.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 17,000,000,000 params \u00d7 340,000,000,000 tokens = 3.47e+22 FLOP (Modern model: 17B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.231099",
      "metadata": {}
    },
    {
      "name": "gemma_3n_e4b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 4000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.92e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.47e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.97e+25 (Low); coding_score: 3.78e+25 (Low); aai_score: 7.51e+24 (Low); mmlu_pro_score: 1.38e+25 (Low) \u2192 weighted average: 2.47e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1305.0,
        "coding_score": 1297.0,
        "aai_score": 18.0,
        "mmlu_pro_score": 48.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 4,000,000,000 params \u00d7 80,000,000,000 tokens = 1.92e+21 FLOP (Modern model: 4B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.231328",
      "metadata": {}
    },
    {
      "name": "qwen_max",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.67e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 64.47,
        "superclue_math": 66.07,
        "superclue_reasoning": 47.41,
        "superclue_code": 75.64,
        "superclue_agents": 68.76
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.02e+25 (Low); superclue_math: 3.91e+25 (Low); superclue_reasoning: 1.98e+25 (Low); superclue_code: 4.28e+25 (Low); superclue_agents: 4.15e+25 (Low) \u2192 weighted average: 3.67e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.231480",
      "metadata": {}
    },
    {
      "name": "llama_3.3_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.63e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.81e+25 (Low); superclue_math: 2.94e+25 (Low); superclue_reasoning: 1.11e+25 (Low); superclue_code: 2.30e+25 (Low); superclue_agents: 4.01e+25 (Low) \u2192 weighted average: 2.63e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "superclue_overall": 55.84,
        "superclue_math": 58.92,
        "superclue_reasoning": 37.65,
        "superclue_code": 59.01,
        "superclue_agents": 67.8
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,400,000,000,000 tokens = 5.88e+23 FLOP (Modern model: 70B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.231789",
      "metadata": {}
    },
    {
      "name": "qwen2.5_plus",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.63e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1315.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1315.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.63e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.231858",
      "metadata": {}
    },
    {
      "name": "claude_3_5_haiku",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.65e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1317.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1317.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.65e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.232311",
      "metadata": {}
    },
    {
      "name": "gpt_4o_mini",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.98e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 58.82,
        "superclue_math": 55.45,
        "superclue_reasoning": 40.37,
        "superclue_code": 67.92,
        "superclue_agents": 71.56
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.20e+25 (Low); superclue_math: 2.52e+25 (Low); superclue_reasoning: 1.32e+25 (Low); superclue_code: 3.27e+25 (Low); superclue_agents: 4.59e+25 (Low) \u2192 weighted average: 2.98e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.232588",
      "metadata": {}
    },
    {
      "name": "gpt_4_preview",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.55e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1266.0,
        "coding_score": 1261.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.63e+25 (Low); coding_score: 3.48e+25 (Low) \u2192 weighted average: 3.55e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.233002",
      "metadata": {}
    },
    {
      "name": "athene",
      "developer": "NexusFlow",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.63e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Benchmark-based (lmarena_score): 1315.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.63e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1315.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited computational resources for large-scale training",
      "last_updated": "2025-08-15T18:13:34.233323",
      "metadata": {}
    },
    {
      "name": "hunyuan_standard",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.71e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.71e+25 (Low); coding_score: 3.71e+25 (Low) \u2192 weighted average: 3.71e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1276.0,
        "coding_score": 1289.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited evidence of computational budget allocation for frontier AI models",
      "last_updated": "2025-08-15T18:13:34.233535",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_flash_002",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.10e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1290.0,
        "coding_score": 1273.0,
        "vision_score": 1187.0,
        "aai_score": 28.0,
        "mmlu_pro_score": 68.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.84e+25 (Low); coding_score: 3.58e+25 (Low); aai_score: 1.82e+25 (Low); mmlu_pro_score: 3.16e+25 (Low) \u2192 weighted average: 3.10e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.233739",
      "metadata": {}
    },
    {
      "name": "mistral_large",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.09e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1269.0,
        "coding_score": 1284.0,
        "aai_score": 27.0,
        "mmlu_pro_score": 69.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.65e+25 (Low); coding_score: 3.67e+25 (Low); aai_score: 1.69e+25 (Low); mmlu_pro_score: 3.36e+25 (Low) \u2192 weighted average: 3.09e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.233891",
      "metadata": {}
    },
    {
      "name": "magistral_medium",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.70e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1253.0,
        "coding_score": 1307.0,
        "aai_score": 38.0,
        "mmlu_pro_score": 75.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.51e+25 (Low); coding_score: 3.87e+25 (Low); aai_score: 3.35e+25 (Low); mmlu_pro_score: 4.08e+25 (Low) \u2192 weighted average: 3.70e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.234002",
      "metadata": {}
    },
    {
      "name": "gemma_3_4b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 4000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.92e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.19e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.86e+25 (Low); coding_score: 3.51e+25 (Low); aai_score: 4.54e+24 (Low); mmlu_pro_score: 9.31e+24 (Low) \u2192 weighted average: 2.19e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1293.0,
        "coding_score": 1265.0,
        "aai_score": 14.0,
        "mmlu_pro_score": 41.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 4,000,000,000 params \u00d7 80,000,000,000 tokens = 1.92e+21 FLOP (Modern model: 4B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.234198",
      "metadata": {}
    },
    {
      "name": "grok_2_mini",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.57e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1307.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1307.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.57e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.234264",
      "metadata": {}
    },
    {
      "name": "athene_70b",
      "developer": "NexusFlow",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.61e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.64e+25 (Low); coding_score: 3.58e+25 (Low) \u2192 weighted average: 3.61e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1268.0,
        "coding_score": 1274.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.234443",
      "metadata": {}
    },
    {
      "name": "qwen2.5_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.73e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.74e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.94e+25 (Low); superclue_math: 3.35e+25 (Low); superclue_reasoning: 1.14e+25 (Low); superclue_code: 2.48e+25 (Low); superclue_agents: 3.81e+25 (Low) \u2192 weighted average: 2.74e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "superclue_overall": 56.84,
        "superclue_math": 62.11,
        "superclue_reasoning": 38.02,
        "superclue_code": 60.79,
        "superclue_agents": 66.42
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 864,000,000,000 tokens = 3.73e+23 FLOP (Mid-era model: 72B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.234607",
      "metadata": {}
    },
    {
      "name": "llama_3.1_nemotron_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.09e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.79e+25 (Low); coding_score: 3.71e+25 (Low); aai_score: 1.57e+25 (Low); mmlu_pro_score: 3.28e+25 (Low) \u2192 weighted average: 3.09e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1285.0,
        "coding_score": 1289.0,
        "aai_score": 26.0,
        "mmlu_pro_score": 69.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,400,000,000,000 tokens = 5.88e+23 FLOP (Modern model: 70B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.234751",
      "metadata": {}
    },
    {
      "name": "hunyuan_large_vision",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.73e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.66e+25 (Low); coding_score: 3.79e+25 (Low) \u2192 weighted average: 3.73e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1270.0,
        "coding_score": 1298.0,
        "vision_score": 1238.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited evidence of computational budget allocation for frontier AI models",
      "last_updated": "2025-08-15T18:13:34.234927",
      "metadata": {}
    },
    {
      "name": "mistral_small_3.1_24b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 24000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.15e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.92e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.65e+25 (Low); coding_score: 3.77e+25 (Low); aai_score: 1.33e+25 (Low); mmlu_pro_score: 2.92e+25 (Low) \u2192 weighted average: 2.92e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1269.0,
        "coding_score": 1295.0,
        "vision_score": 1162.0,
        "aai_score": 24.0,
        "mmlu_pro_score": 65.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 24,000,000,000 params \u00d7 288,000,000,000 tokens = 4.15e+22 FLOP (Mid-era model: 24B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.235086",
      "metadata": {}
    },
    {
      "name": "llama_3.1_tulu_3_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.48e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.57e+25 (Low); coding_score: 3.39e+25 (Low) \u2192 weighted average: 3.48e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1260.0,
        "coding_score": 1251.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,400,000,000,000 tokens = 5.88e+23 FLOP (Modern model: 70B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.235176",
      "metadata": {}
    },
    {
      "name": "llama_3.1_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "known_specification:llama_3.1_70b",
      "context_length": null,
      "architecture": null,
      "training_flop": "6.30e+24",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.16e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.32e+25 (Low); superclue_math: 2.72e+25 (Low); superclue_reasoning: 1.10e+25 (Low); superclue_code: 1.71e+25 (Low); superclue_agents: 2.96e+25 (Low) \u2192 weighted average: 2.16e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "superclue_overall": 51.74,
        "superclue_math": 57.15,
        "superclue_reasoning": 37.41,
        "superclue_code": 52.38,
        "superclue_agents": 60.02
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Known model specification 'llama_3.1_70b': Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 15,000,000,000,000 tokens = 6.30e+24 FLOP",
      "last_updated": "2025-08-15T18:13:34.235316",
      "metadata": {}
    },
    {
      "name": "llama_3.1_nemotron_51b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 51000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.12e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.27e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.33e+25 (Low); coding_score: 3.20e+25 (Low) \u2192 weighted average: 3.27e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1231.0,
        "coding_score": 1227.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 51,000,000,000 params \u00d7 1,020,000,000,000 tokens = 3.12e+23 FLOP (Modern model: 51B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.235431",
      "metadata": {}
    },
    {
      "name": "amazon_nova_pro",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.12e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1262.0,
        "coding_score": 1282.0,
        "vision_score": 1029.0,
        "aai_score": 29.0,
        "mmlu_pro_score": 69.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.59e+25 (Low); coding_score: 3.65e+25 (Low); aai_score: 1.95e+25 (Low); mmlu_pro_score: 3.29e+25 (Low) \u2192 weighted average: 3.12e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.235580",
      "metadata": {}
    },
    {
      "name": "jamba_1.5_large",
      "developer": "AI21 Labs",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.39e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.42e+25 (Low); coding_score: 3.34e+25 (Low); aai_score: 7.51e+24 (Low); mmlu_pro_score: 2.05e+25 (Low) \u2192 weighted average: 2.39e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1242.0,
        "coding_score": 1244.0,
        "aai_score": 18.0,
        "mmlu_pro_score": 57.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited access to large-scale GPU infrastructure",
      "last_updated": "2025-08-15T18:13:34.235867",
      "metadata": {}
    },
    {
      "name": "reka_core",
      "developer": "Reka AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.34e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.49e+25 (Low); coding_score: 3.19e+25 (Low) \u2192 weighted average: 3.34e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1250.0,
        "coding_score": 1226.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Smaller AI company with limited computational budget",
      "last_updated": "2025-08-15T18:13:34.236030",
      "metadata": {}
    },
    {
      "name": "gpt_4",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": 1760000000000,
      "parameter_source": "known_specification:gpt_4",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.10e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "1.37e+26",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Known model specification 'gpt_4': Chinchilla scaling law: 6 \u00d7 1,760,000,000,000 params \u00d7 13,000,000,000,000 tokens = 1.37e+26 FLOP"
        },
        {
          "flop": "3.33e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1277.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.33e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1277.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from scaling analysis",
      "last_updated": "2025-08-15T18:13:34.236127",
      "metadata": {}
    },
    {
      "name": "gemma_2_27b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 27000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.25e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.48e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.61e+25 (Low); superclue_math: 2.90e+25 (Low); superclue_reasoning: 9.90e+24 (Low); superclue_code: 1.87e+25 (Low); superclue_agents: 4.05e+25 (Low) \u2192 weighted average: 2.48e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "superclue_overall": 54.21,
        "superclue_math": 58.63,
        "superclue_reasoning": 35.93,
        "superclue_code": 54.26,
        "superclue_agents": 68.04
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 27,000,000,000 params \u00d7 324,000,000,000 tokens = 5.25e+22 FLOP (Mid-era model: 27B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.236366",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_flash_001",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.38e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1284.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1284.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.38e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.236451",
      "metadata": {}
    },
    {
      "name": "gemma_2_9b_it_simpo",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 9000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.83e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.21e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.35e+25 (Low); coding_score: 3.08e+25 (Low) \u2192 weighted average: 3.21e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1233.0,
        "coding_score": 1211.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 9,000,000,000 params \u00d7 108,000,000,000 tokens = 5.83e+21 FLOP (Mid-era model: 9B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.236688",
      "metadata": {}
    },
    {
      "name": "claude_3_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.30e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1223.0,
        "coding_score": 1232.0,
        "vision_score": 1033.0,
        "aai_score": 16.0,
        "mmlu_pro_score": 57.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.27e+25 (Low); coding_score: 3.24e+25 (Low); aai_score: 5.93e+24 (Low); mmlu_pro_score: 2.11e+25 (Low) \u2192 weighted average: 2.30e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.236858",
      "metadata": {}
    },
    {
      "name": "command_r_plus_08",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.36e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Benchmark-based (lmarena_score): 1281.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.36e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1281.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Smaller AI company with limited computational budget for frontier models",
      "last_updated": "2025-08-15T18:13:34.237083",
      "metadata": {}
    },
    {
      "name": "nemotron_4_340b",
      "developer": "Nvidia",
      "release_date": null,
      "parameters": 340000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.04e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.23e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.32e+25 (Low); coding_score: 3.15e+25 (Low) \u2192 weighted average: 3.23e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1229.0,
        "coding_score": 1220.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 340,000,000,000 params \u00d7 5,100,000,000,000 tokens = 1.04e+25 FLOP (Generic estimate: 340B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.237217",
      "metadata": {}
    },
    {
      "name": "reka_flash",
      "developer": "Reka AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.12e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.23e+25 (Low); coding_score: 3.00e+25 (Low) \u2192 weighted average: 3.12e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1218.0,
        "coding_score": 1201.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Smaller AI company with limited computational budget",
      "last_updated": "2025-08-15T18:13:34.237453",
      "metadata": {}
    },
    {
      "name": "glm_4",
      "developer": "Zhipu AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.07e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.07e+25 (Low); coding_score: 3.06e+25 (Low) \u2192 weighted average: 3.07e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1198.0,
        "coding_score": 1209.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited computational resources for 1e25+ FLOP training runs",
      "last_updated": "2025-08-15T18:13:34.237614",
      "metadata": {}
    },
    {
      "name": "llama_3_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "known_specification:llama_3_70b",
      "context_length": null,
      "architecture": null,
      "training_flop": "6.30e+24",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.26e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.28e+25 (Low); coding_score: 3.12e+25 (Low); aai_score: 5.93e+24 (Low); mmlu_pro_score: 2.07e+25 (Low) \u2192 weighted average: 2.26e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1224.0,
        "coding_score": 1216.0,
        "aai_score": 16.0,
        "mmlu_pro_score": 57.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'llama_3_70b': Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 15,000,000,000,000 tokens = 6.30e+24 FLOP",
      "last_updated": "2025-08-15T18:13:34.237845",
      "metadata": {}
    },
    {
      "name": "mistral_small_24b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 24000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.15e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.31e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1275.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.31e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1275.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 24,000,000,000 params \u00d7 288,000,000,000 tokens = 4.15e+22 FLOP (Mid-era model: 24B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.237936",
      "metadata": {}
    },
    {
      "name": "qwen2.5_coder_32b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.37e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.78e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.37e+25 (Low); coding_score: 3.63e+25 (Low); aai_score: 1.45e+25 (Low); mmlu_pro_score: 2.66e+25 (Low) \u2192 weighted average: 2.78e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1235.0,
        "coding_score": 1279.0,
        "aai_score": 25.0,
        "mmlu_pro_score": 63.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 384,000,000,000 tokens = 7.37e+22 FLOP (Mid-era model: 32B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.239305",
      "metadata": {}
    },
    {
      "name": "c4ai_aya_expanse_32b",
      "developer": "Cohere",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.26e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1269.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.26e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1269.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.239459",
      "metadata": {}
    },
    {
      "name": "olmo_2_32b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.19e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.27e+25 (Low); coding_score: 3.11e+25 (Low) \u2192 weighted average: 3.19e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1223.0,
        "coding_score": 1215.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.239582",
      "metadata": {}
    },
    {
      "name": "qwen2_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.73e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.44e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.15e+25 (Low); coding_score: 3.04e+25 (Low); aai_score: 1.02e+25 (Low); mmlu_pro_score: 2.53e+25 (Low) \u2192 weighted average: 2.44e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1208.0,
        "coding_score": 1206.0,
        "aai_score": 21.0,
        "mmlu_pro_score": 62.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 864,000,000,000 tokens = 3.73e+23 FLOP (Mid-era model: 72B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.239735",
      "metadata": {}
    },
    {
      "name": "command_r_plus",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.24e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Benchmark-based (lmarena_score): 1266.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.24e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1266.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Smaller AI company with limited computational budget for frontier models",
      "last_updated": "2025-08-15T18:13:34.240237",
      "metadata": {}
    },
    {
      "name": "gemma_2_9b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 9000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.83e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.95e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.19e+25 (Low); coding_score: 2.95e+25 (Low); aai_score: 2.32e+24 (Low); mmlu_pro_score: 1.43e+25 (Low) \u2192 weighted average: 1.95e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1213.0,
        "coding_score": 1194.0,
        "aai_score": 10.0,
        "mmlu_pro_score": 49.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 9,000,000,000 params \u00d7 108,000,000,000 tokens = 5.83e+21 FLOP (Mid-era model: 9B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.240481",
      "metadata": {}
    },
    {
      "name": "deepseek_coder_v2",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": 236000000000,
      "parameter_source": "known_specification:deepseek_coder_v2",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.50e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.26e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.06e+25 (Low); coding_score: 3.46e+25 (Low) \u2192 weighted average: 3.26e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1196.0,
        "coding_score": 1259.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'deepseek_coder_v2': Chinchilla scaling law: 6 \u00d7 236,000,000,000 params \u00d7 6,000,000,000,000 tokens = 8.50e+24 FLOP",
      "last_updated": "2025-08-15T18:13:34.240616",
      "metadata": {}
    },
    {
      "name": "claude_3_haiku",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.99e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1200.0,
        "coding_score": 1208.0,
        "vision_score": 1000.0,
        "aai_score": 12.0,
        "mmlu_pro_score": 50.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.09e+25 (Low); coding_score: 3.06e+25 (Low); aai_score: 3.34e+24 (Low); mmlu_pro_score: 1.47e+25 (Low) \u2192 weighted average: 1.99e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.240744",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_flash_8b_001",
      "developer": "Google",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.35e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.33e+25 (Low); coding_score: 3.21e+25 (Low); aai_score: 8.36e+24 (Low); mmlu_pro_score: 2.02e+25 (Low) \u2192 weighted average: 2.35e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1231.0,
        "coding_score": 1228.0,
        "vision_score": 1092.0,
        "aai_score": 19.0,
        "mmlu_pro_score": 56.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.241126",
      "metadata": {}
    },
    {
      "name": "amazon_nova_lite",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.61e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1233.0,
        "coding_score": 1253.0,
        "vision_score": 1039.0,
        "aai_score": 25.0,
        "mmlu_pro_score": 59.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.35e+25 (Low); coding_score: 3.41e+25 (Low); aai_score: 1.45e+25 (Low); mmlu_pro_score: 2.22e+25 (Low) \u2192 weighted average: 2.61e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.241245",
      "metadata": {}
    },
    {
      "name": "phi_4",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.99e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.26e+25 (Low); coding_score: 3.32e+25 (Low); aai_score: 1.82e+25 (Low); mmlu_pro_score: 3.57e+25 (Low) \u2192 weighted average: 2.99e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1222.0,
        "coding_score": 1242.0,
        "aai_score": 28.0,
        "mmlu_pro_score": 71.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Primarily partners with OpenAI rather than training proprietary frontier models",
      "last_updated": "2025-08-15T18:13:34.241623",
      "metadata": {}
    },
    {
      "name": "command_r_08",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.14e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Benchmark-based (lmarena_score): 1253.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.14e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1253.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Smaller AI company with limited computational budget for frontier models",
      "last_updated": "2025-08-15T18:13:34.241753",
      "metadata": {}
    },
    {
      "name": "amazon_nova_micro",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.26e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1215.0,
        "coding_score": 1228.0,
        "aai_score": 20.0,
        "mmlu_pro_score": 53.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.20e+25 (Low); coding_score: 3.21e+25 (Low); aai_score: 9.27e+24 (Low); mmlu_pro_score: 1.70e+25 (Low) \u2192 weighted average: 2.26e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.241880",
      "metadata": {}
    },
    {
      "name": "jamba_1.5_mini",
      "developer": "AI21 Labs",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.00e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.03e+25 (Low); coding_score: 2.97e+25 (Low) \u2192 weighted average: 3.00e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1193.0,
        "coding_score": 1197.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited access to large-scale GPU infrastructure",
      "last_updated": "2025-08-15T18:13:34.242087",
      "metadata": {}
    },
    {
      "name": "hunyuan_standard_256k",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.25e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.16e+25 (Low); coding_score: 3.34e+25 (Low) \u2192 weighted average: 3.25e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1209.0,
        "coding_score": 1244.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited evidence of computational budget allocation for frontier AI models",
      "last_updated": "2025-08-15T18:13:34.242290",
      "metadata": {}
    },
    {
      "name": "ministral_8b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.81e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.07e+25 (Low); coding_score: 3.14e+25 (Low); aai_score: 2.32e+24 (Low); mmlu_pro_score: 7.82e+24 (Low) \u2192 weighted average: 1.81e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1198.0,
        "coding_score": 1219.0,
        "aai_score": 10.0,
        "mmlu_pro_score": 38.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.242462",
      "metadata": {}
    },
    {
      "name": "qwen1.5_110b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 110000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.16e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.09e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 3 benchmarks: openlm_arena_elo: 2.94e+25 (Low); coding_score: 2.94e+25 (Low); aai_score: 3.92e+24 (Low) \u2192 weighted average: 2.09e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1180.0,
        "coding_score": 1192.0,
        "aai_score": 13.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 110,000,000,000 params \u00d7 1,760,000,000,000 tokens = 1.16e+24 FLOP (Chinese model: 110B params * 16 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.242592",
      "metadata": {}
    },
    {
      "name": "qwen1.5_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.98e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.84e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.88e+25 (Low); coding_score: 2.81e+25 (Low) \u2192 weighted average: 2.84e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1172.0,
        "coding_score": 1175.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 1,152,000,000,000 tokens = 4.98e+23 FLOP (Chinese model: 72B params * 16 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.242700",
      "metadata": {}
    },
    {
      "name": "reka_flash_21b_online",
      "developer": "Reka AI",
      "release_date": null,
      "parameters": 21000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.97e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.01e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1235.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.01e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1235.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 21,000,000,000 params \u00d7 315,000,000,000 tokens = 3.97e+22 FLOP (Generic estimate: 21B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.242790",
      "metadata": {}
    },
    {
      "name": "gemini_pro_dev_api",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.00e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1234.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1234.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.00e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.242850",
      "metadata": {}
    },
    {
      "name": "mixtral_8x22b_instruct",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 22000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.23e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.97e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.85e+25 (Low); coding_score: 2.81e+25 (Low); aai_score: 4.54e+24 (Low); mmlu_pro_score: 1.75e+25 (Low) \u2192 weighted average: 1.97e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1168.0,
        "coding_score": 1175.0,
        "aai_score": 14.0,
        "mmlu_pro_score": 53.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 22,000,000,000 params \u00d7 396,000,000,000 tokens = 5.23e+22 FLOP (Specialized model: 22B params * 18 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.243093",
      "metadata": {}
    },
    {
      "name": "command_r",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "1.50e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.85e+25 (Low); coding_score: 2.58e+25 (Low); aai_score: 9.27e+22 (Low); mmlu_pro_score: 5.46e+24 (Low) \u2192 weighted average: 1.50e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1169.0,
        "coding_score": 1141.0,
        "aai_score": 2.0,
        "mmlu_pro_score": 33.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Smaller AI company with limited computational budget for frontier models",
      "last_updated": "2025-08-15T18:13:34.243300",
      "metadata": {}
    },
    {
      "name": "reka_flash_21b",
      "developer": "Reka AI",
      "release_date": null,
      "parameters": 21000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.97e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.97e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1230.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.97e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1230.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 21,000,000,000 params \u00d7 315,000,000,000 tokens = 3.97e+22 FLOP (Generic estimate: 21B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.243400",
      "metadata": {}
    },
    {
      "name": "llama_3.1_tulu_3_8b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.68e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.03e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.09e+25 (Low); coding_score: 2.97e+25 (Low) \u2192 weighted average: 3.03e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1200.0,
        "coding_score": 1197.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 160,000,000,000 tokens = 7.68e+21 FLOP (Modern model: 8B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.243514",
      "metadata": {}
    },
    {
      "name": "gemini_pro",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.91e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1222.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1222.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.91e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.243572",
      "metadata": {}
    },
    {
      "name": "gpt_3.5_turbo",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.59e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1138.0,
        "coding_score": 1136.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.63e+25 (Low); coding_score: 2.54e+25 (Low) \u2192 weighted average: 2.59e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.243771",
      "metadata": {}
    },
    {
      "name": "c4ai_aya_expanse_8b",
      "developer": "Cohere",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.93e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1224.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.93e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1224.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.244110",
      "metadata": {}
    },
    {
      "name": "llama_3_8b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "known_specification:llama_3_8b",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.20e+23",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.66e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.87e+25 (Low); coding_score: 2.73e+25 (Low); aai_score: 1.88e+24 (Low); mmlu_pro_score: 8.65e+24 (Low) \u2192 weighted average: 1.66e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1171.0,
        "coding_score": 1164.0,
        "aai_score": 9.0,
        "mmlu_pro_score": 40.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'llama_3_8b': Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 15,000,000,000,000 tokens = 7.20e+23 FLOP",
      "last_updated": "2025-08-15T18:13:34.244294",
      "metadata": {}
    },
    {
      "name": "zephyr_orpo_141b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 141000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.79e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.66e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.72e+25 (Low); coding_score: 2.60e+25 (Low) \u2192 weighted average: 2.66e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1150.0,
        "coding_score": 1144.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 141,000,000,000 params \u00d7 2,115,000,000,000 tokens = 1.79e+24 FLOP (Generic estimate: 141B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.244445",
      "metadata": {}
    },
    {
      "name": "yi_1.5_34b",
      "developer": "01 AI",
      "release_date": null,
      "parameters": 34000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.11e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.25e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.43e+25 (Low); superclue_math: 2.63e+25 (Low); superclue_reasoning: 1.02e+25 (Low); superclue_code: 2.11e+25 (Low); superclue_agents: 3.09e+25 (Low) \u2192 weighted average: 2.25e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "superclue_overall": 52.69,
        "superclue_math": 56.35,
        "superclue_reasoning": 36.3,
        "superclue_code": 57.03,
        "superclue_agents": 61.08
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 34,000,000,000 params \u00d7 544,000,000,000 tokens = 1.11e+23 FLOP (Chinese model: 34B params * 16 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.244725",
      "metadata": {}
    },
    {
      "name": "granite_3.1_8b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.85e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.77e+25 (Low); coding_score: 2.93e+25 (Low) \u2192 weighted average: 2.85e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1158.0,
        "coding_score": 1191.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.245684",
      "metadata": {}
    },
    {
      "name": "llama_3.1_8b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "known_specification:llama_3.1_8b",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.20e+23",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.92e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.03e+25 (Low); coding_score: 3.02e+25 (Low); aai_score: 3.34e+24 (Low); mmlu_pro_score: 1.30e+25 (Low) \u2192 weighted average: 1.92e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1193.0,
        "coding_score": 1203.0,
        "aai_score": 12.0,
        "mmlu_pro_score": 47.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'llama_3.1_8b': Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 15,000,000,000,000 tokens = 7.20e+23 FLOP",
      "last_updated": "2025-08-15T18:13:34.245871",
      "metadata": {}
    },
    {
      "name": "qwen1.5_32b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.83e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.70e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.68e+25 (Low); coding_score: 2.73e+25 (Low) \u2192 weighted average: 2.70e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1144.0,
        "coding_score": 1163.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 512,000,000,000 tokens = 9.83e+22 FLOP (Chinese model: 32B params * 16 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.245992",
      "metadata": {}
    },
    {
      "name": "phi_3_medium_4k",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "1.87e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.68e+25 (Low); coding_score: 2.61e+25 (Low); aai_score: 3.92e+24 (Low); mmlu_pro_score: 1.80e+25 (Low) \u2192 weighted average: 1.87e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1144.0,
        "coding_score": 1146.0,
        "aai_score": 13.0,
        "mmlu_pro_score": 54.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Primarily partners with OpenAI rather than training proprietary frontier models",
      "last_updated": "2025-08-15T18:13:34.246298",
      "metadata": {}
    },
    {
      "name": "mixtral_8x7b_instruct",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.29e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.50e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.63e+25 (Low); coding_score: 2.54e+25 (Low); aai_score: 5.79e+23 (Low); mmlu_pro_score: 7.72e+24 (Low) \u2192 weighted average: 1.50e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1138.0,
        "coding_score": 1136.0,
        "aai_score": 5.0,
        "mmlu_pro_score": 38.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 126,000,000,000 tokens = 5.29e+21 FLOP (Specialized model: 7B params * 18 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.249551",
      "metadata": {}
    },
    {
      "name": "gemma_2_2b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.88e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.66e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.81e+25 (Low); coding_score: 2.50e+25 (Low) \u2192 weighted average: 2.66e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1163.0,
        "coding_score": 1130.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 24,000,000,000 tokens = 2.88e+20 FLOP (Mid-era model: 2B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.250039",
      "metadata": {}
    },
    {
      "name": "internlm2_5_20b",
      "developer": "InternLM",
      "release_date": null,
      "parameters": 20000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.76e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1200.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.76e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1200.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 20,000,000,000 params \u00d7 300,000,000,000 tokens = 3.60e+22 FLOP (Generic estimate: 20B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.250176",
      "metadata": {}
    },
    {
      "name": "qwen1.5_14b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 14000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.88e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.60e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.61e+25 (Low); coding_score: 2.60e+25 (Low) \u2192 weighted average: 2.60e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1135.0,
        "coding_score": 1144.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 14,000,000,000 params \u00d7 224,000,000,000 tokens = 1.88e+22 FLOP (Chinese model: 14B params * 16 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.250953",
      "metadata": {}
    },
    {
      "name": "dbrx_instruct_preview",
      "developer": "Databricks",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.56e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.55e+25 (Low); coding_score: 2.58e+25 (Low) \u2192 weighted average: 2.56e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1126.0,
        "coding_score": 1141.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Focus on enterprise solutions, limited frontier model training resources",
      "last_updated": "2025-08-15T18:13:34.251497",
      "metadata": {}
    },
    {
      "name": "wizardlm_70b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.41e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.55e+25 (Low); coding_score: 2.26e+25 (Low) \u2192 weighted average: 2.41e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1126.0,
        "coding_score": 1093.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.252478",
      "metadata": {}
    },
    {
      "name": "granite_3.0_8b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.42e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.43e+25 (Low); coding_score: 2.40e+25 (Low) \u2192 weighted average: 2.42e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1108.0,
        "coding_score": 1115.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.252720",
      "metadata": {}
    },
    {
      "name": "deepseek_llm_67b",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": 67000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.04e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.65e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 3 benchmarks: openlm_arena_elo: 2.45e+25 (Low); coding_score: 2.35e+25 (Low); aai_score: 1.48e+24 (Low) \u2192 weighted average: 1.65e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1111.0,
        "coding_score": 1106.0,
        "aai_score": 8.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 67,000,000,000 params \u00d7 1,005,000,000,000 tokens = 4.04e+23 FLOP (Generic estimate: 67B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.252922",
      "metadata": {}
    },
    {
      "name": "yi_34b",
      "developer": "01 AI",
      "release_date": null,
      "parameters": 34000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.11e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.55e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.61e+25 (Low); coding_score: 2.49e+25 (Low) \u2192 weighted average: 2.55e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1134.0,
        "coding_score": 1129.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 34,000,000,000 params \u00d7 544,000,000,000 tokens = 1.11e+23 FLOP (Chinese model: 34B params * 16 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.253058",
      "metadata": {}
    },
    {
      "name": "openchat",
      "developer": "OpenChat",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.28e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.40e+25 (Low); coding_score: 2.17e+25 (Low) \u2192 weighted average: 2.28e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1103.0,
        "coding_score": 1077.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Open source project with limited computational resources",
      "last_updated": "2025-08-15T18:13:34.253559",
      "metadata": {}
    },
    {
      "name": "granite_3.1_2b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.68e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.62e+25 (Low); coding_score: 2.75e+25 (Low) \u2192 weighted average: 2.68e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1136.0,
        "coding_score": 1166.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 30,000,000,000 tokens = 3.60e+20 FLOP (Generic estimate: 2B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.253716",
      "metadata": {}
    },
    {
      "name": "tulu_2_dpo_70b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.50e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.56e+25 (Low); coding_score: 2.44e+25 (Low) \u2192 weighted average: 2.50e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1127.0,
        "coding_score": 1120.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.253840",
      "metadata": {}
    },
    {
      "name": "openhermes_2.5_mistral_7b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.29e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.38e+25 (Low); coding_score: 2.20e+25 (Low) \u2192 weighted average: 2.29e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1100.0,
        "coding_score": 1083.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.254014",
      "metadata": {}
    },
    {
      "name": "snowflake_arctic",
      "developer": "Snowflake",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.38e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.44e+25 (Low); coding_score: 2.31e+25 (Low) \u2192 weighted average: 2.38e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1109.0,
        "coding_score": 1101.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Data platform company without large-scale AI model training focus",
      "last_updated": "2025-08-15T18:13:34.254295",
      "metadata": {}
    },
    {
      "name": "gemma_1.1_7b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.37e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.39e+25 (Low); coding_score: 2.34e+25 (Low) \u2192 weighted average: 2.37e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1102.0,
        "coding_score": 1105.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.254437",
      "metadata": {}
    },
    {
      "name": "vicuna_33b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 33000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.80e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.36e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.46e+25 (Low); coding_score: 2.25e+25 (Low) \u2192 weighted average: 2.36e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1113.0,
        "coding_score": 1091.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 33,000,000,000 params \u00d7 495,000,000,000 tokens = 9.80e+22 FLOP (Generic estimate: 33B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.254556",
      "metadata": {}
    },
    {
      "name": "phi_3_small_8k",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.47e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.49e+25 (Low); coding_score: 2.46e+25 (Low) \u2192 weighted average: 2.47e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1117.0,
        "coding_score": 1123.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Primarily partners with OpenAI rather than training proprietary frontier models",
      "last_updated": "2025-08-15T18:13:34.256377",
      "metadata": {}
    },
    {
      "name": "starling_lm_7b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.64e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.64e+25 (Low); coding_score: 2.64e+25 (Low) \u2192 weighted average: 2.64e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1139.0,
        "coding_score": 1151.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.256607",
      "metadata": {}
    },
    {
      "name": "llama_2_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "known_specification:llama_2_70b",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.40e+23",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.46e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.52e+25 (Low); coding_score: 2.30e+25 (Low); aai_score: 1.48e+24 (Low); mmlu_pro_score: 8.76e+24 (Low) \u2192 weighted average: 1.46e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1122.0,
        "coding_score": 1099.0,
        "aai_score": 8.0,
        "mmlu_pro_score": 40.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'llama_2_70b': Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 2,000,000,000,000 tokens = 8.40e+23 FLOP",
      "last_updated": "2025-08-15T18:13:34.256770",
      "metadata": {}
    },
    {
      "name": "nous_hermes_2_mixtral_8x7b_dpo",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.41e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.50e+25 (Low); coding_score: 2.33e+25 (Low) \u2192 weighted average: 2.41e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1119.0,
        "coding_score": 1103.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.256963",
      "metadata": {}
    },
    {
      "name": "qwq_32b_preview",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.58e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1174.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.58e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1174.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.257060",
      "metadata": {}
    },
    {
      "name": "llama_3.2_3b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 3000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.08e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.37e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.50e+25 (Low); coding_score: 2.29e+25 (Low); aai_score: 1.14e+24 (Low); mmlu_pro_score: 5.88e+24 (Low) \u2192 weighted average: 1.37e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1118.0,
        "coding_score": 1097.0,
        "aai_score": 7.0,
        "mmlu_pro_score": 34.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 3,000,000,000 params \u00d7 60,000,000,000 tokens = 1.08e+21 FLOP (Modern model: 3B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.257202",
      "metadata": {}
    },
    {
      "name": "starling_lm_7b_alpha",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.40e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.47e+25 (Low); coding_score: 2.33e+25 (Low) \u2192 weighted average: 2.40e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1114.0,
        "coding_score": 1104.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.257315",
      "metadata": {}
    },
    {
      "name": "llama2_70b_steerlm",
      "developer": "Nvidia",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.52e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1164.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.52e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1164.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.257403",
      "metadata": {}
    },
    {
      "name": "solar_10.7b_instruct",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 10700000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.24e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.23e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.32e+25 (Low); coding_score: 2.14e+25 (Low) \u2192 weighted average: 2.23e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1091.0,
        "coding_score": 1073.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 10,700,000,000 params \u00d7 192,600,000,000 tokens = 1.24e+22 FLOP (Specialized model: 11B params * 18 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.257556",
      "metadata": {}
    },
    {
      "name": "dolphin_2.2.1_mistral_7b",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.15e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.30e+25 (Low); coding_score: 2.00e+25 (Low) \u2192 weighted average: 2.15e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1088.0,
        "coding_score": 1049.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.257747",
      "metadata": {}
    },
    {
      "name": "granite_3.0_2b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.33e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.32e+25 (Low); coding_score: 2.33e+25 (Low) \u2192 weighted average: 2.33e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1091.0,
        "coding_score": 1104.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 30,000,000,000 tokens = 3.60e+20 FLOP (Generic estimate: 2B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.257864",
      "metadata": {}
    },
    {
      "name": "mpt_30b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 30000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.10e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.13e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.23e+25 (Low); coding_score: 2.04e+25 (Low) \u2192 weighted average: 2.13e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1076.0,
        "coding_score": 1055.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 30,000,000,000 params \u00d7 450,000,000,000 tokens = 8.10e+22 FLOP (Generic estimate: 30B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.257975",
      "metadata": {}
    },
    {
      "name": "mistral_7b_instruct",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.22e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.36e+25 (Low); coding_score: 2.27e+25 (Low); aai_score: 2.32e+22 (Low); mmlu_pro_score: 2.46e+24 (Low) \u2192 weighted average: 1.22e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1097.0,
        "coding_score": 1094.0,
        "aai_score": 1.0,
        "mmlu_pro_score": 24.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.258162",
      "metadata": {}
    },
    {
      "name": "falcon_180b",
      "developer": "TII",
      "release_date": null,
      "parameters": 180000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.92e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.40e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1145.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.40e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1145.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 180,000,000,000 params \u00d7 2,700,000,000,000 tokens = 2.92e+24 FLOP (Generic estimate: 180B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.258273",
      "metadata": {}
    },
    {
      "name": "wizardlm_13b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.14e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.28e+25 (Low); coding_score: 2.00e+25 (Low) \u2192 weighted average: 2.14e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1084.0,
        "coding_score": 1048.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.258392",
      "metadata": {}
    },
    {
      "name": "phi_3_mini_4k_instruct_june",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.42e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Benchmark-based (lmarena_score): 1149.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.42e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1149.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Primarily partners with OpenAI rather than training proprietary frontier models",
      "last_updated": "2025-08-15T18:13:34.258657",
      "metadata": {}
    },
    {
      "name": "llama_2_13b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.22e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.38e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.33e+25 (Low); coding_score: 2.17e+25 (Low); aai_score: 1.48e+24 (Low); mmlu_pro_score: 8.71e+24 (Low) \u2192 weighted average: 1.38e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1093.0,
        "coding_score": 1077.0,
        "aai_score": 8.0,
        "mmlu_pro_score": 40.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 156,000,000,000 tokens = 1.22e+22 FLOP (Mid-era model: 13B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.258816",
      "metadata": {}
    },
    {
      "name": "qwen1.5_7b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.70e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.34e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.31e+25 (Low); coding_score: 2.37e+25 (Low) \u2192 weighted average: 2.34e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1090.0,
        "coding_score": 1110.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 112,000,000,000 tokens = 4.70e+21 FLOP (Chinese model: 7B params * 16 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.258925",
      "metadata": {}
    },
    {
      "name": "vicuna_13b",
      "developer": "LMSYS",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.40e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1146.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.40e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1146.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.259019",
      "metadata": {}
    },
    {
      "name": "codellama_34b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 34000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.39e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.15e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.21e+25 (Low); coding_score: 2.09e+25 (Low) \u2192 weighted average: 2.15e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1073.0,
        "coding_score": 1065.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 34,000,000,000 params \u00d7 680,000,000,000 tokens = 1.39e+23 FLOP (Modern model: 34B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.259128",
      "metadata": {}
    },
    {
      "name": "palm_2",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.37e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1141.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1141.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.37e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.259191",
      "metadata": {}
    },
    {
      "name": "qwen_14b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 14000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.88e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.37e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1140.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.37e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1140.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 14,000,000,000 params \u00d7 224,000,000,000 tokens = 1.88e+22 FLOP (Chinese model: 14B params * 16 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.259436",
      "metadata": {}
    },
    {
      "name": "gemma_7b",
      "developer": "Google",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.36e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1139.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.36e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1139.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.259518",
      "metadata": {}
    },
    {
      "name": "codellama_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.29e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.31e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1131.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.31e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1131.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,260,000,000,000 tokens = 5.29e+23 FLOP (Specialized model: 70B params * 18 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.259594",
      "metadata": {}
    },
    {
      "name": "smollm2_1.7b",
      "developer": "HuggingFace",
      "release_date": null,
      "parameters": 1700000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.60e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.32e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1132.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.32e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1132.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 1,700,000,000 params \u00d7 25,500,000,000 tokens = 2.60e+20 FLOP (Generic estimate: 2B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.259723",
      "metadata": {}
    },
    {
      "name": "zephyr_7b_alpha",
      "developer": "HuggingFace",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.32e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1132.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.32e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1132.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.259860",
      "metadata": {}
    },
    {
      "name": "phi_3_mini_128k",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.35e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Benchmark-based (lmarena_score): 1138.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.35e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1138.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Primarily partners with OpenAI rather than training proprietary frontier models",
      "last_updated": "2025-08-15T18:13:34.260043",
      "metadata": {}
    },
    {
      "name": "zephyr_7b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.13e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.23e+25 (Low); coding_score: 2.02e+25 (Low) \u2192 weighted average: 2.13e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1076.0,
        "coding_score": 1053.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.260167",
      "metadata": {}
    },
    {
      "name": "phi_3_mini_4k",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.29e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.26e+25 (Low); coding_score: 2.32e+25 (Low) \u2192 weighted average: 2.29e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1082.0,
        "coding_score": 1102.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Primarily partners with OpenAI rather than training proprietary frontier models",
      "last_updated": "2025-08-15T18:13:34.260330",
      "metadata": {}
    },
    {
      "name": "guanaco_33b",
      "developer": "UW",
      "release_date": null,
      "parameters": 33000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.80e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.30e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1129.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.30e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1129.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 33,000,000,000 params \u00d7 495,000,000,000 tokens = 9.80e+22 FLOP (Generic estimate: 33B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.260426",
      "metadata": {}
    },
    {
      "name": "stripedhyena_nous_7b",
      "developer": "Together AI",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.27e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1124.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.27e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1124.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.260513",
      "metadata": {}
    },
    {
      "name": "vicuna_7b",
      "developer": "LMSYS",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.24e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1119.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.24e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1119.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.260599",
      "metadata": {}
    },
    {
      "name": "llama_3.2_1b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 1000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.20e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.10e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.17e+25 (Low); coding_score: 2.08e+25 (Low); aai_score: 2.32e+22 (Low); mmlu_pro_score: 1.48e+24 (Low) \u2192 weighted average: 1.10e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1067.0,
        "coding_score": 1063.0,
        "aai_score": 1.0,
        "mmlu_pro_score": 20.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 1,000,000,000 params \u00d7 20,000,000,000 tokens = 1.20e+20 FLOP (Modern model: 1B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.260738",
      "metadata": {}
    },
    {
      "name": "mistral_7b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.21e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1114.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.21e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1114.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.260850",
      "metadata": {}
    },
    {
      "name": "llama_2_7b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "known_specification:llama_2_7b",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.40e+22",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.20e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1113.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.20e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "lmarena_score": 1113.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Known model specification 'llama_2_7b': Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 2,000,000,000,000 tokens = 8.40e+22 FLOP",
      "last_updated": "2025-08-15T18:13:34.260933",
      "metadata": {}
    },
    {
      "name": "gemma_1.1_2b",
      "developer": "Google",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.20e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1112.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.20e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1112.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 30,000,000,000 tokens = 3.60e+20 FLOP (Generic estimate: 2B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.261011",
      "metadata": {}
    },
    {
      "name": "gemma_2b",
      "developer": "Google",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.88e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.08e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1092.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.08e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1092.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 24,000,000,000 tokens = 2.88e+20 FLOP (Mid-era model: 2B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.261197",
      "metadata": {}
    },
    {
      "name": "qwen1.5_4b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 4000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.54e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.07e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1091.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.07e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1091.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 4,000,000,000 params \u00d7 64,000,000,000 tokens = 1.54e+21 FLOP (Chinese model: 4B params * 16 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.261294",
      "metadata": {}
    },
    {
      "name": "olmo_7b",
      "developer": "Allen AI",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.02e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1082.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.02e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1082.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.261378",
      "metadata": {}
    },
    {
      "name": "koala_13b",
      "developer": "UC Berkeley",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.97e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1072.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.97e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1072.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.261461",
      "metadata": {}
    },
    {
      "name": "gpt4all_13b_snoozy",
      "developer": "Nomic AI",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.95e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1068.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.95e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1068.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.261575",
      "metadata": {}
    },
    {
      "name": "alpaca_13b",
      "developer": "Stanford",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.93e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1066.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.93e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1066.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.261637",
      "metadata": {}
    },
    {
      "name": "mpt_7b",
      "developer": "MosaicML",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.92e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1063.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.92e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1063.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.261698",
      "metadata": {}
    },
    {
      "name": "chatglm3_6b",
      "developer": "Tsinghua",
      "release_date": null,
      "parameters": 6000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.89e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.88e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1055.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.88e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1055.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 6,000,000,000 params \u00d7 108,000,000,000 tokens = 3.89e+21 FLOP (Specialized model: 6B params * 18 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.261760",
      "metadata": {}
    },
    {
      "name": "rwkv_4_raven_14b",
      "developer": "RWKV",
      "release_date": null,
      "parameters": 14000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.76e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.82e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1044.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.82e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1044.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 14,000,000,000 params \u00d7 210,000,000,000 tokens = 1.76e+22 FLOP (Generic estimate: 14B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.261942",
      "metadata": {}
    },
    {
      "name": "chatglm2_6b",
      "developer": "Tsinghua",
      "release_date": null,
      "parameters": 6000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.89e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.76e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1033.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.76e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1033.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 6,000,000,000 params \u00d7 108,000,000,000 tokens = 3.89e+21 FLOP (Specialized model: 6B params * 18 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.262067",
      "metadata": {}
    },
    {
      "name": "oasst_pythia_12b",
      "developer": "OpenAssistant",
      "release_date": null,
      "parameters": 12000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.30e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.70e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1021.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.70e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1021.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 12,000,000,000 params \u00d7 180,000,000,000 tokens = 1.30e+22 FLOP (Generic estimate: 12B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.262222",
      "metadata": {}
    },
    {
      "name": "chatglm_6b",
      "developer": "Tsinghua",
      "release_date": null,
      "parameters": 6000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.89e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.62e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1004.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.62e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1004.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 6,000,000,000 params \u00d7 108,000,000,000 tokens = 3.89e+21 FLOP (Specialized model: 6B params * 18 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.262316",
      "metadata": {}
    },
    {
      "name": "fastchat_t5_3b",
      "developer": "LMSYS",
      "release_date": null,
      "parameters": 3000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.72e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.57e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 995.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.57e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 995.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 3,000,000,000 params \u00d7 54,000,000,000 tokens = 9.72e+20 FLOP (Specialized model: 3B params * 18 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.262404",
      "metadata": {}
    },
    {
      "name": "llama_13b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.11e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.49e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 978.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.49e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 978.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 104,000,000,000 tokens = 8.11e+21 FLOP (Early era model: 13B params * 8 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.262486",
      "metadata": {}
    },
    {
      "name": "dolly_v2_12b",
      "developer": "Databricks",
      "release_date": null,
      "parameters": 12000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.30e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.48e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 976.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.48e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 976.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 12,000,000,000 params \u00d7 180,000,000,000 tokens = 1.30e+22 FLOP (Generic estimate: 12B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.262568",
      "metadata": {}
    },
    {
      "name": "stablelm_tuned_alpha_7b",
      "developer": "Stability AI",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.40e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 956.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.40e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 956.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.262686",
      "metadata": {}
    },
    {
      "name": "claude_3.5_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": 250000000000,
      "parameter_source": "known_specification:claude_3.5_sonnet",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "1.50e+25",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Known model specification 'claude_3.5_sonnet': Chinchilla scaling law: 6 \u00d7 250,000,000,000 params \u00d7 10,000,000,000,000 tokens = 1.50e+25 FLOP"
        },
        {
          "flop": "3.81e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.11e+25 (Low); superclue_math: 2.88e+25 (Low); superclue_reasoning: 1.79e+25 (Low); superclue_code: 4.72e+25 (Low); superclue_agents: 5.57e+25 (Low) \u2192 weighted average: 3.81e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "superclue_overall": 65.01,
        "superclue_math": 58.45,
        "superclue_reasoning": 45.56,
        "superclue_code": 78.61,
        "superclue_agents": 77.31
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from benchmarks",
      "last_updated": "2025-08-15T18:13:34.262875",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_pro",
      "developer": "Google",
      "release_date": null,
      "parameters": 300000000000,
      "parameter_source": "known_specification:gemini_1.5_pro",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.16e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "olympic_overall": 35.09,
        "olympic_math": 21.05,
        "olympic_physics": 23.16,
        "olympic_chemistry": 39.74,
        "olympic_biology": 50.0
      },
      "sources": [
        "https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects)"
      ],
      "reasoning": "Known model specification 'gemini_1.5_pro': Chinchilla scaling law: 6 \u00d7 300,000,000,000 params \u00d7 12,000,000,000,000 tokens = 2.16e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.263095",
      "metadata": {}
    },
    {
      "name": "llama_3.1_405b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 405000000000,
      "parameter_source": "known_specification:llama_3.1_405b",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.80e+25",
      "training_flop_confidence": "high",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "3.65e+25",
          "confidence": "high",
          "method": "scaling_laws",
          "reasoning": "Known model specification 'llama_3.1_405b': Chinchilla scaling law: 6 \u00d7 405,000,000,000 params \u00d7 15,000,000,000,000 tokens = 3.65e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_above_1e25",
      "benchmarks": {
        "olympic_overall": 28.67,
        "olympic_math": 17.58,
        "olympic_physics": 18.56,
        "olympic_chemistry": 32.47,
        "olympic_biology": 40.32
      },
      "sources": [
        "https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: High-precision estimate from Meta disclosure",
      "last_updated": "2025-08-15T18:13:34.263281",
      "metadata": {}
    },
    {
      "name": "gemini_1.0_pro",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "olympic_overall": 24.12,
        "olympic_math": 13.28,
        "olympic_physics": 14.09,
        "olympic_chemistry": 26.62,
        "olympic_biology": 32.26
      },
      "sources": [
        "https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-15T18:13:22.130067",
      "metadata": {}
    },
    {
      "name": "gpt_5",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.47e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 75.34,
        "superclue_math": 76.86,
        "superclue_reasoning": 54.2,
        "superclue_code": 87.92,
        "superclue_agents": 83.21
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 5.94e+25 (Low); superclue_math: 5.70e+25 (Low); superclue_reasoning: 2.77e+25 (Low); superclue_code: 6.24e+25 (Low); superclue_agents: 6.69e+25 (Low) \u2192 weighted average: 5.47e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.263532",
      "metadata": {}
    },
    {
      "name": "glm",
      "developer": "Zhipu AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "5.76e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.24e+25 (Low); coding_score: 5.25e+25 (Low); aai_score: 7.27e+25 (Low); mmlu_pro_score: 5.28e+25 (Low) \u2192 weighted average: 5.76e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1431.0,
        "coding_score": 1447.0,
        "aai_score": 56.0,
        "mmlu_pro_score": 83.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited computational resources for 1e25+ FLOP training runs",
      "last_updated": "2025-08-15T18:13:34.263799",
      "metadata": {}
    },
    {
      "name": "claude_opus",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": 175000000000,
      "parameter_source": "known_specification:claude_opus",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.60e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "8.40e+24",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Known model specification 'claude_opus': Chinchilla scaling law: 6 \u00d7 175,000,000,000 params \u00d7 8,000,000,000,000 tokens = 8.40e+24 FLOP"
        },
        {
          "flop": "6.30e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.08e+25 (Low); coding_score: 5.51e+25 (Low); aai_score: 8.62e+25 (Low); mmlu_pro_score: 5.99e+25 (Low) \u2192 weighted average: 6.30e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1417.0,
        "coding_score": 1470.0,
        "aai_score": 61.0,
        "mmlu_pro_score": 87.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from industry analysis",
      "last_updated": "2025-08-15T18:13:34.264049",
      "metadata": {}
    },
    {
      "name": "gemini_2.0_pro",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.44e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1397.0,
        "coding_score": 1396.0,
        "vision_score": 1220.0,
        "aai_score": 38.0,
        "mmlu_pro_score": 80.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.87e+25 (Low); coding_score: 4.72e+25 (Low); aai_score: 3.35e+25 (Low); mmlu_pro_score: 4.82e+25 (Low) \u2192 weighted average: 4.44e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.264235",
      "metadata": {}
    },
    {
      "name": "gemini_2.0_flash",
      "developer": "Google",
      "release_date": null,
      "parameters": 300000000000,
      "parameter_source": "known_specification:gemini_2.0_flash",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.24e+25",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.22e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.58e+25 (Low); coding_score: 4.47e+25 (Low); aai_score: 3.35e+25 (Low); mmlu_pro_score: 4.48e+25 (Low) \u2192 weighted average: 4.22e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1369.0,
        "coding_score": 1371.0,
        "vision_score": 1241.0,
        "aai_score": 38.0,
        "mmlu_pro_score": 78.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'gemini_2.0_flash': Chinchilla scaling law: 6 \u00d7 300,000,000,000 params \u00d7 18,000,000,000,000 tokens = 3.24e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.264421",
      "metadata": {}
    },
    {
      "name": "glm_4.5_air",
      "developer": "Zhipu AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "5.08e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.81e+25 (Low); coding_score: 4.98e+25 (Low); aai_score: 5.56e+25 (Low); mmlu_pro_score: 4.97e+25 (Low) \u2192 weighted average: 5.08e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1391.0,
        "coding_score": 1422.0,
        "aai_score": 49.0,
        "mmlu_pro_score": 81.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited computational resources for 1e25+ FLOP training runs",
      "last_updated": "2025-08-15T18:13:34.264746",
      "metadata": {}
    },
    {
      "name": "gpt_5_mini",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "7.33e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "aai_score": 64.0,
        "mmlu_pro_score": 82.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: aai_score: 9.49e+25 (Low); mmlu_pro_score: 5.17e+25 (Low) \u2192 weighted average: 7.33e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.264934",
      "metadata": {}
    },
    {
      "name": "gpt_oss_120b",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": 120000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.30e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "5.74e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.60e+25 (Low); coding_score: 4.87e+25 (Low); aai_score: 8.62e+25 (Low); mmlu_pro_score: 4.86e+25 (Low) \u2192 weighted average: 5.74e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1371.0,
        "coding_score": 1411.0,
        "aai_score": 61.0,
        "mmlu_pro_score": 80.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 120,000,000,000 params \u00d7 1,800,000,000,000 tokens = 1.30e+24 FLOP (Generic estimate: 120B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.265350",
      "metadata": {}
    },
    {
      "name": "mistral_medium_3",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.23e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1369.0,
        "coding_score": 1387.0,
        "vision_score": 1199.0,
        "aai_score": 39.0,
        "mmlu_pro_score": 76.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.58e+25 (Low); coding_score: 4.63e+25 (Low); aai_score: 3.52e+25 (Low); mmlu_pro_score: 4.17e+25 (Low) \u2192 weighted average: 4.23e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.265691",
      "metadata": {}
    },
    {
      "name": "step_1o_turbo",
      "developer": "StepFun",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "4.33e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 4.29e+25 (Low); coding_score: 4.36e+25 (Low) \u2192 weighted average: 4.33e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1339.0,
        "coding_score": 1360.0,
        "vision_score": 1227.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited computational resources for 1e25+ FLOP training runs",
      "last_updated": "2025-08-15T18:13:34.269776",
      "metadata": {}
    },
    {
      "name": "llama_3.3_nemotron_super_49b_v1.5",
      "developer": "Meta",
      "release_date": null,
      "parameters": 49000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.97e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.29e+25 (Low); coding_score: 4.35e+25 (Low); aai_score: 6.26e+25 (Low); mmlu_pro_score: 4.96e+25 (Low) \u2192 weighted average: 4.97e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1339.0,
        "coding_score": 1359.0,
        "aai_score": 52.0,
        "mmlu_pro_score": 81.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 49,000,000,000 params \u00d7 980,000,000,000 tokens = 2.88e+23 FLOP (Modern model: 49B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.270049",
      "metadata": {}
    },
    {
      "name": "gemini_2.5_flash_lite",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.32e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "aai_score": 44.0,
        "mmlu_pro_score": 75.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: aai_score: 4.49e+25 (Low); mmlu_pro_score: 4.16e+25 (Low) \u2192 weighted average: 4.32e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.270158",
      "metadata": {}
    },
    {
      "name": "command_a",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.56e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.18e+25 (Low); coding_score: 4.13e+25 (Low); aai_score: 2.37e+25 (Low); mmlu_pro_score: 3.55e+25 (Low) \u2192 weighted average: 3.56e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1327.0,
        "coding_score": 1336.0,
        "aai_score": 32.0,
        "mmlu_pro_score": 71.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Smaller AI company with limited computational budget for frontier models",
      "last_updated": "2025-08-15T18:13:34.270453",
      "metadata": {}
    },
    {
      "name": "amazon_nova_chat_05_14",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.73e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1323.0,
        "coding_score": 1337.0,
        "aai_score": 35.0,
        "mmlu_pro_score": 73.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.14e+25 (Low); coding_score: 4.14e+25 (Low); aai_score: 2.84e+25 (Low); mmlu_pro_score: 3.81e+25 (Low) \u2192 weighted average: 3.73e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.270576",
      "metadata": {}
    },
    {
      "name": "claude_3.7_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.02e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1301.0,
        "coding_score": 1341.0,
        "vision_score": 1195.0,
        "aai_score": 37.0,
        "mmlu_pro_score": 80.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.93e+25 (Low); coding_score: 4.18e+25 (Low); aai_score: 3.17e+25 (Low); mmlu_pro_score: 4.79e+25 (Low) \u2192 weighted average: 4.02e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.270755",
      "metadata": {}
    },
    {
      "name": "gpt_5_nano",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.55e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "aai_score": 54.0,
        "mmlu_pro_score": 77.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: aai_score: 6.76e+25 (Low); mmlu_pro_score: 4.34e+25 (Low) \u2192 weighted average: 5.55e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.270924",
      "metadata": {}
    },
    {
      "name": "gpt_oss_20b",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": 20000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.48e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.05e+25 (Low); coding_score: 4.46e+25 (Low); aai_score: 5.56e+25 (Low); mmlu_pro_score: 3.85e+25 (Low) \u2192 weighted average: 4.48e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1314.0,
        "coding_score": 1370.0,
        "aai_score": 49.0,
        "mmlu_pro_score": 73.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 20,000,000,000 params \u00d7 300,000,000,000 tokens = 3.60e+22 FLOP (Generic estimate: 20B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.271129",
      "metadata": {}
    },
    {
      "name": "llama_3.3_nemotron_super_49b_v1",
      "developer": "Meta",
      "release_date": null,
      "parameters": 49000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.06e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.02e+25 (Low); coding_score: 3.99e+25 (Low); aai_score: 3.71e+25 (Low); mmlu_pro_score: 4.53e+25 (Low) \u2192 weighted average: 4.06e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1310.0,
        "coding_score": 1320.0,
        "aai_score": 40.0,
        "mmlu_pro_score": 78.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 49,000,000,000 params \u00d7 980,000,000,000 tokens = 2.88e+23 FLOP (Modern model: 49B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.271264",
      "metadata": {}
    },
    {
      "name": "grok_2_08_13",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.27e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1306.0,
        "coding_score": 1298.0,
        "aai_score": 28.0,
        "mmlu_pro_score": 70.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.98e+25 (Low); coding_score: 3.79e+25 (Low); aai_score: 1.82e+25 (Low); mmlu_pro_score: 3.51e+25 (Low) \u2192 weighted average: 3.27e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.271372",
      "metadata": {}
    },
    {
      "name": "athene_v2_chat_72b",
      "developer": "NexusFlow",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.60e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.93e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.87e+25 (Low); coding_score: 3.99e+25 (Low) \u2192 weighted average: 3.93e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1294.0,
        "coding_score": 1320.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 1,296,000,000,000 tokens = 5.60e+23 FLOP (Specialized model: 72B params * 18 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.271543",
      "metadata": {}
    },
    {
      "name": "yi_lightning_lite",
      "developer": "01 AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.73e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.78e+25 (Low); coding_score: 3.69e+25 (Low) \u2192 weighted average: 3.73e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1284.0,
        "coding_score": 1286.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited computational resources for 1e25 FLOP training runs",
      "last_updated": "2025-08-15T18:13:34.271699",
      "metadata": {}
    },
    {
      "name": "grok_2_mini_08_13",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.70e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1283.0,
        "coding_score": 1279.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.77e+25 (Low); coding_score: 3.63e+25 (Low) \u2192 weighted average: 3.70e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.271784",
      "metadata": {}
    },
    {
      "name": "claude_3.5_haiku",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.76e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 56.12,
        "superclue_math": 49.26,
        "superclue_reasoning": 33.83,
        "superclue_code": 66.93,
        "superclue_agents": 74.48
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.84e+25 (Low); superclue_math: 1.88e+25 (Low); superclue_reasoning: 8.52e+24 (Low); superclue_code: 3.15e+25 (Low); superclue_agents: 5.07e+25 (Low) \u2192 weighted average: 2.76e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.271962",
      "metadata": {}
    },
    {
      "name": "deepseek_v2_api",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.44e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1240.0,
        "coding_score": 1260.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.41e+25 (Low); coding_score: 3.47e+25 (Low) \u2192 weighted average: 3.44e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.272100",
      "metadata": {}
    },
    {
      "name": "mistral_small_3_24b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 24000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.15e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.74e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.39e+25 (Low); coding_score: 3.39e+25 (Low); aai_score: 1.33e+25 (Low); mmlu_pro_score: 2.85e+25 (Low) \u2192 weighted average: 2.74e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1238.0,
        "coding_score": 1251.0,
        "aai_score": 24.0,
        "mmlu_pro_score": 65.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 24,000,000,000 params \u00d7 288,000,000,000 tokens = 4.15e+22 FLOP (Mid-era model: 24B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.272323",
      "metadata": {}
    },
    {
      "name": "yi_large",
      "developer": "01 AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.36e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.37e+25 (Low); coding_score: 3.29e+25 (Low); aai_score: 5.93e+24 (Low); mmlu_pro_score: 2.18e+25 (Low) \u2192 weighted average: 2.36e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1236.0,
        "coding_score": 1238.0,
        "aai_score": 16.0,
        "mmlu_pro_score": 58.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited computational resources for 1e25 FLOP training runs",
      "last_updated": "2025-08-15T18:13:34.272536",
      "metadata": {}
    },
    {
      "name": "aya_expanse_32b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.82e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.32e+25 (Low); coding_score: 3.08e+25 (Low); aai_score: 1.48e+24 (Low); mmlu_pro_score: 7.23e+24 (Low) \u2192 weighted average: 1.82e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1229.0,
        "coding_score": 1211.0,
        "aai_score": 8.0,
        "mmlu_pro_score": 37.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.272682",
      "metadata": {}
    },
    {
      "name": "aya_expanse_8b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.60e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.03e+25 (Low); coding_score: 2.88e+25 (Low); aai_score: 3.71e+23 (Low); mmlu_pro_score: 4.51e+24 (Low) \u2192 weighted average: 1.60e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1193.0,
        "coding_score": 1184.0,
        "aai_score": 4.0,
        "mmlu_pro_score": 31.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.272935",
      "metadata": {}
    },
    {
      "name": "claude_1",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.82e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1178.0,
        "coding_score": 1161.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.92e+25 (Low); coding_score: 2.71e+25 (Low) \u2192 weighted average: 2.82e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.273041",
      "metadata": {}
    },
    {
      "name": "internlm2.5_20b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 20000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.84e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.85e+25 (Low); coding_score: 2.84e+25 (Low) \u2192 weighted average: 2.84e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1168.0,
        "coding_score": 1179.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 20,000,000,000 params \u00d7 300,000,000,000 tokens = 3.60e+22 FLOP (Generic estimate: 20B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.273272",
      "metadata": {}
    },
    {
      "name": "gemini_1.0_pro_001",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.61e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1155.0,
        "coding_score": 1125.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.75e+25 (Low); coding_score: 2.47e+25 (Low) \u2192 weighted average: 2.61e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.273360",
      "metadata": {}
    },
    {
      "name": "claude_instant_1",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.55e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1135.0,
        "coding_score": 1136.0,
        "aai_score": 2.0,
        "mmlu_pro_score": 43.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.61e+25 (Low); coding_score: 2.54e+25 (Low); aai_score: 9.27e+22 (Low); mmlu_pro_score: 1.03e+25 (Low) \u2192 weighted average: 1.55e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.273569",
      "metadata": {}
    },
    {
      "name": "nv_llama2_70b_steerlm",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.20e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.42e+25 (Low); coding_score: 1.99e+25 (Low) \u2192 weighted average: 2.20e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1106.0,
        "coding_score": 1047.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.273783",
      "metadata": {}
    },
    {
      "name": "pplx_70b_online",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.20e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.37e+25 (Low); coding_score: 2.04e+25 (Low) \u2192 weighted average: 2.20e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1099.0,
        "coding_score": 1055.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.273898",
      "metadata": {}
    },
    {
      "name": "phi_3_mini_4k_instruct_june_24",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.30e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.30e+25 (Low); coding_score: 2.29e+25 (Low) \u2192 weighted average: 2.30e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1088.0,
        "coding_score": 1098.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Primarily partners with OpenAI rather than training proprietary frontier models",
      "last_updated": "2025-08-15T18:13:34.274106",
      "metadata": {}
    },
    {
      "name": "qwen2.5_vl_32b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.37e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1199.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 384,000,000,000 tokens = 7.37e+22 FLOP (Mid-era model: 32B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.274184",
      "metadata": {}
    },
    {
      "name": "step_1o_vision_32k",
      "developer": "StepFun",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1169.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-15T18:13:24.629212",
      "metadata": {}
    },
    {
      "name": "qwen2.5_vl_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.73e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1154.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 864,000,000,000 tokens = 3.73e+23 FLOP (Mid-era model: 72B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.274269",
      "metadata": {}
    },
    {
      "name": "pixtral_large",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.49e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1138.0,
        "aai_score": 26.0,
        "mmlu_pro_score": 70.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: aai_score: 1.57e+25 (Low); mmlu_pro_score: 3.41e+25 (Low) \u2192 weighted average: 2.49e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.274360",
      "metadata": {}
    },
    {
      "name": "qwen_vl_max",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1106.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-15T18:13:24.629282",
      "metadata": {}
    },
    {
      "name": "qwen2_vl_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.73e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1094.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 864,000,000,000 tokens = 3.73e+23 FLOP (Mid-era model: 72B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.274527",
      "metadata": {}
    },
    {
      "name": "step_1v_32k",
      "developer": "StepFun",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1093.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-15T18:13:24.629328",
      "metadata": {}
    },
    {
      "name": "molmo_72b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.67e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1060.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 1,080,000,000,000 tokens = 4.67e+23 FLOP (Generic estimate: 72B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.274611",
      "metadata": {}
    },
    {
      "name": "pixtral_12b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 12000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.30e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "7.78e+24",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: aai_score: 2.80e+24 (Low); mmlu_pro_score: 1.28e+25 (Low) \u2192 weighted average: 7.78e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1056.0,
        "aai_score": 11.0,
        "mmlu_pro_score": 47.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 12,000,000,000 params \u00d7 180,000,000,000 tokens = 1.30e+22 FLOP (Generic estimate: 12B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.274729",
      "metadata": {}
    },
    {
      "name": "internvl2_26b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 26000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "6.08e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1053.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 26,000,000,000 params \u00d7 390,000,000,000 tokens = 6.08e+22 FLOP (Generic estimate: 26B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.274801",
      "metadata": {}
    },
    {
      "name": "llama_3.2_90b_vision",
      "developer": "Meta",
      "release_date": null,
      "parameters": 90000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.72e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.09e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: aai_score: 1.12e+25 (Low); mmlu_pro_score: 3.06e+25 (Low) \u2192 weighted average: 2.09e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1047.0,
        "aai_score": 22.0,
        "mmlu_pro_score": 67.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 90,000,000,000 params \u00d7 1,800,000,000,000 tokens = 9.72e+23 FLOP (Modern model: 90B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.274938",
      "metadata": {}
    },
    {
      "name": "hunyuan_standard_vision",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1046.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-15T18:13:24.629522",
      "metadata": {}
    },
    {
      "name": "aya_vision_32b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1042.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.275034",
      "metadata": {}
    },
    {
      "name": "qwen2_vl_7b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1038.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.275093",
      "metadata": {}
    },
    {
      "name": "yi_vision",
      "developer": "01 AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1028.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-15T18:13:24.629590",
      "metadata": {}
    },
    {
      "name": "llama_3.2_11b_vision",
      "developer": "Meta",
      "release_date": null,
      "parameters": 11000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.45e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "8.04e+24",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: aai_score: 3.92e+24 (Low); mmlu_pro_score: 1.22e+25 (Low) \u2192 weighted average: 8.04e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1014.0,
        "aai_score": 13.0,
        "mmlu_pro_score": 46.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 11,000,000,000 params \u00d7 220,000,000,000 tokens = 1.45e+22 FLOP (Modern model: 11B params * 20 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.275218",
      "metadata": {}
    },
    {
      "name": "molmo_7b_d",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1007.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.275282",
      "metadata": {}
    },
    {
      "name": "videopoet",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "1.10e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Threshold-based (physics_iq_score): 20.3 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level)"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "physics_iq_score": 20.3
      },
      "sources": [
        "https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Models with unidentified developers cannot be evaluated for resource availability",
      "last_updated": "2025-08-15T18:13:34.275448",
      "metadata": {}
    },
    {
      "name": "lumiere",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "1.10e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Threshold-based (physics_iq_score): 19.0 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level)"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "physics_iq_score": 19.0
      },
      "sources": [
        "https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Models with unidentified developers cannot be evaluated for resource availability",
      "last_updated": "2025-08-15T18:13:34.275599",
      "metadata": {}
    },
    {
      "name": "runway_gen_3",
      "developer": "Runway",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.10e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "physics_iq_score": 22.8
      },
      "sources": [
        "https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models)"
      ],
      "reasoning": "Threshold-based (physics_iq_score): 22.8 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level)",
      "last_updated": "2025-08-15T18:13:34.275664",
      "metadata": {}
    },
    {
      "name": "stable_video_diffusion",
      "developer": "Stability AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.10e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "physics_iq_score": 14.8
      },
      "sources": [
        "https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models)"
      ],
      "reasoning": "Threshold-based (physics_iq_score): 14.8 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level)",
      "last_updated": "2025-08-15T18:13:34.275801",
      "metadata": {}
    },
    {
      "name": "pika",
      "developer": "Pika Labs",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "1.10e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Threshold-based (physics_iq_score): 13.0 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level)"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "physics_iq_score": 13.0
      },
      "sources": [
        "https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited computational resources for 1e25+ FLOP training runs",
      "last_updated": "2025-08-15T18:13:34.276005",
      "metadata": {}
    },
    {
      "name": "sora",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.00e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "video_arena_elo": 1234.0,
        "video_quality": 84.3
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 1.00e+25 (Medium); video_quality: 1.00e+25 (Medium) \u2192 weighted average: 1.00e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.276099",
      "metadata": {}
    },
    {
      "name": "doubao_seed_1.6",
      "developer": "ByteDance",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "4.90e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.60e+25 (Low); superclue_math: 4.64e+25 (Low); superclue_reasoning: 1.33e+25 (Low); superclue_code: 5.63e+25 (Low); superclue_agents: 8.29e+25 (Low) \u2192 weighted average: 4.90e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 68.04,
        "superclue_math": 70.77,
        "superclue_reasoning": 40.49,
        "superclue_code": 84.36,
        "superclue_agents": 90.67
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited evidence of 1e25+ FLOP computational budget allocation",
      "last_updated": "2025-08-15T18:13:34.276419",
      "metadata": {}
    },
    {
      "name": "claude_opus_4_reasoning",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.10e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 67.02,
        "superclue_math": 60.16,
        "superclue_reasoning": 45.93,
        "superclue_code": 80.4,
        "superclue_agents": 80.6
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.43e+25 (Low); superclue_math: 3.09e+25 (Low); superclue_reasoning: 1.83e+25 (Low); superclue_code: 4.99e+25 (Low); superclue_agents: 6.18e+25 (Low) \u2192 weighted average: 4.10e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.276579",
      "metadata": {}
    },
    {
      "name": "hunyuan_pro",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.60e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.78e+25 (Low); superclue_math: 2.88e+25 (Low); superclue_reasoning: 1.06e+25 (Low); superclue_code: 5.24e+25 (Low); superclue_agents: 5.03e+25 (Low) \u2192 weighted average: 3.60e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 62.91,
        "superclue_math": 58.51,
        "superclue_reasoning": 36.91,
        "superclue_code": 81.98,
        "superclue_agents": 74.23
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited evidence of computational budget allocation for frontier AI models",
      "last_updated": "2025-08-15T18:13:34.276917",
      "metadata": {}
    },
    {
      "name": "doubao_pro",
      "developer": "ByteDance",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.49e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.73e+25 (Low); superclue_math: 3.13e+25 (Low); superclue_reasoning: 1.55e+25 (Low); superclue_code: 3.89e+25 (Low); superclue_agents: 5.15e+25 (Low) \u2192 weighted average: 3.49e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 62.53,
        "superclue_math": 60.45,
        "superclue_reasoning": 42.96,
        "superclue_code": 72.77,
        "superclue_agents": 74.93
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited evidence of 1e25+ FLOP computational budget allocation",
      "last_updated": "2025-08-15T18:13:34.277156",
      "metadata": {}
    },
    {
      "name": "step_2_16k",
      "developer": "StepFun",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.41e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.61e+25 (Low); superclue_math: 3.99e+25 (Low); superclue_reasoning: 1.10e+25 (Low); superclue_code: 3.52e+25 (Low); superclue_agents: 4.83e+25 (Low) \u2192 weighted average: 3.41e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 61.74,
        "superclue_math": 66.61,
        "superclue_reasoning": 37.41,
        "superclue_code": 69.9,
        "superclue_agents": 73.04
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited computational resources for 1e25+ FLOP training runs",
      "last_updated": "2025-08-15T18:13:34.277396",
      "metadata": {}
    },
    {
      "name": "minimax_01",
      "developer": "MiniMax",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.36e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.55e+25 (Low); superclue_math: 2.48e+25 (Low); superclue_reasoning: 1.18e+25 (Low); superclue_code: 4.23e+25 (Low); superclue_agents: 5.39e+25 (Low) \u2192 weighted average: 3.36e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 61.29,
        "superclue_math": 55.09,
        "superclue_reasoning": 38.52,
        "superclue_code": 75.25,
        "superclue_agents": 76.31
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited computational resources for 1e25+ FLOP training runs",
      "last_updated": "2025-08-15T18:13:34.277645",
      "metadata": {}
    },
    {
      "name": "moonshot",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.30e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.49e+25 (Low); superclue_math: 2.90e+25 (Low); superclue_reasoning: 1.24e+25 (Low); superclue_code: 3.42e+25 (Low); superclue_agents: 5.44e+25 (Low) \u2192 weighted average: 3.30e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 60.89,
        "superclue_math": 58.63,
        "superclue_reasoning": 39.26,
        "superclue_code": 69.11,
        "superclue_agents": 76.57
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Models with unidentified developers cannot be evaluated for resource availability",
      "last_updated": "2025-08-15T18:13:34.277875",
      "metadata": {}
    },
    {
      "name": "baichuan_4",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.10e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.30e+25 (Low); superclue_math: 2.91e+25 (Low); superclue_reasoning: 1.06e+25 (Low); superclue_code: 3.67e+25 (Low); superclue_agents: 4.58e+25 (Low) \u2192 weighted average: 3.10e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 59.54,
        "superclue_math": 58.68,
        "superclue_reasoning": 36.91,
        "superclue_code": 71.09,
        "superclue_agents": 71.48
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Models with unidentified developers cannot be evaluated for resource availability",
      "last_updated": "2025-08-15T18:13:34.278107",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_flash",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.92e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 57.89,
        "superclue_math": 54.55,
        "superclue_reasoning": 35.56,
        "superclue_code": 68.91,
        "superclue_agents": 72.56
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.07e+25 (Low); superclue_math: 2.42e+25 (Low); superclue_reasoning: 9.65e+24 (Low); superclue_code: 3.39e+25 (Low); superclue_agents: 4.75e+25 (Low) \u2192 weighted average: 2.92e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.278252",
      "metadata": {}
    },
    {
      "name": "internlm_3",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.65e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.82e+25 (Low); superclue_math: 3.10e+25 (Low); superclue_reasoning: 1.18e+25 (Low); superclue_code: 2.11e+25 (Low); superclue_agents: 4.04e+25 (Low) \u2192 weighted average: 2.65e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 55.93,
        "superclue_math": 60.22,
        "superclue_reasoning": 38.52,
        "superclue_code": 57.03,
        "superclue_agents": 67.97
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Models with unidentified developers cannot be evaluated for resource availability",
      "last_updated": "2025-08-15T18:13:34.278564",
      "metadata": {}
    },
    {
      "name": "glm_4_flash",
      "developer": "Zhipu AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.60e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before resource constraint cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.70e+25 (Low); superclue_math: 1.93e+25 (Low); superclue_reasoning: 7.05e+24 (Low); superclue_code: 3.67e+25 (Low); superclue_agents: 3.98e+25 (Low) \u2192 weighted average: 2.60e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 54.97,
        "superclue_math": 49.85,
        "superclue_reasoning": 31.36,
        "superclue_code": 71.09,
        "superclue_agents": 67.56
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to resource constraint policy: Limited computational resources for 1e25+ FLOP training runs",
      "last_updated": "2025-08-15T18:13:34.278796",
      "metadata": {}
    },
    {
      "name": "marco_o1",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.54e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 54.76,
        "superclue_math": 60.97,
        "superclue_reasoning": 39.75,
        "superclue_code": 50.59,
        "superclue_agents": 67.73
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.68e+25 (Low); superclue_math: 3.20e+25 (Low); superclue_reasoning: 1.27e+25 (Low); superclue_code: 1.57e+25 (Low); superclue_agents: 4.00e+25 (Low) \u2192 weighted average: 2.54e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.278971",
      "metadata": {}
    },
    {
      "name": "mixtral_8x22b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 22000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.36e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.08e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.22e+25 (Low); superclue_math: 1.98e+25 (Low); superclue_reasoning: 9.73e+24 (Low); superclue_code: 1.84e+25 (Low); superclue_agents: 3.40e+25 (Low) \u2192 weighted average: 2.08e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "superclue_overall": 50.86,
        "superclue_math": 50.33,
        "superclue_reasoning": 35.68,
        "superclue_code": 53.96,
        "superclue_agents": 63.46
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 22,000,000,000 params \u00d7 330,000,000,000 tokens = 4.36e+22 FLOP (Generic estimate: 22B params * 15 tokens/param)",
      "last_updated": "2025-08-15T18:13:34.279223",
      "metadata": {}
    },
    {
      "name": "veo_3_preview",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.10e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "video_arena_elo": 1298.0,
        "video_quality": 89.2
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 1.10e+25 (Medium); video_quality: 1.10e+25 (Medium) \u2192 weighted average: 1.10e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.279299",
      "metadata": {}
    },
    {
      "name": "veo_2",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.10e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "video_arena_elo": 1267.0,
        "video_quality": 86.7
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 1.10e+25 (Medium); video_quality: 1.10e+25 (Medium) \u2192 weighted average: 1.10e+25 FLOP",
      "last_updated": "2025-08-15T18:13:34.279412",
      "metadata": {}
    },
    {
      "name": "seedance",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.25e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "video_arena_elo": 1205.0,
        "video_quality": 81.5
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 9.31e+24 (Low); video_quality: 9.19e+24 (Low) \u2192 weighted average: 9.25e+24 FLOP",
      "last_updated": "2025-08-15T18:13:34.279527",
      "metadata": {}
    },
    {
      "name": "waver",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "8.81e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "video_arena_elo": 1187.0,
        "video_quality": 79.8
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 8.90e+24 (Low); video_quality: 8.72e+24 (Low) \u2192 weighted average: 8.81e+24 FLOP",
      "last_updated": "2025-08-15T18:13:34.279595",
      "metadata": {}
    },
    {
      "name": "runway_gen3",
      "developer": "Runway",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "7.87e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "video_arena_elo": 1156.0,
        "video_quality": 75.2
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 8.22e+24 (Low); video_quality: 7.52e+24 (Low) \u2192 weighted average: 7.87e+24 FLOP",
      "last_updated": "2025-08-15T18:13:34.279670",
      "metadata": {}
    },
    {
      "name": "kling",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "7.35e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "video_arena_elo": 1134.0,
        "video_quality": 72.8
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 7.76e+24 (Low); video_quality: 6.93e+24 (Low) \u2192 weighted average: 7.35e+24 FLOP",
      "last_updated": "2025-08-15T18:13:34.279740",
      "metadata": {}
    }
  ]
}