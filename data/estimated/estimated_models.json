{
  "metadata": {
    "saved_at": "2025-08-01T12:44:10.325179",
    "source": "consolidated_estimated",
    "last_updated": "2025-08-01T12:44:10.325134",
    "model_count": 238,
    "stage": "estimated"
  },
  "models": [
    {
      "name": "gemini_2.5_pro",
      "developer": "Google",
      "release_date": null,
      "parameters": 500000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "6.00e+25",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "5.54e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1474.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 5.54e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1474.0,
        "coding_score": 1472.0,
        "vision_score": 1315.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 500,000,000,000 params \u00d7 20,000,000,000,000 tokens = 6.00e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.269824",
      "metadata": {
        "arena_elo": 1474.0,
        "coding_score": 1472.0,
        "vision_score": 1315.0,
        "aai_score": 70.5,
        "mmlu_pro_score": 86.2,
        "votes": 23460.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "o3",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.07e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1431.0,
        "coding_score": 1442.0,
        "vision_score": 1269.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1431.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 5.07e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.270109",
      "metadata": {
        "arena_elo": 1431.0,
        "coding_score": 1442.0,
        "vision_score": 1269.0,
        "aai_score": 70.0,
        "mmlu_pro_score": 85.3,
        "votes": 29536.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "chatgpt_4o",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": 1760000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.80e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "1.37e+26",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 1,760,000,000,000 params \u00d7 13,000,000,000,000 tokens = 1.37e+26 FLOP"
        },
        {
          "flop": "5.06e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1430.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 5.06e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1430.0,
        "coding_score": 1431.0,
        "vision_score": 1295.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Epoch AI: Low-precision estimate from OpenAI patterns",
      "last_updated": "2025-08-01T12:44:10.270309",
      "metadata": {
        "arena_elo": 1430.0,
        "coding_score": 1431.0,
        "vision_score": 1295.0,
        "aai_score": 50.3,
        "mmlu_pro_score": 80.3,
        "votes": 29006.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gpt_4.5_preview",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": 1760000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "6.40e+25",
      "training_flop_confidence": "low",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "1.37e+26",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 1,760,000,000,000 params \u00d7 13,000,000,000,000 tokens = 1.37e+26 FLOP"
        },
        {
          "flop": "4.93e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1418.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.93e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1418.0,
        "coding_score": 1418.0,
        "vision_score": 1236.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Epoch AI: Low-precision estimate for GPT-4.5",
      "last_updated": "2025-08-01T12:44:10.270483",
      "metadata": {
        "arena_elo": 1418.0,
        "coding_score": 1418.0,
        "vision_score": 1236.0,
        "aai_score": 53.0,
        "mmlu_pro_score": 81.0,
        "votes": 15271.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "grok_4",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.14e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1438.0,
        "coding_score": 1440.0,
        "vision_score": 1266.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1438.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 5.14e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.270597",
      "metadata": {
        "arena_elo": 1438.0,
        "coding_score": 1440.0,
        "vision_score": 1266.0,
        "aai_score": 73.2,
        "mmlu_pro_score": 86.6,
        "votes": 10622.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "claude_opus_4_thinking_16k",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": 175000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.50e+26",
      "training_flop_confidence": "low",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "8.40e+24",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 175,000,000,000 params \u00d7 8,000,000,000,000 tokens = 8.40e+24 FLOP"
        },
        {
          "flop": "4.96e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1421.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.96e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_above_1e25",
      "benchmarks": {
        "lmarena_score": 1421.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Epoch AI: Speculative estimate for next-gen Claude",
      "last_updated": "2025-08-01T12:44:10.270720",
      "metadata": {
        "lmarena_score": 1421.0,
        "lmarena_rank": "5",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "kimi_k2_preview",
      "developer": "Moonshot",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.55e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1380.0,
        "coding_score": 1402.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1380.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.55e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.270919",
      "metadata": {
        "arena_elo": 1380.0,
        "coding_score": 1402.0,
        "aai_score": 57.6,
        "mmlu_pro_score": 82.4,
        "votes": 8752.0,
        "license": "Modified MIT",
        "source": "openlm_arena"
      }
    },
    {
      "name": "deepseek_r1",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": 671000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.00e+24",
      "training_flop_confidence": "high",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "5.96e+25",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 671,000,000,000 params \u00d7 14,800,000,000,000 tokens = 5.96e+25 FLOP"
        },
        {
          "flop": "4.49e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1374.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.49e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1374.0,
        "coding_score": 1379.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1",
      "last_updated": "2025-08-01T12:44:10.271064",
      "metadata": {
        "arena_elo": 1374.0,
        "coding_score": 1379.0,
        "aai_score": 60.2,
        "mmlu_pro_score": 84.4,
        "votes": 19430.0,
        "license": "MIT",
        "source": "openlm_arena"
      }
    },
    {
      "name": "claude_opus_4",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": 175000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.50e+26",
      "training_flop_confidence": "low",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "8.40e+24",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 175,000,000,000 params \u00d7 8,000,000,000,000 tokens = 8.40e+24 FLOP"
        },
        {
          "flop": "4.43e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1368.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.43e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1368.0,
        "coding_score": 1404.0,
        "vision_score": 1210.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Epoch AI: Speculative estimate for next-gen Claude",
      "last_updated": "2025-08-01T12:44:10.271131",
      "metadata": {
        "arena_elo": 1368.0,
        "coding_score": 1404.0,
        "vision_score": 1210.0,
        "aai_score": 57.7,
        "mmlu_pro_score": 86.0,
        "votes": 24372.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "grok_3",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.03e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1427.0,
        "coding_score": 1435.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1427.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 5.03e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.271229",
      "metadata": {
        "arena_elo": 1427.0,
        "coding_score": 1435.0,
        "aai_score": 56.1,
        "mmlu_pro_score": 79.9,
        "votes": 30158.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gemini_2.5_flash",
      "developer": "Google",
      "release_date": null,
      "parameters": 500000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "6.00e+25",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.89e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1414.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.89e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1414.0,
        "coding_score": 1416.0,
        "vision_score": 1275.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 500,000,000,000 params \u00d7 20,000,000,000,000 tokens = 6.00e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.271389",
      "metadata": {
        "arena_elo": 1414.0,
        "coding_score": 1416.0,
        "vision_score": 1275.0,
        "aai_score": 65.1,
        "mmlu_pro_score": 83.2,
        "votes": 28816.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "claude_sonnet_4_thinking_32k",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.75e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1400.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1400.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.75e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.271591",
      "metadata": {
        "lmarena_score": 1400.0,
        "lmarena_rank": "10",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "o1",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.43e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1368.0,
        "coding_score": 1375.0,
        "vision_score": 1215.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1368.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.43e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.271807",
      "metadata": {
        "arena_elo": 1368.0,
        "coding_score": 1375.0,
        "vision_score": 1215.0,
        "aai_score": 61.9,
        "mmlu_pro_score": 84.1,
        "votes": 29038.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwen3_235b_a22b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 235000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.41e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.65e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1390.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.65e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1390.0,
        "coding_score": 1408.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 235,000,000,000 params \u00d7 10,000,000,000,000 tokens = 1.41e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.272142",
      "metadata": {
        "arena_elo": 1390.0,
        "coding_score": 1408.0,
        "aai_score": 60.4,
        "mmlu_pro_score": 82.8,
        "votes": 22745.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "o4_mini",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.37e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1362.0,
        "coding_score": 1381.0,
        "vision_score": 1240.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1362.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.37e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.272313",
      "metadata": {
        "arena_elo": 1362.0,
        "coding_score": 1381.0,
        "vision_score": 1240.0,
        "aai_score": 69.8,
        "mmlu_pro_score": 83.2,
        "votes": 22753.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwen3_coder_480b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 480000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.32e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.43e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1368.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.43e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1368.0,
        "coding_score": 1405.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 480,000,000,000 params \u00d7 15,000,000,000,000 tokens = 4.32e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.285151",
      "metadata": {
        "arena_elo": 1368.0,
        "coding_score": 1405.0,
        "aai_score": 56.7,
        "mmlu_pro_score": 78.8,
        "votes": 2534.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "claude_sonnet_4",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.19e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1343.0,
        "coding_score": 1381.0,
        "vision_score": 1212.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1343.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.19e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.285303",
      "metadata": {
        "arena_elo": 1343.0,
        "coding_score": 1381.0,
        "vision_score": 1212.0,
        "aai_score": 53.0,
        "mmlu_pro_score": 83.7,
        "votes": 20829.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "claude_3_7_sonnet_thinking_32k",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.62e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1387.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1387.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.62e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.285379",
      "metadata": {
        "lmarena_score": 1387.0,
        "lmarena_rank": "14",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "o1_preview",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.60e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1385.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1385.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.60e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.285440",
      "metadata": {
        "lmarena_score": 1385.0,
        "lmarena_rank": "17",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "mistral_medium",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.80e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1173.0,
        "coding_score": 1171.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1173.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.80e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.285502",
      "metadata": {
        "arena_elo": 1173.0,
        "coding_score": 1171.0,
        "aai_score": 22.8,
        "mmlu_pro_score": 49.1,
        "votes": 35556.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "hunyuan_turbos",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.52e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1377.0,
        "coding_score": 1383.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1377.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.52e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.285560",
      "metadata": {
        "arena_elo": 1377.0,
        "coding_score": 1383.0,
        "mmlu_pro_score": 78.0,
        "votes": 10808.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "minimax_m1",
      "developer": "MiniMax",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.32e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1357.0,
        "coding_score": 1368.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1357.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.32e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.285615",
      "metadata": {
        "arena_elo": 1357.0,
        "coding_score": 1368.0,
        "aai_score": 63.0,
        "mmlu_pro_score": 81.6,
        "votes": 15126.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gpt_4.1_mini",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": 1760000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.10e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "1.37e+26",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 1,760,000,000,000 params \u00d7 13,000,000,000,000 tokens = 1.37e+26 FLOP"
        },
        {
          "flop": "4.14e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1338.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.14e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1338.0,
        "coding_score": 1367.0,
        "vision_score": 1230.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Epoch AI: Low-precision estimate from scaling analysis",
      "last_updated": "2025-08-01T12:44:10.285693",
      "metadata": {
        "arena_elo": 1338.0,
        "coding_score": 1367.0,
        "vision_score": 1230.0,
        "aai_score": 52.6,
        "mmlu_pro_score": 78.1,
        "votes": 22156.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwen3_235b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 235000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.41e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.43e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1368.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.43e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1368.0,
        "coding_score": 1393.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 235,000,000,000 params \u00d7 10,000,000,000,000 tokens = 1.41e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.285768",
      "metadata": {
        "arena_elo": 1368.0,
        "coding_score": 1393.0,
        "aai_score": 62.3,
        "mmlu_pro_score": 82.8,
        "votes": 18971.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwen2.5_max",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.43e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1368.0,
        "coding_score": 1371.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1368.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.43e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.285830",
      "metadata": {
        "arena_elo": 1368.0,
        "coding_score": 1371.0,
        "aai_score": 45.3,
        "mmlu_pro_score": 76.2,
        "votes": 34768.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "claude_3_7_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.47e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1372.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1372.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.47e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.285887",
      "metadata": {
        "lmarena_score": 1372.0,
        "lmarena_rank": "24",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "claude_3_5_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.16e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1340.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1340.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.16e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.285998",
      "metadata": {
        "lmarena_score": 1340.0,
        "lmarena_rank": "39",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "gemini_2.0_flash_001",
      "developer": "Google",
      "release_date": null,
      "parameters": 500000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "6.00e+25",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.41e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1366.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.41e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_above_1e25",
      "benchmarks": {
        "lmarena_score": 1366.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 500,000,000,000 params \u00d7 20,000,000,000,000 tokens = 6.00e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.286079",
      "metadata": {
        "lmarena_score": 1366.0,
        "lmarena_rank": "25",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "o3_mini",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.98e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1320.0,
        "coding_score": 1359.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1320.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.98e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.286141",
      "metadata": {
        "arena_elo": 1320.0,
        "coding_score": 1359.0,
        "aai_score": 62.9,
        "mmlu_pro_score": 79.1,
        "votes": 41489.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gemma_3_27b",
      "developer": "Google",
      "release_date": null,
      "parameters": 27000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.75e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.32e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1357.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.32e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1357.0,
        "coding_score": 1343.0,
        "vision_score": 1208.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 27,000,000,000 params \u00d7 540,000,000,000 tokens = 8.75e+22 FLOP (Modern model: 27B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.286233",
      "metadata": {
        "arena_elo": 1357.0,
        "coding_score": 1343.0,
        "vision_score": 1208.0,
        "aai_score": 37.6,
        "mmlu_pro_score": 66.9,
        "votes": 30519.0,
        "license": "Gemma",
        "source": "openlm_arena"
      }
    },
    {
      "name": "grok_3_mini",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.38e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1363.0,
        "coding_score": 1373.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1363.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.38e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.286278",
      "metadata": {
        "arena_elo": 1363.0,
        "coding_score": 1373.0,
        "aai_score": 66.7,
        "mmlu_pro_score": 82.8,
        "votes": 8614.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "llama_3.1_nemotron_ultra_253b",
      "developer": "Nvidia",
      "release_date": null,
      "parameters": 253000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.00e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1322.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.00e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1322.0,
        "coding_score": 1344.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 253,000,000,000 params \u00d7 3,795,000,000,000 tokens = 5.76e+24 FLOP (Modern large model: 253B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.286368",
      "metadata": {
        "arena_elo": 1322.0,
        "coding_score": 1344.0,
        "aai_score": 60.8,
        "mmlu_pro_score": 82.5,
        "votes": 2656.0,
        "license": "Nvidia Open Model",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gemini_2.0_flash_lite",
      "developer": "Google",
      "release_date": null,
      "parameters": 500000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "6.00e+25",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.09e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1332.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.09e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1332.0,
        "coding_score": 1335.0,
        "vision_score": 1145.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 500,000,000,000 params \u00d7 20,000,000,000,000 tokens = 6.00e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.286406",
      "metadata": {
        "arena_elo": 1332.0,
        "coding_score": 1335.0,
        "vision_score": 1145.0,
        "aai_score": 41.4,
        "mmlu_pro_score": 72.4,
        "votes": 26104.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gemini_1.5_pro_002",
      "developer": "Google",
      "release_date": null,
      "parameters": 300000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.16e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.00e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1322.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.00e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1322.0,
        "coding_score": 1311.0,
        "vision_score": 1207.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 300,000,000,000 params \u00d7 12,000,000,000,000 tokens = 2.16e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.286496",
      "metadata": {
        "arena_elo": 1322.0,
        "coding_score": 1311.0,
        "vision_score": 1207.0,
        "aai_score": 44.6,
        "mmlu_pro_score": 75.0,
        "votes": 58645.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "mistral_small",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.12e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1335.0,
        "coding_score": 1361.0,
        "vision_score": 1180.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1335.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.12e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.286556",
      "metadata": {
        "arena_elo": 1335.0,
        "coding_score": 1361.0,
        "vision_score": 1180.0,
        "aai_score": 42.3,
        "mmlu_pro_score": 68.1,
        "votes": 7493.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "command_a_03",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.23e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1347.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1347.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.23e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.286613",
      "metadata": {
        "lmarena_score": 1347.0,
        "lmarena_rank": "34",
        "license": "CC-BY-NC-4.0",
        "source": "lmarena"
      }
    },
    {
      "name": "qwen_plus",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.64e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1241.0,
        "coding_score": 1259.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1241.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.64e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.286669",
      "metadata": {
        "arena_elo": 1241.0,
        "coding_score": 1259.0,
        "votes": 14626.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwen3_32b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.23e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.16e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1340.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.16e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1340.0,
        "coding_score": 1373.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 640,000,000,000 tokens = 1.23e+23 FLOP (Modern model: 32B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.286742",
      "metadata": {
        "arena_elo": 1340.0,
        "coding_score": 1373.0,
        "aai_score": 59.2,
        "mmlu_pro_score": 79.8,
        "votes": 4074.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "glm_4_plus",
      "developer": "Zhipu",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.75e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1294.0,
        "coding_score": 1298.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1294.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.75e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.286771",
      "metadata": {
        "arena_elo": 1294.0,
        "coding_score": 1298.0,
        "mmlu_pro_score": 70.2,
        "votes": 27788.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "hunyuan_turbo",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.93e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1315.0,
        "coding_score": 1334.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1315.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.93e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.286829",
      "metadata": {
        "arena_elo": 1315.0,
        "coding_score": 1334.0,
        "votes": 2510.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gemma_3_12b",
      "developer": "Google",
      "release_date": null,
      "parameters": 12000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.73e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.10e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1333.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.10e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1333.0,
        "coding_score": 1308.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 12,000,000,000 params \u00d7 240,000,000,000 tokens = 1.73e+22 FLOP (Modern model: 12B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.286901",
      "metadata": {
        "arena_elo": 1333.0,
        "coding_score": 1308.0,
        "aai_score": 33.8,
        "mmlu_pro_score": 59.5,
        "votes": 3976.0,
        "license": "Gemma",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gpt_4o",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": 1760000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.80e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "1.37e+26",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 1,760,000,000,000 params \u00d7 13,000,000,000,000 tokens = 1.37e+26 FLOP"
        },
        {
          "flop": "3.84e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1304.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.84e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1304.0,
        "coding_score": 1305.0,
        "vision_score": 1181.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Epoch AI: Low-precision estimate from OpenAI patterns",
      "last_updated": "2025-08-01T12:44:10.286945",
      "metadata": {
        "arena_elo": 1304.0,
        "coding_score": 1305.0,
        "vision_score": 1181.0,
        "aai_score": 41.5,
        "mmlu_pro_score": 74.8,
        "votes": 117747.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwq_32b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.09e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1332.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.09e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1332.0,
        "coding_score": 1343.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.287023",
      "metadata": {
        "arena_elo": 1332.0,
        "coding_score": 1343.0,
        "aai_score": 58.1,
        "mmlu_pro_score": 76.4,
        "votes": 20973.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "step_2",
      "developer": "StepFun",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.00e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1322.0,
        "coding_score": 1312.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1322.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.00e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.287050",
      "metadata": {
        "arena_elo": 1322.0,
        "coding_score": 1312.0,
        "votes": 5126.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "o1_mini",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.98e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1320.0,
        "coding_score": 1364.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1320.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.98e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.287106",
      "metadata": {
        "arena_elo": 1320.0,
        "coding_score": 1364.0,
        "aai_score": 53.8,
        "mmlu_pro_score": 74.2,
        "votes": 54951.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "llama_3.1_405b_instruct_bf16",
      "developer": "Meta",
      "release_date": null,
      "parameters": 405000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.80e+25",
      "training_flop_confidence": "high",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "3.65e+25",
          "confidence": "high",
          "method": "scaling_laws",
          "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 405,000,000,000 params \u00d7 15,000,000,000,000 tokens = 3.65e+25 FLOP"
        },
        {
          "flop": "3.69e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1287.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.69e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1287.0,
        "coding_score": 1298.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Epoch AI: High-precision estimate from Meta disclosure",
      "last_updated": "2025-08-01T12:44:10.287183",
      "metadata": {
        "arena_elo": 1287.0,
        "coding_score": 1298.0,
        "aai_score": 40.5,
        "mmlu_pro_score": 73.2,
        "votes": 43788.0,
        "license": "Llama 3.1",
        "source": "openlm_arena"
      }
    },
    {
      "name": "llama_3.1_405b_instruct_fp8",
      "developer": "Meta",
      "release_date": null,
      "parameters": 405000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.80e+25",
      "training_flop_confidence": "high",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "3.65e+25",
          "confidence": "high",
          "method": "scaling_laws",
          "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 405,000,000,000 params \u00d7 15,000,000,000,000 tokens = 3.65e+25 FLOP"
        },
        {
          "flop": "3.68e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1286.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.68e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1286.0,
        "coding_score": 1290.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Epoch AI: High-precision estimate from Meta disclosure",
      "last_updated": "2025-08-01T12:44:10.287286",
      "metadata": {
        "arena_elo": 1286.0,
        "coding_score": 1290.0,
        "aai_score": 40.5,
        "mmlu_pro_score": 73.2,
        "votes": 63038.0,
        "license": "Llama 3.1",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gemini_advanced",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.09e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1332.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1332.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.09e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.287631",
      "metadata": {
        "lmarena_score": 1332.0,
        "lmarena_rank": "43",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "grok_2",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.00e+25",
      "training_flop_confidence": "high",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "4.09e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1332.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.09e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1332.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Epoch AI: High-precision estimate from xAI disclosure",
      "last_updated": "2025-08-01T12:44:10.287715",
      "metadata": {
        "lmarena_score": 1332.0,
        "lmarena_rank": "43",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "qwen3_30b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 30000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.08e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.01e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1323.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.01e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1323.0,
        "coding_score": 1342.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 30,000,000,000 params \u00d7 600,000,000,000 tokens = 1.08e+23 FLOP (Modern model: 30B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.287792",
      "metadata": {
        "arena_elo": 1323.0,
        "coding_score": 1342.0,
        "aai_score": 55.6,
        "mmlu_pro_score": 77.7,
        "votes": 18763.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "llama_4_maverick_17b_128e",
      "developer": "Meta",
      "release_date": null,
      "parameters": 17000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.47e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.73e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1292.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.73e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1292.0,
        "coding_score": 1313.0,
        "vision_score": 1183.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 17,000,000,000 params \u00d7 340,000,000,000 tokens = 3.47e+22 FLOP (Modern model: 17B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.287836",
      "metadata": {
        "arena_elo": 1292.0,
        "coding_score": 1313.0,
        "vision_score": 1183.0,
        "aai_score": 50.5,
        "mmlu_pro_score": 80.9,
        "votes": 22839.0,
        "license": "Llama 4",
        "source": "openlm_arena"
      }
    },
    {
      "name": "llama_3.3_nemotron_49b_super",
      "developer": "Nvidia",
      "release_date": null,
      "parameters": 49000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.02e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1325.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.02e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1325.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 49,000,000,000 params \u00d7 980,000,000,000 tokens = 2.88e+23 FLOP (Modern model: 49B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.287883",
      "metadata": {
        "lmarena_score": 1325.0,
        "lmarena_rank": "43",
        "license": "Nvidia",
        "source": "lmarena"
      }
    },
    {
      "name": "yi_lightning",
      "developer": "01 AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.84e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1305.0,
        "coding_score": 1321.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1305.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.84e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.287911",
      "metadata": {
        "arena_elo": 1305.0,
        "coding_score": 1321.0,
        "votes": 28968.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "hunyuan_large",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.71e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1290.0,
        "coding_score": 1310.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1290.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.71e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.287971",
      "metadata": {
        "arena_elo": 1290.0,
        "coding_score": 1310.0,
        "votes": 3856.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gpt_4.1_nano",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": 1760000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.10e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "1.37e+26",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 1,760,000,000,000 params \u00d7 13,000,000,000,000 tokens = 1.37e+26 FLOP"
        },
        {
          "flop": "3.70e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1289.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.70e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1289.0,
        "coding_score": 1310.0,
        "vision_score": 1113.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Epoch AI: Low-precision estimate from scaling analysis",
      "last_updated": "2025-08-01T12:44:10.288043",
      "metadata": {
        "arena_elo": 1289.0,
        "coding_score": 1310.0,
        "vision_score": 1113.0,
        "aai_score": 41.0,
        "mmlu_pro_score": 65.7,
        "votes": 6302.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gpt_4_turbo",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": 1760000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.10e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "1.37e+26",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 1,760,000,000,000 params \u00d7 13,000,000,000,000 tokens = 1.37e+26 FLOP"
        },
        {
          "flop": "3.59e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1276.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.59e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1276.0,
        "coding_score": 1278.0,
        "vision_score": 1133.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Epoch AI: Low-precision estimate from scaling analysis",
      "last_updated": "2025-08-01T12:44:10.288124",
      "metadata": {
        "arena_elo": 1276.0,
        "coding_score": 1278.0,
        "vision_score": 1133.0,
        "aai_score": 38.8,
        "mmlu_pro_score": 69.4,
        "votes": 102133.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "claude_3_opus",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": 175000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.60e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "8.40e+24",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 175,000,000,000 params \u00d7 8,000,000,000,000 tokens = 8.40e+24 FLOP"
        },
        {
          "flop": "2.81e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1267.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.81e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1267.0,
        "coding_score": 1268.0,
        "vision_score": 1066.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Epoch AI: Low-precision estimate from industry analysis",
      "last_updated": "2025-08-01T12:44:10.288197",
      "metadata": {
        "arena_elo": 1267.0,
        "coding_score": 1268.0,
        "vision_score": 1066.0,
        "aai_score": 35.1,
        "mmlu_pro_score": 69.6,
        "votes": 202641.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gemini_1.5_pro_001",
      "developer": "Google",
      "release_date": null,
      "parameters": 300000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.16e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.98e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1320.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.98e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1320.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 300,000,000,000 params \u00d7 12,000,000,000,000 tokens = 2.16e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.288296",
      "metadata": {
        "lmarena_score": 1320.0,
        "lmarena_rank": "58",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "amazon_nova_experimental_chat_05_14",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.96e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1318.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1318.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.96e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.288363",
      "metadata": {
        "lmarena_score": 1318.0,
        "lmarena_rank": "58",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "llama_4_scout_17b_16e",
      "developer": "Meta",
      "release_date": null,
      "parameters": 17000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.47e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.61e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1278.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.61e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1278.0,
        "coding_score": 1283.0,
        "vision_score": 1169.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 17,000,000,000 params \u00d7 340,000,000,000 tokens = 3.47e+22 FLOP (Modern model: 17B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.288440",
      "metadata": {
        "arena_elo": 1278.0,
        "coding_score": 1283.0,
        "vision_score": 1169.0,
        "aai_score": 43.0,
        "mmlu_pro_score": 75.2,
        "votes": 12455.0,
        "license": "Llama 4",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gemma_3n_e4b",
      "developer": "Google",
      "release_date": null,
      "parameters": 4000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.92e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.84e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1305.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.84e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1305.0,
        "coding_score": 1292.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 4,000,000,000 params \u00d7 80,000,000,000 tokens = 1.92e+21 FLOP (Modern model: 4B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.288481",
      "metadata": {
        "arena_elo": 1305.0,
        "coding_score": 1292.0,
        "aai_score": 28.0,
        "mmlu_pro_score": 48.8,
        "votes": 11642.0,
        "license": "Gemma",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwen_max",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.15e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1200.0,
        "coding_score": 1205.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1200.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.15e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.288509",
      "metadata": {
        "arena_elo": 1200.0,
        "coding_score": 1205.0,
        "votes": 25696.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "llama_3.3_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.59e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1276.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.59e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1276.0,
        "coding_score": 1277.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,400,000,000,000 tokens = 5.88e+23 FLOP (Modern model: 70B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.288583",
      "metadata": {
        "arena_elo": 1276.0,
        "coding_score": 1277.0,
        "aai_score": 41.1,
        "mmlu_pro_score": 71.3,
        "votes": 50580.0,
        "license": "Llama 3.3",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwen2.5_plus",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.93e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1315.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1315.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.93e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.288613",
      "metadata": {
        "lmarena_score": 1315.0,
        "lmarena_rank": "61",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "claude_3_5_haiku",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.95e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1317.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1317.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.95e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.288682",
      "metadata": {
        "lmarena_score": 1317.0,
        "lmarena_rank": "62",
        "license": "Propretary",
        "source": "lmarena"
      }
    },
    {
      "name": "gpt_4o_mini",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": 1760000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.80e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "1.37e+26",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 1,760,000,000,000 params \u00d7 13,000,000,000,000 tokens = 1.37e+26 FLOP"
        },
        {
          "flop": "3.72e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1291.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.72e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1291.0,
        "coding_score": 1297.0,
        "vision_score": 1110.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Epoch AI: Low-precision estimate from OpenAI patterns",
      "last_updated": "2025-08-01T12:44:10.288790",
      "metadata": {
        "arena_elo": 1291.0,
        "coding_score": 1297.0,
        "vision_score": 1110.0,
        "aai_score": 35.7,
        "mmlu_pro_score": 64.8,
        "votes": 72425.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gpt_4_preview",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": 1760000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.10e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "1.37e+26",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 1,760,000,000,000 params \u00d7 13,000,000,000,000 tokens = 1.37e+26 FLOP"
        },
        {
          "flop": "2.82e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1268.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.82e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1268.0,
        "coding_score": 1260.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Epoch AI: Low-precision estimate from scaling analysis",
      "last_updated": "2025-08-01T12:44:10.288904",
      "metadata": {
        "arena_elo": 1268.0,
        "coding_score": 1260.0,
        "votes": 97079.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "athene",
      "developer": "NexusFlow",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.93e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1315.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1315.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.93e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.288996",
      "metadata": {
        "lmarena_score": 1315.0,
        "lmarena_rank": "62",
        "license": "NexusFlow",
        "source": "lmarena"
      }
    },
    {
      "name": "hunyuan_standard",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.61e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1278.0,
        "coding_score": 1286.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1278.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.61e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.289088",
      "metadata": {
        "arena_elo": 1278.0,
        "coding_score": 1286.0,
        "votes": 4014.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gemini_1.5_flash_002",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.73e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1292.0,
        "coding_score": 1272.0,
        "vision_score": 1186.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1292.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.73e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.289187",
      "metadata": {
        "arena_elo": 1292.0,
        "coding_score": 1272.0,
        "vision_score": 1186.0,
        "aai_score": 39.0,
        "mmlu_pro_score": 68.0,
        "votes": 37021.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "mistral_large",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.84e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1271.0,
        "coding_score": 1281.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1271.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.84e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.289310",
      "metadata": {
        "arena_elo": 1271.0,
        "coding_score": 1281.0,
        "aai_score": 38.3,
        "mmlu_pro_score": 69.7,
        "votes": 29633.0,
        "license": "MRL",
        "source": "openlm_arena"
      }
    },
    {
      "name": "magistral_medium",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.75e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1257.0,
        "coding_score": 1301.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1257.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.75e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.289405",
      "metadata": {
        "arena_elo": 1257.0,
        "coding_score": 1301.0,
        "aai_score": 56.0,
        "mmlu_pro_score": 75.3,
        "votes": 7459.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gemma_3_4b",
      "developer": "Google",
      "release_date": null,
      "parameters": 4000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.92e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.74e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1293.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.74e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1293.0,
        "coding_score": 1263.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 4,000,000,000 params \u00d7 80,000,000,000 tokens = 1.92e+21 FLOP (Modern model: 4B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.293083",
      "metadata": {
        "arena_elo": 1293.0,
        "coding_score": 1263.0,
        "aai_score": 25.4,
        "mmlu_pro_score": 41.7,
        "votes": 4321.0,
        "license": "Gemma",
        "source": "openlm_arena"
      }
    },
    {
      "name": "grok_2_mini",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.00e+25",
      "training_flop_confidence": "high",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "3.86e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1307.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.86e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1307.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Epoch AI: High-precision estimate from xAI disclosure",
      "last_updated": "2025-08-01T12:44:10.293147",
      "metadata": {
        "lmarena_score": 1307.0,
        "lmarena_rank": "73",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "athene_70b",
      "developer": "NexusFlow",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.83e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1270.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.83e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1270.0,
        "coding_score": 1278.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.293342",
      "metadata": {
        "arena_elo": 1270.0,
        "coding_score": 1278.0,
        "votes": 20580.0,
        "license": "CC-BY-NC-4.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwen2.5_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.73e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.86e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1274.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.86e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1274.0,
        "coding_score": 1301.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 864,000,000,000 tokens = 3.73e+23 FLOP (Mid-era model: 72B params * 12 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.293421",
      "metadata": {
        "arena_elo": 1274.0,
        "coding_score": 1301.0,
        "aai_score": 40.4,
        "mmlu_pro_score": 72.0,
        "votes": 41519.0,
        "license": "Qwen",
        "source": "openlm_arena"
      }
    },
    {
      "name": "llama_3.1_nemotron_70b",
      "developer": "Nvidia",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.68e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1286.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.68e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1286.0,
        "coding_score": 1289.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,400,000,000,000 tokens = 5.88e+23 FLOP (Modern model: 70B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.293485",
      "metadata": {
        "arena_elo": 1286.0,
        "coding_score": 1289.0,
        "aai_score": 37.3,
        "mmlu_pro_score": 69.0,
        "votes": 7577.0,
        "license": "Llama 3.1",
        "source": "openlm_arena"
      }
    },
    {
      "name": "hunyuan_large_vision",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.82e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1268.0,
        "coding_score": 1288.0,
        "vision_score": 1233.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1268.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.82e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.293523",
      "metadata": {
        "arena_elo": 1268.0,
        "coding_score": 1288.0,
        "vision_score": 1233.0,
        "votes": 5759.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "mistral_small_3.1_24b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 24000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.15e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.83e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1270.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.83e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1270.0,
        "coding_score": 1294.0,
        "vision_score": 1155.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 24,000,000,000 params \u00d7 288,000,000,000 tokens = 4.15e+22 FLOP (Mid-era model: 24B params * 12 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.293649",
      "metadata": {
        "arena_elo": 1270.0,
        "coding_score": 1294.0,
        "vision_score": 1155.0,
        "aai_score": 35.3,
        "mmlu_pro_score": 65.9,
        "votes": 12385.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "llama_3.1_tulu_3_70b",
      "developer": "Ai2",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.78e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1262.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.78e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1262.0,
        "coding_score": 1254.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,400,000,000,000 tokens = 5.88e+23 FLOP (Modern model: 70B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.293703",
      "metadata": {
        "arena_elo": 1262.0,
        "coding_score": 1254.0,
        "votes": 3010.0,
        "license": "Llama 3.1",
        "source": "openlm_arena"
      }
    },
    {
      "name": "llama_3.1_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "6.30e+24",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.81e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1267.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.81e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1267.0,
        "coding_score": 1266.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 15,000,000,000,000 tokens = 6.30e+24 FLOP",
      "last_updated": "2025-08-01T12:44:10.293749",
      "metadata": {
        "arena_elo": 1267.0,
        "coding_score": 1266.0,
        "aai_score": 35.4,
        "mmlu_pro_score": 67.6,
        "votes": 58637.0,
        "license": "Llama 3.1",
        "source": "openlm_arena"
      }
    },
    {
      "name": "llama_3.1_nemotron_51b",
      "developer": "Nvidia",
      "release_date": null,
      "parameters": 51000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.12e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.60e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1235.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.60e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1235.0,
        "coding_score": 1225.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 51,000,000,000 params \u00d7 1,020,000,000,000 tokens = 3.12e+23 FLOP (Modern model: 51B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.293799",
      "metadata": {
        "arena_elo": 1235.0,
        "coding_score": 1225.0,
        "votes": 3889.0,
        "license": "Llama 3.1",
        "source": "openlm_arena"
      }
    },
    {
      "name": "amazon_nova_pro",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.79e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1263.0,
        "coding_score": 1282.0,
        "vision_score": 1027.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1263.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.79e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.293835",
      "metadata": {
        "arena_elo": 1263.0,
        "coding_score": 1282.0,
        "vision_score": 1027.0,
        "aai_score": 37.1,
        "mmlu_pro_score": 69.1,
        "votes": 26371.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "jamba_1.5_large",
      "developer": "AI21 Labs",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.66e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1244.0,
        "coding_score": 1243.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1244.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.66e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.293918",
      "metadata": {
        "arena_elo": 1244.0,
        "coding_score": 1243.0,
        "aai_score": 29.3,
        "mmlu_pro_score": 57.2,
        "votes": 9125.0,
        "license": "Jamba Open",
        "source": "openlm_arena"
      }
    },
    {
      "name": "reka_core",
      "developer": "Reka AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.71e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1252.0,
        "coding_score": 1226.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1252.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.71e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.293987",
      "metadata": {
        "arena_elo": 1252.0,
        "coding_score": 1226.0,
        "votes": 13279.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gpt_4",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": 1760000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.10e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "1.37e+26",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 1,760,000,000,000 params \u00d7 13,000,000,000,000 tokens = 1.37e+26 FLOP"
        },
        {
          "flop": "3.60e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1277.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.60e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1277.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Epoch AI: Low-precision estimate from scaling analysis",
      "last_updated": "2025-08-01T12:44:10.294090",
      "metadata": {
        "lmarena_score": 1277.0,
        "lmarena_rank": "97",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "gemma_2_27b",
      "developer": "Google",
      "release_date": null,
      "parameters": 27000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.25e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.62e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1237.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.62e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1237.0,
        "coding_score": 1223.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 27,000,000,000 params \u00d7 324,000,000,000 tokens = 5.25e+22 FLOP (Mid-era model: 27B params * 12 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.294235",
      "metadata": {
        "arena_elo": 1237.0,
        "coding_score": 1223.0,
        "aai_score": 31.7,
        "mmlu_pro_score": 57.5,
        "votes": 79538.0,
        "license": "Gemma",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gemini_1.5_flash_001",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.66e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1284.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1284.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.66e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.294324",
      "metadata": {
        "lmarena_score": 1284.0,
        "lmarena_rank": "92",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "gemma_2_9b_it_simpo",
      "developer": "Princeton",
      "release_date": null,
      "parameters": 9000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.83e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.61e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1236.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.61e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1236.0,
        "coding_score": 1210.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 9,000,000,000 params \u00d7 108,000,000,000 tokens = 5.83e+21 FLOP (Mid-era model: 9B params * 12 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.294652",
      "metadata": {
        "arena_elo": 1236.0,
        "coding_score": 1210.0,
        "votes": 10548.0,
        "license": "MIT",
        "source": "openlm_arena"
      }
    },
    {
      "name": "claude_3_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.28e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1224.0,
        "coding_score": 1232.0,
        "vision_score": 1029.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1224.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.28e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.294713",
      "metadata": {
        "arena_elo": 1224.0,
        "coding_score": 1232.0,
        "vision_score": 1029.0,
        "aai_score": 27.8,
        "mmlu_pro_score": 57.9,
        "votes": 113067.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "command_r_plus_08",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.64e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1281.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1281.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.64e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.294843",
      "metadata": {
        "lmarena_score": 1281.0,
        "lmarena_rank": "93",
        "license": "CC-BY-NC-4.0",
        "source": "lmarena"
      }
    },
    {
      "name": "nemotron_4_340b",
      "developer": "Nvidia",
      "release_date": null,
      "parameters": 340000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.04e+25",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.58e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1231.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.58e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1231.0,
        "coding_score": 1220.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 340,000,000,000 params \u00d7 5,100,000,000,000 tokens = 1.04e+25 FLOP (Generic estimate: 340B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.294992",
      "metadata": {
        "arena_elo": 1231.0,
        "coding_score": 1220.0,
        "votes": 20608.0,
        "license": "Nvidia Open Model",
        "source": "openlm_arena"
      }
    },
    {
      "name": "reka_flash",
      "developer": "Reka AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.26e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1220.0,
        "coding_score": 1197.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1220.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.26e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.295115",
      "metadata": {
        "arena_elo": 1220.0,
        "coding_score": 1197.0,
        "votes": 13725.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "glm_4",
      "developer": "Zhipu",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.14e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1199.0,
        "coding_score": 1207.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1199.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.14e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.295212",
      "metadata": {
        "arena_elo": 1199.0,
        "coding_score": 1207.0,
        "votes": 7579.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "llama_3_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "6.30e+24",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.55e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1226.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.55e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1226.0,
        "coding_score": 1216.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 15,000,000,000,000 tokens = 6.30e+24 FLOP",
      "last_updated": "2025-08-01T12:44:10.295339",
      "metadata": {
        "arena_elo": 1226.0,
        "coding_score": 1216.0,
        "aai_score": 27.5,
        "mmlu_pro_score": 57.4,
        "votes": 163629.0,
        "license": "Llama 3",
        "source": "openlm_arena"
      }
    },
    {
      "name": "mistral_small_24b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 24000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.15e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.63e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1239.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.63e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1239.0,
        "coding_score": 1252.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 24,000,000,000 params \u00d7 288,000,000,000 tokens = 4.15e+22 FLOP (Mid-era model: 24B params * 12 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.295431",
      "metadata": {
        "arena_elo": 1239.0,
        "coding_score": 1252.0,
        "aai_score": 35.3,
        "mmlu_pro_score": 65.2,
        "votes": 15321.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwen2.5_coder_32b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.37e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.61e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1236.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.61e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1236.0,
        "coding_score": 1275.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 384,000,000,000 tokens = 7.37e+22 FLOP (Mid-era model: 32B params * 12 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.295504",
      "metadata": {
        "arena_elo": 1236.0,
        "coding_score": 1275.0,
        "aai_score": 36.3,
        "mmlu_pro_score": 63.5,
        "votes": 5730.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "c4ai_aya_expanse_32b",
      "developer": "Cohere",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.83e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1269.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.83e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1269.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.295581",
      "metadata": {
        "lmarena_score": 1269.0,
        "lmarena_rank": "104",
        "license": "CC-BY-NC-4.0",
        "source": "lmarena"
      }
    },
    {
      "name": "olmo_2_32b",
      "developer": "Ai2",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.56e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1228.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.56e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1228.0,
        "coding_score": 1210.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.295654",
      "metadata": {
        "arena_elo": 1228.0,
        "coding_score": 1210.0,
        "votes": 3460.0,
        "license": "Apache-2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwen2_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.73e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.21e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1211.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.21e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1211.0,
        "coding_score": 1199.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 864,000,000,000 tokens = 3.73e+23 FLOP (Mid-era model: 72B params * 12 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.295720",
      "metadata": {
        "arena_elo": 1211.0,
        "coding_score": 1199.0,
        "aai_score": 32.6,
        "mmlu_pro_score": 62.2,
        "votes": 38872.0,
        "license": "Qianwen",
        "source": "openlm_arena"
      }
    },
    {
      "name": "command_r_plus",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.81e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1266.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1266.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.81e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.295767",
      "metadata": {
        "lmarena_score": 1266.0,
        "lmarena_rank": "109",
        "license": "CC-BY-NC-4.0",
        "source": "lmarena"
      }
    },
    {
      "name": "gemma_2_9b",
      "developer": "Google",
      "release_date": null,
      "parameters": 9000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.83e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.23e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1215.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.23e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1215.0,
        "coding_score": 1194.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 9,000,000,000 params \u00d7 108,000,000,000 tokens = 5.83e+21 FLOP (Mid-era model: 9B params * 12 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.295977",
      "metadata": {
        "arena_elo": 1215.0,
        "coding_score": 1194.0,
        "aai_score": 22.2,
        "mmlu_pro_score": 49.5,
        "votes": 57197.0,
        "license": "Gemma",
        "source": "openlm_arena"
      }
    },
    {
      "name": "deepseek_coder",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": 671000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.96e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.14e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1198.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.14e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "high_confidence_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1198.0,
        "coding_score": 1256.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 671,000,000,000 params \u00d7 14,800,000,000,000 tokens = 5.96e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.296045",
      "metadata": {
        "arena_elo": 1198.0,
        "coding_score": 1256.0,
        "votes": 15753.0,
        "license": "DeepSeek",
        "source": "openlm_arena"
      }
    },
    {
      "name": "claude_3_haiku",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.16e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1202.0,
        "coding_score": 1207.0,
        "vision_score": 1000.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1202.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.16e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.300393",
      "metadata": {
        "arena_elo": 1202.0,
        "coding_score": 1207.0,
        "vision_score": 1000.0,
        "aai_score": 24.1,
        "mmlu_pro_score": 50.0,
        "votes": 122309.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gemini_1.5_flash_8b_001",
      "developer": "Google",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.59e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1232.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.59e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1232.0,
        "coding_score": 1229.0,
        "vision_score": 1091.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.300621",
      "metadata": {
        "arena_elo": 1232.0,
        "coding_score": 1229.0,
        "vision_score": 1091.0,
        "aai_score": 30.8,
        "mmlu_pro_score": 56.9,
        "votes": 37697.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "amazon_nova_lite",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.59e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1233.0,
        "coding_score": 1252.0,
        "vision_score": 1034.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1233.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.59e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.300659",
      "metadata": {
        "arena_elo": 1233.0,
        "coding_score": 1252.0,
        "vision_score": 1034.0,
        "aai_score": 32.5,
        "mmlu_pro_score": 59.0,
        "votes": 20646.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "phi_4",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.28e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1223.0,
        "coding_score": 1240.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1223.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.28e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.300731",
      "metadata": {
        "arena_elo": 1223.0,
        "coding_score": 1240.0,
        "aai_score": 40.2,
        "mmlu_pro_score": 71.4,
        "votes": 25213.0,
        "license": "MIT",
        "source": "openlm_arena"
      }
    },
    {
      "name": "command_r_08",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.72e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1253.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1253.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.72e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.300795",
      "metadata": {
        "lmarena_score": 1253.0,
        "lmarena_rank": "116",
        "license": "CC-BY-NC-4.0",
        "source": "lmarena"
      }
    },
    {
      "name": "amazon_nova_micro",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.25e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1218.0,
        "coding_score": 1226.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1218.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.25e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.300859",
      "metadata": {
        "arena_elo": 1218.0,
        "coding_score": 1226.0,
        "aai_score": 28.3,
        "mmlu_pro_score": 53.1,
        "votes": 20654.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "jamba_1.5_mini",
      "developer": "AI21 Labs",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.13e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1196.0,
        "coding_score": 1196.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1196.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.13e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.300920",
      "metadata": {
        "arena_elo": 1196.0,
        "coding_score": 1196.0,
        "votes": 9274.0,
        "license": "Jamba Open",
        "source": "openlm_arena"
      }
    },
    {
      "name": "hunyuan_standard_256k",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.21e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1211.0,
        "coding_score": 1243.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1211.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.21e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.300982",
      "metadata": {
        "arena_elo": 1211.0,
        "coding_score": 1243.0,
        "votes": 2901.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "ministral_8b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.14e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1198.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.14e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1198.0,
        "coding_score": 1216.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.301063",
      "metadata": {
        "arena_elo": 1198.0,
        "coding_score": 1216.0,
        "aai_score": 22.3,
        "mmlu_pro_score": 38.9,
        "votes": 5111.0,
        "license": "MRL",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwen1.5_110b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 110000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.16e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.05e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1182.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.05e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1182.0,
        "coding_score": 1188.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 110,000,000,000 params \u00d7 1,760,000,000,000 tokens = 1.16e+24 FLOP (Chinese model: 110B params * 16 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.301117",
      "metadata": {
        "arena_elo": 1182.0,
        "coding_score": 1188.0,
        "aai_score": 25.0,
        "votes": 27430.0,
        "license": "Qianwen",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwen1.5_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.98e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.81e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1174.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.81e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1174.0,
        "coding_score": 1173.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 1,152,000,000,000 tokens = 4.98e+23 FLOP (Chinese model: 72B params * 16 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.301164",
      "metadata": {
        "arena_elo": 1174.0,
        "coding_score": 1173.0,
        "votes": 40658.0,
        "license": "Qianwen",
        "source": "openlm_arena"
      }
    },
    {
      "name": "reka_flash_21b_online",
      "developer": "Reka AI",
      "release_date": null,
      "parameters": 21000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.97e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.60e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1235.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.60e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1235.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 21,000,000,000 params \u00d7 315,000,000,000 tokens = 3.97e+22 FLOP (Generic estimate: 21B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.301246",
      "metadata": {
        "lmarena_score": 1235.0,
        "lmarena_rank": "124",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "gemini_pro_dev_api",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.60e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1234.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1234.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.60e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.301290",
      "metadata": {
        "lmarena_score": 1234.0,
        "lmarena_rank": "124",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "mixtral_8x22b_instruct",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 22000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.23e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.79e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1169.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.79e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1169.0,
        "coding_score": 1174.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 22,000,000,000 params \u00d7 396,000,000,000 tokens = 5.23e+22 FLOP (Specialized model: 22B params * 18 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.301485",
      "metadata": {
        "arena_elo": 1169.0,
        "coding_score": 1174.0,
        "aai_score": 26.2,
        "mmlu_pro_score": 53.7,
        "votes": 53751.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "command_r",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.79e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1171.0,
        "coding_score": 1139.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1171.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.79e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.301531",
      "metadata": {
        "arena_elo": 1171.0,
        "coding_score": 1139.0,
        "aai_score": 14.7,
        "mmlu_pro_score": 33.7,
        "votes": 56398.0,
        "license": "CC-BY-NC-4.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "reka_flash_21b",
      "developer": "Reka AI",
      "release_date": null,
      "parameters": 21000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.97e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.57e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1230.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.57e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1230.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 21,000,000,000 params \u00d7 315,000,000,000 tokens = 3.97e+22 FLOP (Generic estimate: 21B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.302676",
      "metadata": {
        "lmarena_score": 1230.0,
        "lmarena_rank": "126",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "llama_3.1_tulu_3_8b",
      "developer": "Ai2",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.68e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.16e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1202.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.16e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1202.0,
        "coding_score": 1196.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 160,000,000,000 tokens = 7.68e+21 FLOP (Modern model: 8B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.302733",
      "metadata": {
        "arena_elo": 1202.0,
        "coding_score": 1196.0,
        "votes": 3074.0,
        "license": "Llama 3.1",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gemini_pro",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.27e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1222.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1222.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.27e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.302776",
      "metadata": {
        "lmarena_score": 1222.0,
        "lmarena_rank": "126",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "gpt_3.5_turbo",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.66e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1141.0,
        "coding_score": 1138.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1141.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.66e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.302874",
      "metadata": {
        "arena_elo": 1141.0,
        "coding_score": 1138.0,
        "votes": 5640.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "c4ai_aya_expanse_8b",
      "developer": "Cohere",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.28e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1224.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.28e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1224.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.302957",
      "metadata": {
        "lmarena_score": 1224.0,
        "lmarena_rank": "127",
        "license": "CC-BY-NC-4.0",
        "source": "lmarena"
      }
    },
    {
      "name": "llama_3_8b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.20e+23",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.80e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1173.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.80e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1173.0,
        "coding_score": 1163.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 15,000,000,000,000 tokens = 7.20e+23 FLOP",
      "last_updated": "2025-08-01T12:44:10.303000",
      "metadata": {
        "arena_elo": 1173.0,
        "coding_score": 1163.0,
        "aai_score": 21.5,
        "mmlu_pro_score": 40.5,
        "votes": 109056.0,
        "license": "Llama 3",
        "source": "openlm_arena"
      }
    },
    {
      "name": "zephyr_orpo_141b",
      "developer": "HuggingFace",
      "release_date": null,
      "parameters": 141000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.79e+24",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.71e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1153.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.71e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1153.0,
        "coding_score": 1148.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 141,000,000,000 params \u00d7 2,115,000,000,000 tokens = 1.79e+24 FLOP (Generic estimate: 141B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.303048",
      "metadata": {
        "arena_elo": 1153.0,
        "coding_score": 1148.0,
        "votes": 4854.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "yi_1.5_34b",
      "developer": "01 AI",
      "release_date": null,
      "parameters": 34000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.11e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.04e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1180.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.04e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1180.0,
        "coding_score": 1180.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 34,000,000,000 params \u00d7 544,000,000,000 tokens = 1.11e+23 FLOP (Chinese model: 34B params * 16 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.303094",
      "metadata": {
        "arena_elo": 1180.0,
        "coding_score": 1180.0,
        "votes": 25135.0,
        "license": "Apache-2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "granite_3.1_8b",
      "developer": "IBM",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.74e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1159.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.74e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1159.0,
        "coding_score": 1186.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.303139",
      "metadata": {
        "arena_elo": 1159.0,
        "coding_score": 1186.0,
        "votes": 3289.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "llama_3.1_8b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.20e+23",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.12e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1194.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.12e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1194.0,
        "coding_score": 1201.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 15,000,000,000,000 tokens = 7.20e+23 FLOP",
      "last_updated": "2025-08-01T12:44:10.303211",
      "metadata": {
        "arena_elo": 1194.0,
        "coding_score": 1201.0,
        "aai_score": 23.7,
        "mmlu_pro_score": 47.6,
        "votes": 52578.0,
        "license": "Llama 3.1",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwen1.5_32b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.83e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.68e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1146.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.68e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1146.0,
        "coding_score": 1159.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 512,000,000,000 tokens = 9.83e+22 FLOP (Chinese model: 32B params * 16 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.303582",
      "metadata": {
        "arena_elo": 1146.0,
        "coding_score": 1159.0,
        "votes": 22765.0,
        "license": "Qianwen",
        "source": "openlm_arena"
      }
    },
    {
      "name": "phi_3_medium_4k",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.68e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1146.0,
        "coding_score": 1146.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1146.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.68e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.303725",
      "metadata": {
        "arena_elo": 1146.0,
        "coding_score": 1146.0,
        "aai_score": 24.5,
        "mmlu_pro_score": 54.3,
        "votes": 26105.0,
        "license": "MIT",
        "source": "openlm_arena"
      }
    },
    {
      "name": "mixtral_8x7b_instruct",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.29e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.66e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1140.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.66e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1140.0,
        "coding_score": 1136.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 126,000,000,000 tokens = 5.29e+21 FLOP (Specialized model: 7B params * 18 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.303915",
      "metadata": {
        "arena_elo": 1140.0,
        "coding_score": 1136.0,
        "aai_score": 17.0,
        "mmlu_pro_score": 38.7,
        "votes": 76126.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gemma_2_2b",
      "developer": "Google",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.88e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.77e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1165.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.77e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1165.0,
        "coding_score": 1129.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 24,000,000,000 tokens = 2.88e+20 FLOP (Mid-era model: 2B params * 12 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.303969",
      "metadata": {
        "arena_elo": 1165.0,
        "coding_score": 1129.0,
        "votes": 48892.0,
        "license": "Gemma",
        "source": "openlm_arena"
      }
    },
    {
      "name": "internlm2_5_20b",
      "developer": "InternLM",
      "release_date": null,
      "parameters": 20000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.15e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1200.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.15e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1200.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 20,000,000,000 params \u00d7 300,000,000,000 tokens = 3.60e+22 FLOP (Generic estimate: 20B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.304019",
      "metadata": {
        "lmarena_score": 1200.0,
        "lmarena_rank": "144",
        "license": "Other",
        "source": "lmarena"
      }
    },
    {
      "name": "qwen1.5_14b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 14000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.88e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.65e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1139.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.65e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1139.0,
        "coding_score": 1145.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 14,000,000,000 params \u00d7 224,000,000,000 tokens = 1.88e+22 FLOP (Chinese model: 14B params * 16 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.304066",
      "metadata": {
        "arena_elo": 1139.0,
        "coding_score": 1145.0,
        "votes": 18687.0,
        "license": "Qianwen",
        "source": "openlm_arena"
      }
    },
    {
      "name": "dbrx_instruct_preview",
      "developer": "Databricks",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.60e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1127.0,
        "coding_score": 1140.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1127.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.60e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.304100",
      "metadata": {
        "arena_elo": 1127.0,
        "coding_score": 1140.0,
        "votes": 33743.0,
        "license": "DBRX",
        "source": "openlm_arena"
      }
    },
    {
      "name": "wizardlm_70b",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.60e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1127.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.60e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1127.0,
        "coding_score": 1089.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.304247",
      "metadata": {
        "arena_elo": 1127.0,
        "coding_score": 1089.0,
        "votes": 8383.0,
        "license": "Llama 2",
        "source": "openlm_arena"
      }
    },
    {
      "name": "granite_3.0_8b",
      "developer": "IBM",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.02e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1109.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 1.02e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1109.0,
        "coding_score": 1116.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.304303",
      "metadata": {
        "arena_elo": 1109.0,
        "coding_score": 1116.0,
        "votes": 7002.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "deepseek_llm_67b",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": 671000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.96e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.04e+23",
          "confidence": "low",
          "method": "scaling_laws",
          "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 67,000,000,000 params \u00d7 1,005,000,000,000 tokens = 4.04e+23 FLOP (Generic estimate: 67B params * 15 tokens/param)"
        },
        {
          "flop": "1.03e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1111.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 1.03e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "high_confidence_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1111.0,
        "coding_score": 1115.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 671,000,000,000 params \u00d7 14,800,000,000,000 tokens = 5.96e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.304362",
      "metadata": {
        "arena_elo": 1111.0,
        "coding_score": 1115.0,
        "aai_score": 20.0,
        "votes": 4988.0,
        "license": "DeepSeek",
        "source": "openlm_arena"
      }
    },
    {
      "name": "yi_34b",
      "developer": "01 AI",
      "release_date": null,
      "parameters": 34000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.11e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.63e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1135.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.63e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1135.0,
        "coding_score": 1126.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 34,000,000,000 params \u00d7 544,000,000,000 tokens = 1.11e+23 FLOP (Chinese model: 34B params * 16 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.304453",
      "metadata": {
        "arena_elo": 1135.0,
        "coding_score": 1126.0,
        "votes": 15917.0,
        "license": "Yi",
        "source": "openlm_arena"
      }
    },
    {
      "name": "openchat",
      "developer": "OpenChat",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.01e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1105.0,
        "coding_score": 1080.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1105.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 1.01e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.304488",
      "metadata": {
        "arena_elo": 1105.0,
        "coding_score": 1080.0,
        "votes": 8106.0,
        "license": "Apache-2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "granite_3.1_2b",
      "developer": "IBM",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+20",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.64e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1137.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.64e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1137.0,
        "coding_score": 1162.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 30,000,000,000 tokens = 3.60e+20 FLOP (Generic estimate: 2B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.304570",
      "metadata": {
        "arena_elo": 1137.0,
        "coding_score": 1162.0,
        "votes": 3380.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "tulu_2_dpo_70b",
      "developer": "Ai2",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.61e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1129.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.61e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1129.0,
        "coding_score": 1124.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.304619",
      "metadata": {
        "arena_elo": 1129.0,
        "coding_score": 1124.0,
        "votes": 6658.0,
        "license": "Ai2 ImpACT",
        "source": "openlm_arena"
      }
    },
    {
      "name": "openhermes_2.5_mistral_7b",
      "developer": "NousResearch",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.01e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1104.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 1.01e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1104.0,
        "coding_score": 1086.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.304665",
      "metadata": {
        "arena_elo": 1104.0,
        "coding_score": 1086.0,
        "votes": 5088.0,
        "license": "Apache-2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "snowflake_arctic",
      "developer": "Snowflake",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.03e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1110.0,
        "coding_score": 1098.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1110.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 1.03e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.304695",
      "metadata": {
        "arena_elo": 1110.0,
        "coding_score": 1098.0,
        "votes": 34173.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gemma_1.1_7b",
      "developer": "Google",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.01e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1105.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 1.01e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1105.0,
        "coding_score": 1105.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.304776",
      "metadata": {
        "arena_elo": 1105.0,
        "coding_score": 1105.0,
        "votes": 25070.0,
        "license": "Gemma",
        "source": "openlm_arena"
      }
    },
    {
      "name": "vicuna_33b",
      "developer": "LMSYS",
      "release_date": null,
      "parameters": 33000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.80e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.04e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1114.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 1.04e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1114.0,
        "coding_score": 1089.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 33,000,000,000 params \u00d7 495,000,000,000 tokens = 9.80e+22 FLOP (Generic estimate: 33B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.304822",
      "metadata": {
        "arena_elo": 1114.0,
        "coding_score": 1089.0,
        "votes": 22936.0,
        "license": "Non-commercial",
        "source": "openlm_arena"
      }
    },
    {
      "name": "phi_3_small_8k",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.05e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1119.0,
        "coding_score": 1119.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1119.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 1.05e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.304853",
      "metadata": {
        "arena_elo": 1119.0,
        "coding_score": 1119.0,
        "votes": 18476.0,
        "license": "MIT",
        "source": "openlm_arena"
      }
    },
    {
      "name": "starling_lm_7b",
      "developer": "Nexusflow",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.65e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1139.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.65e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1139.0,
        "coding_score": 1148.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.304933",
      "metadata": {
        "arena_elo": 1139.0,
        "coding_score": 1148.0,
        "votes": 16676.0,
        "license": "Apache-2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "llama_2_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.40e+23",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.07e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1124.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 1.07e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1124.0,
        "coding_score": 1102.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 2,000,000,000,000 tokens = 8.40e+23 FLOP",
      "last_updated": "2025-08-01T12:44:10.304974",
      "metadata": {
        "arena_elo": 1124.0,
        "coding_score": 1102.0,
        "aai_score": 20.0,
        "mmlu_pro_score": 40.7,
        "votes": 39595.0,
        "license": "Llama 2",
        "source": "openlm_arena"
      }
    },
    {
      "name": "nous_hermes_2_mixtral_8x7b_dpo",
      "developer": "NousResearch",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.59e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1125.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.59e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1125.0,
        "coding_score": 1106.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.305025",
      "metadata": {
        "arena_elo": 1125.0,
        "coding_score": 1106.0,
        "votes": 3836.0,
        "license": "Apache-2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwq_32b_preview",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.81e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1174.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.81e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1174.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.305071",
      "metadata": {
        "lmarena_score": 1174.0,
        "lmarena_rank": "154",
        "license": "Apache 2.0",
        "source": "lmarena"
      }
    },
    {
      "name": "llama_3.2_3b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 3000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.08e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.05e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1119.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 1.05e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1119.0,
        "coding_score": 1093.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 3,000,000,000 params \u00d7 60,000,000,000 tokens = 1.08e+21 FLOP (Modern model: 3B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.305113",
      "metadata": {
        "arena_elo": 1119.0,
        "coding_score": 1093.0,
        "aai_score": 19.5,
        "mmlu_pro_score": 34.7,
        "votes": 8390.0,
        "license": "Llama 3.2",
        "source": "openlm_arena"
      }
    },
    {
      "name": "starling_lm_7b_alpha",
      "developer": "UC Berkeley",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.04e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1116.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 1.04e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1116.0,
        "coding_score": 1104.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.306105",
      "metadata": {
        "arena_elo": 1116.0,
        "coding_score": 1104.0,
        "votes": 10415.0,
        "license": "CC-BY-NC-4.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "llama2_70b_steerlm",
      "developer": "Nvidia",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.76e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1164.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.76e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1164.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.306329",
      "metadata": {
        "lmarena_score": 1164.0,
        "lmarena_rank": "157",
        "license": "Llama 2 Community",
        "source": "lmarena"
      }
    },
    {
      "name": "solar_10.7b_instruct",
      "developer": "Upstage AI",
      "release_date": null,
      "parameters": 10700000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.24e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "9.92e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1097.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 9.92e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1097.0,
        "coding_score": 1077.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 10,700,000,000 params \u00d7 192,600,000,000 tokens = 1.24e+22 FLOP (Specialized model: 11B params * 18 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.307237",
      "metadata": {
        "arena_elo": 1097.0,
        "coding_score": 1077.0,
        "votes": 4286.0,
        "license": "CC-BY-NC-4.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "dolphin_2.2.1_mistral_7b",
      "developer": "Cognitive",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "9.70e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1089.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 9.70e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1089.0,
        "coding_score": 1052.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.307309",
      "metadata": {
        "arena_elo": 1089.0,
        "coding_score": 1052.0,
        "votes": 1714.0,
        "license": "Apache-2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "granite_3.0_2b",
      "developer": "IBM",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+20",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "9.78e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1092.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 9.78e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1092.0,
        "coding_score": 1105.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 30,000,000,000 tokens = 3.60e+20 FLOP (Generic estimate: 2B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.307359",
      "metadata": {
        "arena_elo": 1092.0,
        "coding_score": 1105.0,
        "votes": 7191.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "mpt_30b",
      "developer": "MosaicML",
      "release_date": null,
      "parameters": 30000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.10e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "9.41e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1078.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 9.41e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1078.0,
        "coding_score": 1056.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 30,000,000,000 params \u00d7 450,000,000,000 tokens = 8.10e+22 FLOP (Generic estimate: 30B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.307405",
      "metadata": {
        "arena_elo": 1078.0,
        "coding_score": 1056.0,
        "votes": 2644.0,
        "license": "CC-BY-NC-SA-4.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "mistral_7b_instruct",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "9.97e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1099.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 9.97e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1099.0,
        "coding_score": 1098.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.308548",
      "metadata": {
        "arena_elo": 1099.0,
        "coding_score": 1098.0,
        "aai_score": 10.1,
        "mmlu_pro_score": 24.5,
        "votes": 20067.0,
        "license": "Apache-2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "falcon_180b",
      "developer": "TII",
      "release_date": null,
      "parameters": 180000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.92e+24",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.68e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1145.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.68e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1145.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 180,000,000,000 params \u00d7 2,700,000,000,000 tokens = 2.92e+24 FLOP (Generic estimate: 180B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.308616",
      "metadata": {
        "lmarena_score": 1145.0,
        "lmarena_rank": "170",
        "license": "Falcon-180B TII License",
        "source": "lmarena"
      }
    },
    {
      "name": "wizardlm_13b",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "9.60e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1085.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 9.60e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1085.0,
        "coding_score": 1052.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.308664",
      "metadata": {
        "arena_elo": 1085.0,
        "coding_score": 1052.0,
        "votes": 7176.0,
        "license": "Llama 2",
        "source": "openlm_arena"
      }
    },
    {
      "name": "phi_3_mini_4k_instruct_june",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.70e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1149.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1149.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.70e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.308699",
      "metadata": {
        "lmarena_score": 1149.0,
        "lmarena_rank": "173",
        "license": "MIT",
        "source": "lmarena"
      }
    },
    {
      "name": "llama_2_13b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.22e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "9.84e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1094.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 9.84e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1094.0,
        "coding_score": 1079.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 156,000,000,000 tokens = 1.22e+22 FLOP (Mid-era model: 13B params * 12 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.308903",
      "metadata": {
        "arena_elo": 1094.0,
        "coding_score": 1079.0,
        "aai_score": 19.9,
        "mmlu_pro_score": 40.6,
        "votes": 19722.0,
        "license": "Llama 2",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwen1.5_7b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.70e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "9.73e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1090.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 9.73e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1090.0,
        "coding_score": 1109.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 112,000,000,000 tokens = 4.70e+21 FLOP (Chinese model: 7B params * 16 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.308954",
      "metadata": {
        "arena_elo": 1090.0,
        "coding_score": 1109.0,
        "votes": 4872.0,
        "license": "Qianwen",
        "source": "openlm_arena"
      }
    },
    {
      "name": "vicuna_13b",
      "developer": "LMSYS",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.68e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1146.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.68e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1146.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.309016",
      "metadata": {
        "lmarena_score": 1146.0,
        "lmarena_rank": "174",
        "license": "Llama 2 Community",
        "source": "lmarena"
      }
    },
    {
      "name": "codellama_34b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 34000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.39e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "9.39e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1077.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 9.39e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1077.0,
        "coding_score": 1074.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 34,000,000,000 params \u00d7 680,000,000,000 tokens = 1.39e+23 FLOP (Modern model: 34B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.309077",
      "metadata": {
        "arena_elo": 1077.0,
        "coding_score": 1074.0,
        "votes": 7509.0,
        "license": "Llama 2",
        "source": "openlm_arena"
      }
    },
    {
      "name": "palm_2",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.66e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1141.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1141.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.66e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.309151",
      "metadata": {
        "lmarena_score": 1141.0,
        "lmarena_rank": "174",
        "license": "Proprietary",
        "source": "lmarena"
      }
    },
    {
      "name": "qwen_14b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 14000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.88e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.66e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1140.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.66e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1140.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 14,000,000,000 params \u00d7 224,000,000,000 tokens = 1.88e+22 FLOP (Chinese model: 14B params * 16 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.309670",
      "metadata": {
        "lmarena_score": 1140.0,
        "lmarena_rank": "174",
        "license": "Qianwen LICENSE",
        "source": "lmarena"
      }
    },
    {
      "name": "gemma_7b",
      "developer": "Google",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.65e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1139.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.65e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1139.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.309779",
      "metadata": {
        "lmarena_score": 1139.0,
        "lmarena_rank": "174",
        "license": "Gemma license",
        "source": "lmarena"
      }
    },
    {
      "name": "codellama_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.29e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.62e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1131.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.62e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1131.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,260,000,000,000 tokens = 5.29e+23 FLOP (Specialized model: 70B params * 18 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.309857",
      "metadata": {
        "lmarena_score": 1131.0,
        "lmarena_rank": "174",
        "license": "Llama 2 Community",
        "source": "lmarena"
      }
    },
    {
      "name": "smollm2_1.7b",
      "developer": "HuggingFace",
      "release_date": null,
      "parameters": 1700000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.60e+20",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.62e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1132.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.62e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1132.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 1,700,000,000 params \u00d7 25,500,000,000 tokens = 2.60e+20 FLOP (Generic estimate: 2B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.309923",
      "metadata": {
        "lmarena_score": 1132.0,
        "lmarena_rank": "176",
        "license": "Apache 2.0",
        "source": "lmarena"
      }
    },
    {
      "name": "zephyr_7b_alpha",
      "developer": "HuggingFace",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.62e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1132.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.62e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1132.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.309988",
      "metadata": {
        "lmarena_score": 1132.0,
        "lmarena_rank": "176",
        "license": "MIT",
        "source": "lmarena"
      }
    },
    {
      "name": "phi_3_mini_128k",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.65e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "lmarena_score": 1138.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1138.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.65e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.310034",
      "metadata": {
        "lmarena_score": 1138.0,
        "lmarena_rank": "177",
        "license": "MIT",
        "source": "lmarena"
      }
    },
    {
      "name": "zephyr_7b",
      "developer": "HuggingFace",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.64e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1136.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.64e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1136.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.310325",
      "metadata": {
        "lmarena_score": 1136.0,
        "lmarena_rank": "177",
        "license": "MIT",
        "source": "lmarena"
      }
    },
    {
      "name": "phi_3_mini_4k",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.54e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1083.0,
        "coding_score": 1097.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1083.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 9.54e+24 FLOP",
      "last_updated": "2025-08-01T12:44:10.310375",
      "metadata": {
        "arena_elo": 1083.0,
        "coding_score": 1097.0,
        "votes": 21097.0,
        "license": "MIT",
        "source": "openlm_arena"
      }
    },
    {
      "name": "guanaco_33b",
      "developer": "UW",
      "release_date": null,
      "parameters": 33000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.80e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.61e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1129.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.61e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1129.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 33,000,000,000 params \u00d7 495,000,000,000 tokens = 9.80e+22 FLOP (Generic estimate: 33B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.310443",
      "metadata": {
        "lmarena_score": 1129.0,
        "lmarena_rank": "178",
        "license": "Non-commercial",
        "source": "lmarena"
      }
    },
    {
      "name": "stripedhyena_nous_7b",
      "developer": "Together AI",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.07e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1124.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 1.07e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1124.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.310523",
      "metadata": {
        "lmarena_score": 1124.0,
        "lmarena_rank": "183",
        "license": "Apache 2.0",
        "source": "lmarena"
      }
    },
    {
      "name": "vicuna_7b",
      "developer": "LMSYS",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.05e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1119.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 1.05e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1119.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.310598",
      "metadata": {
        "lmarena_score": 1119.0,
        "lmarena_rank": "185",
        "license": "Llama 2 Community",
        "source": "lmarena"
      }
    },
    {
      "name": "llama_3.2_1b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 1000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.20e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "5.25e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1067.0 vs reference llama_405b (ELO 1050, 5.00e+24 FLOP) with power law scaling \u03b1=3.0 = 5.25e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1067.0,
        "coding_score": 1060.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 1,000,000,000 params \u00d7 20,000,000,000 tokens = 1.20e+20 FLOP (Modern model: 1B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.310675",
      "metadata": {
        "arena_elo": 1067.0,
        "coding_score": 1060.0,
        "aai_score": 9.7,
        "mmlu_pro_score": 20.0,
        "votes": 8523.0,
        "license": "Llama 3.2",
        "source": "openlm_arena"
      }
    },
    {
      "name": "mistral_7b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.04e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1114.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 1.04e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1114.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.310753",
      "metadata": {
        "lmarena_score": 1114.0,
        "lmarena_rank": "192",
        "license": "Apache 2.0",
        "source": "lmarena"
      }
    },
    {
      "name": "llama_2_7b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.40e+22",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.04e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1113.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 1.04e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1113.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 2,000,000,000,000 tokens = 8.40e+22 FLOP",
      "last_updated": "2025-08-01T12:44:10.310816",
      "metadata": {
        "lmarena_score": 1113.0,
        "lmarena_rank": "192",
        "license": "Llama 2 Community",
        "source": "lmarena"
      }
    },
    {
      "name": "gemma_1.1_2b",
      "developer": "Google",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+20",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.03e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1112.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 1.03e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1112.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 30,000,000,000 tokens = 3.60e+20 FLOP (Generic estimate: 2B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.310880",
      "metadata": {
        "lmarena_score": 1112.0,
        "lmarena_rank": "192",
        "license": "Gemma license",
        "source": "lmarena"
      }
    },
    {
      "name": "gemma_2b",
      "developer": "Google",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.88e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "9.78e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1092.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 9.78e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1092.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 24,000,000,000 tokens = 2.88e+20 FLOP (Mid-era model: 2B params * 12 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.310946",
      "metadata": {
        "lmarena_score": 1092.0,
        "lmarena_rank": "201",
        "license": "Gemma license",
        "source": "lmarena"
      }
    },
    {
      "name": "qwen1.5_4b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 4000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.54e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "9.76e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1091.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 9.76e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1091.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 4,000,000,000 params \u00d7 64,000,000,000 tokens = 1.54e+21 FLOP (Chinese model: 4B params * 16 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.311019",
      "metadata": {
        "lmarena_score": 1091.0,
        "lmarena_rank": "201",
        "license": "Qianwen LICENSE",
        "source": "lmarena"
      }
    },
    {
      "name": "olmo_7b",
      "developer": "Allen AI",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "9.52e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1082.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 9.52e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1082.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.311093",
      "metadata": {
        "lmarena_score": 1082.0,
        "lmarena_rank": "202",
        "license": "Apache-2.0",
        "source": "lmarena"
      }
    },
    {
      "name": "koala_13b",
      "developer": "UC Berkeley",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "5.32e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1072.0 vs reference llama_405b (ELO 1050, 5.00e+24 FLOP) with power law scaling \u03b1=3.0 = 5.32e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1072.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.311209",
      "metadata": {
        "lmarena_score": 1072.0,
        "lmarena_rank": "202",
        "license": "Non-commercial",
        "source": "lmarena"
      }
    },
    {
      "name": "gpt4all_13b_snoozy",
      "developer": "Nomic AI",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "5.26e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1068.0 vs reference llama_405b (ELO 1050, 5.00e+24 FLOP) with power law scaling \u03b1=3.0 = 5.26e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1068.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.311286",
      "metadata": {
        "lmarena_score": 1068.0,
        "lmarena_rank": "202",
        "license": "Non-commercial",
        "source": "lmarena"
      }
    },
    {
      "name": "alpaca_13b",
      "developer": "Stanford",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "5.23e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1066.0 vs reference llama_405b (ELO 1050, 5.00e+24 FLOP) with power law scaling \u03b1=3.0 = 5.23e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1066.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.311356",
      "metadata": {
        "lmarena_score": 1066.0,
        "lmarena_rank": "203",
        "license": "Non-commercial",
        "source": "lmarena"
      }
    },
    {
      "name": "mpt_7b",
      "developer": "MosaicML",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "5.19e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1063.0 vs reference llama_405b (ELO 1050, 5.00e+24 FLOP) with power law scaling \u03b1=3.0 = 5.19e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1063.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.320240",
      "metadata": {
        "lmarena_score": 1063.0,
        "lmarena_rank": "204",
        "license": "CC-BY-NC-SA-4.0",
        "source": "lmarena"
      }
    },
    {
      "name": "chatglm3_6b",
      "developer": "Tsinghua",
      "release_date": null,
      "parameters": 6000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.89e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "5.07e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1055.0 vs reference llama_405b (ELO 1050, 5.00e+24 FLOP) with power law scaling \u03b1=3.0 = 5.07e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1055.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 6,000,000,000 params \u00d7 108,000,000,000 tokens = 3.89e+21 FLOP (Specialized model: 6B params * 18 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.320481",
      "metadata": {
        "lmarena_score": 1055.0,
        "lmarena_rank": "204",
        "license": "Apache-2.0",
        "source": "lmarena"
      }
    },
    {
      "name": "rwkv_4_raven_14b",
      "developer": "RWKV",
      "release_date": null,
      "parameters": 14000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.76e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.91e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1044.0 vs reference llama_405b (ELO 1050, 5.00e+24 FLOP) with power law scaling \u03b1=3.0 = 4.91e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1044.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 14,000,000,000 params \u00d7 210,000,000,000 tokens = 1.76e+22 FLOP (Generic estimate: 14B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.320549",
      "metadata": {
        "lmarena_score": 1044.0,
        "lmarena_rank": "206",
        "license": "Apache 2.0",
        "source": "lmarena"
      }
    },
    {
      "name": "chatglm2_6b",
      "developer": "Tsinghua",
      "release_date": null,
      "parameters": 6000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.89e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.76e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1033.0 vs reference llama_405b (ELO 1050, 5.00e+24 FLOP) with power law scaling \u03b1=3.0 = 4.76e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1033.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 6,000,000,000 params \u00d7 108,000,000,000 tokens = 3.89e+21 FLOP (Specialized model: 6B params * 18 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.320598",
      "metadata": {
        "lmarena_score": 1033.0,
        "lmarena_rank": "208",
        "license": "Apache-2.0",
        "source": "lmarena"
      }
    },
    {
      "name": "oasst_pythia_12b",
      "developer": "OpenAssistant",
      "release_date": null,
      "parameters": 12000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.30e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.06e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1021.0 vs reference llama_405b (ELO 1000, 1.00e+24 FLOP) with power law scaling \u03b1=3.0 = 1.06e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1021.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 12,000,000,000 params \u00d7 180,000,000,000 tokens = 1.30e+22 FLOP (Generic estimate: 12B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.320655",
      "metadata": {
        "lmarena_score": 1021.0,
        "lmarena_rank": "210",
        "license": "Apache 2.0",
        "source": "lmarena"
      }
    },
    {
      "name": "chatglm_6b",
      "developer": "Tsinghua",
      "release_date": null,
      "parameters": 6000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.89e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.01e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 1004.0 vs reference llama_405b (ELO 1000, 1.00e+24 FLOP) with power law scaling \u03b1=3.0 = 1.01e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 1004.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 6,000,000,000 params \u00d7 108,000,000,000 tokens = 3.89e+21 FLOP (Specialized model: 6B params * 18 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.320702",
      "metadata": {
        "lmarena_score": 1004.0,
        "lmarena_rank": "211",
        "license": "Non-commercial",
        "source": "lmarena"
      }
    },
    {
      "name": "fastchat_t5_3b",
      "developer": "LMSYS",
      "release_date": null,
      "parameters": 3000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.72e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "9.85e+23",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 995.0 vs reference llama_405b (ELO 1000, 1.00e+24 FLOP) with power law scaling \u03b1=3.0 = 9.85e+23 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 995.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 3,000,000,000 params \u00d7 54,000,000,000 tokens = 9.72e+20 FLOP (Specialized model: 3B params * 18 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.320759",
      "metadata": {
        "lmarena_score": 995.0,
        "lmarena_rank": "212",
        "license": "Apache 2.0",
        "source": "lmarena"
      }
    },
    {
      "name": "llama_13b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.11e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "9.35e+23",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 978.0 vs reference llama_405b (ELO 1000, 1.00e+24 FLOP) with power law scaling \u03b1=3.0 = 9.35e+23 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 978.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 104,000,000,000 tokens = 8.11e+21 FLOP (Early era model: 13B params * 8 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.320828",
      "metadata": {
        "lmarena_score": 978.0,
        "lmarena_rank": "213",
        "license": "Non-commercial",
        "source": "lmarena"
      }
    },
    {
      "name": "dolly_v2_12b",
      "developer": "Databricks",
      "release_date": null,
      "parameters": 12000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.30e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "9.30e+23",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 976.0 vs reference llama_405b (ELO 1000, 1.00e+24 FLOP) with power law scaling \u03b1=3.0 = 9.30e+23 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 976.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 12,000,000,000 params \u00d7 180,000,000,000 tokens = 1.30e+22 FLOP (Generic estimate: 12B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.320899",
      "metadata": {
        "lmarena_score": 976.0,
        "lmarena_rank": "213",
        "license": "MIT",
        "source": "lmarena"
      }
    },
    {
      "name": "stablelm_tuned_alpha_7b",
      "developer": "Stability AI",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "8.74e+23",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): Benchmark-based estimation: ELO 956.0 vs reference llama_405b (ELO 1000, 1.00e+24 FLOP) with power law scaling \u03b1=3.0 = 8.74e+23 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "lmarena_score": 956.0
      },
      "sources": [
        "LMArena Manual Data Collection (CSV file with manually collected leaderboard data)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.320961",
      "metadata": {
        "lmarena_score": 956.0,
        "lmarena_rank": "215",
        "license": "CC-BY-NC-S",
        "source": "lmarena"
      }
    },
    {
      "name": "gemini_2.0_pro",
      "developer": "Google",
      "release_date": null,
      "parameters": 500000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "6.00e+25",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.74e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1399.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.74e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1399.0,
        "coding_score": 1397.0,
        "vision_score": 1214.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 500,000,000,000 params \u00d7 20,000,000,000,000 tokens = 6.00e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.321100",
      "metadata": {
        "arena_elo": 1399.0,
        "coding_score": 1397.0,
        "vision_score": 1214.0,
        "aai_score": 49.2,
        "mmlu_pro_score": 80.5,
        "votes": 20120.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gemini_2.0_flash",
      "developer": "Google",
      "release_date": null,
      "parameters": 500000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "6.00e+25",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.47e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1372.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.47e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1372.0,
        "coding_score": 1370.0,
        "vision_score": 1239.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 500,000,000,000 params \u00d7 20,000,000,000,000 tokens = 6.00e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.321319",
      "metadata": {
        "arena_elo": 1372.0,
        "coding_score": 1370.0,
        "vision_score": 1239.0,
        "aai_score": 48.1,
        "mmlu_pro_score": 78.2,
        "votes": 22500.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "mistral_medium_3",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.47e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1372.0,
        "coding_score": 1384.0,
        "vision_score": 1190.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1372.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.47e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.321439",
      "metadata": {
        "arena_elo": 1372.0,
        "coding_score": 1384.0,
        "vision_score": 1190.0,
        "aai_score": 49.0,
        "mmlu_pro_score": 76.0,
        "votes": 26340.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "command_a",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.05e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1328.0,
        "coding_score": 1331.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1328.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.05e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.321536",
      "metadata": {
        "arena_elo": 1328.0,
        "coding_score": 1331.0,
        "aai_score": 40.0,
        "mmlu_pro_score": 71.2,
        "votes": 29251.0,
        "license": "CC-BY-NC-4.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "amazon_nova_chat_05_14",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.01e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1324.0,
        "coding_score": 1333.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1324.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 4.01e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.321642",
      "metadata": {
        "arena_elo": 1324.0,
        "coding_score": 1333.0,
        "aai_score": 42.6,
        "mmlu_pro_score": 73.3,
        "votes": 12267.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "claude_3.7_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.84e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1304.0,
        "coding_score": 1338.0,
        "vision_score": 1196.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1304.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.84e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.321741",
      "metadata": {
        "arena_elo": 1304.0,
        "coding_score": 1338.0,
        "vision_score": 1196.0,
        "aai_score": 48.2,
        "mmlu_pro_score": 80.3,
        "votes": 35838.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "llama_3.3_nemotron_super_49b",
      "developer": "Nvidia",
      "release_date": null,
      "parameters": 49000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.90e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1311.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.90e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1311.0,
        "coding_score": 1318.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 49,000,000,000 params \u00d7 980,000,000,000 tokens = 2.88e+23 FLOP (Modern model: 49B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.321983",
      "metadata": {
        "arena_elo": 1311.0,
        "coding_score": 1318.0,
        "aai_score": 51.2,
        "mmlu_pro_score": 78.5,
        "votes": 2371.0,
        "license": "Nvidia Open Model",
        "source": "openlm_arena"
      }
    },
    {
      "name": "grok_2_08_13",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.00e+25",
      "training_flop_confidence": "high",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "3.86e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1307.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.86e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1307.0,
        "coding_score": 1296.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Epoch AI: High-precision estimate from xAI disclosure",
      "last_updated": "2025-08-01T12:44:10.322083",
      "metadata": {
        "arena_elo": 1307.0,
        "coding_score": 1296.0,
        "aai_score": 39.2,
        "mmlu_pro_score": 70.9,
        "votes": 67084.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "claude_3.5_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": 250000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "1.50e+25",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 250,000,000,000 params \u00d7 10,000,000,000,000 tokens = 1.50e+25 FLOP"
        },
        {
          "flop": "3.67e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1285.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.67e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1285.0,
        "coding_score": 1307.0,
        "vision_score": 1165.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Epoch AI: Low-precision estimate from benchmarks",
      "last_updated": "2025-08-01T12:44:10.322247",
      "metadata": {
        "arena_elo": 1285.0,
        "coding_score": 1307.0,
        "vision_score": 1165.0,
        "aai_score": 40.0,
        "mmlu_pro_score": 75.1,
        "votes": 86159.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "athene_v2_chat_72b",
      "developer": "NexusFlow",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.60e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.77e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1296.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.77e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1296.0,
        "coding_score": 1319.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 1,296,000,000,000 tokens = 5.60e+23 FLOP (Specialized model: 72B params * 18 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.322395",
      "metadata": {
        "arena_elo": 1296.0,
        "coding_score": 1319.0,
        "votes": 26074.0,
        "license": "NexusFlow",
        "source": "openlm_arena"
      }
    },
    {
      "name": "yi_lightning_lite",
      "developer": "01 AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.68e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1286.0,
        "coding_score": 1286.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1286.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.68e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.322448",
      "metadata": {
        "arena_elo": 1286.0,
        "coding_score": 1286.0,
        "votes": 17067.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "grok_2_mini_08_13",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.00e+25",
      "training_flop_confidence": "high",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "3.67e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1285.0 vs reference llama_405b (ELO 1300, 3.80e+25 FLOP) with power law scaling \u03b1=3.0 = 3.67e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1285.0,
        "coding_score": 1277.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Epoch AI: High-precision estimate from xAI disclosure",
      "last_updated": "2025-08-01T12:44:10.322559",
      "metadata": {
        "arena_elo": 1285.0,
        "coding_score": 1277.0,
        "votes": 55442.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "claude_3.5_haiku",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.75e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1257.0,
        "coding_score": 1284.0,
        "vision_score": 1145.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1257.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.75e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.322657",
      "metadata": {
        "arena_elo": 1257.0,
        "coding_score": 1284.0,
        "vision_score": 1145.0,
        "aai_score": 34.7,
        "mmlu_pro_score": 63.4,
        "votes": 54196.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "deepseek_v2_api",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": 671000000000,
      "parameter_source": "known_specification",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.96e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.65e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1242.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.65e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "high_confidence_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1242.0,
        "coding_score": 1261.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Scaling laws with documented parameters: Chinchilla scaling law: 6 \u00d7 671,000,000,000 params \u00d7 14,800,000,000,000 tokens = 5.96e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.322767",
      "metadata": {
        "arena_elo": 1242.0,
        "coding_score": 1261.0,
        "votes": 19508.0,
        "license": "DeepSeek",
        "source": "openlm_arena"
      }
    },
    {
      "name": "yi_large",
      "developer": "01 AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.62e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1238.0,
        "coding_score": 1239.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1238.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.62e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.322858",
      "metadata": {
        "arena_elo": 1238.0,
        "coding_score": 1239.0,
        "aai_score": 27.9,
        "mmlu_pro_score": 58.6,
        "votes": 16624.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "aya_expanse_32b",
      "developer": "Cohere",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.58e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1231.0 vs reference llama_405b (ELO 1250, 2.70e+25 FLOP) with power law scaling \u03b1=3.0 = 2.58e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1231.0,
        "coding_score": 1213.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.323012",
      "metadata": {
        "arena_elo": 1231.0,
        "coding_score": 1213.0,
        "aai_score": 20.1,
        "mmlu_pro_score": 37.7,
        "votes": 28768.0,
        "license": "CC-BY-NC-4.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "aya_expanse_8b",
      "developer": "Cohere",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.10e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1191.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.10e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1191.0,
        "coding_score": 1182.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.323119",
      "metadata": {
        "arena_elo": 1191.0,
        "coding_score": 1182.0,
        "aai_score": 16.0,
        "mmlu_pro_score": 31.2,
        "votes": 10391.0,
        "license": "CC-BY-NC-4.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "claude_1",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.05e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1181.0,
        "coding_score": 1162.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1181.0 vs reference llama_405b (ELO 1200, 2.15e+25 FLOP) with power law scaling \u03b1=3.0 = 2.05e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.323166",
      "metadata": {
        "arena_elo": 1181.0,
        "coding_score": 1162.0,
        "votes": 21149.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "internlm2.5_20b",
      "developer": "InternLM",
      "release_date": null,
      "parameters": 20000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.79e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1170.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.79e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1170.0,
        "coding_score": 1180.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 20,000,000,000 params \u00d7 300,000,000,000 tokens = 3.60e+22 FLOP (Generic estimate: 20B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.323298",
      "metadata": {
        "arena_elo": 1170.0,
        "coding_score": 1180.0,
        "votes": 10599.0,
        "license": "Other",
        "source": "openlm_arena"
      }
    },
    {
      "name": "gemini_1.0_pro_001",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.73e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1157.0,
        "coding_score": 1127.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1157.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.73e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.323359",
      "metadata": {
        "arena_elo": 1157.0,
        "coding_score": 1127.0,
        "votes": 18800.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "claude_instant_1",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.64e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1137.0,
        "coding_score": 1135.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1137.0 vs reference llama_405b (ELO 1150, 1.70e+25 FLOP) with power law scaling \u03b1=3.0 = 1.64e+25 FLOP",
      "last_updated": "2025-08-01T12:44:10.323565",
      "metadata": {
        "arena_elo": 1137.0,
        "coding_score": 1135.0,
        "votes": 20631.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "nv_llama2_70b_steerlm",
      "developer": "Nvidia",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.02e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1108.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 1.02e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1108.0,
        "coding_score": 1051.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.323702",
      "metadata": {
        "arena_elo": 1108.0,
        "coding_score": 1051.0,
        "votes": 3636.0,
        "license": "Llama 2",
        "source": "openlm_arena"
      }
    },
    {
      "name": "pplx_70b_online",
      "developer": "Perplexity AI",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "9.95e+24",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1098.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 9.95e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1098.0,
        "coding_score": 1059.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.323776",
      "metadata": {
        "arena_elo": 1098.0,
        "coding_score": 1059.0,
        "votes": 6898.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "phi_3_mini_4k_instruct_june_24",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.70e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "openlm_arena_elo": 1089.0,
        "coding_score": 1097.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Benchmark-based (openlm_arena_elo): Benchmark-based estimation: ELO 1089.0 vs reference llama_405b (ELO 1100, 1.00e+25 FLOP) with power law scaling \u03b1=3.0 = 9.70e+24 FLOP",
      "last_updated": "2025-08-01T12:44:10.323827",
      "metadata": {
        "arena_elo": 1089.0,
        "coding_score": 1097.0,
        "votes": 12808.0,
        "license": "MIT",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwen2.5_vl_32b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.37e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "vision_score": 1198.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 384,000,000,000 tokens = 7.37e+22 FLOP (Mid-era model: 32B params * 12 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.323880",
      "metadata": {
        "vision_score": 1198.0,
        "votes": 1505.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "step_1o_vision_32k",
      "developer": "StepFun",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "vision_score": 1165.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-01T12:43:47.413333",
      "metadata": {
        "vision_score": 1165.0,
        "votes": 2891.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwen2.5_vl_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.73e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "vision_score": 1154.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 864,000,000,000 tokens = 3.73e+23 FLOP (Mid-era model: 72B params * 12 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.323946",
      "metadata": {
        "vision_score": 1154.0,
        "votes": 3884.0,
        "license": "Qwen",
        "source": "openlm_arena"
      }
    },
    {
      "name": "pixtral_large",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "vision_score": 1135.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-01T12:43:47.413494",
      "metadata": {
        "vision_score": 1135.0,
        "aai_score": 37.4,
        "mmlu_pro_score": 70.1,
        "votes": 5546.0,
        "license": "MRL",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwen_vl_max",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "vision_score": 1103.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-01T12:43:47.413568",
      "metadata": {
        "vision_score": 1103.0,
        "votes": 1449.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwen2_vl_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.73e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "vision_score": 1091.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 864,000,000,000 tokens = 3.73e+23 FLOP (Mid-era model: 72B params * 12 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.324051",
      "metadata": {
        "vision_score": 1091.0,
        "votes": 6028.0,
        "license": "Qwen",
        "source": "openlm_arena"
      }
    },
    {
      "name": "step_1v_32k",
      "developer": "StepFun",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "vision_score": 1090.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-01T12:43:47.413745",
      "metadata": {
        "vision_score": 1090.0,
        "votes": 1553.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "molmo_72b",
      "developer": "Ai2",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.67e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "vision_score": 1059.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 1,080,000,000,000 tokens = 4.67e+23 FLOP (Generic estimate: 72B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.324138",
      "metadata": {
        "vision_score": 1059.0,
        "votes": 3092.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "pixtral_12b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 12000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.30e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "vision_score": 1056.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 12,000,000,000 params \u00d7 180,000,000,000 tokens = 1.30e+22 FLOP (Generic estimate: 12B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.324193",
      "metadata": {
        "vision_score": 1056.0,
        "aai_score": 23.4,
        "mmlu_pro_score": 47.3,
        "votes": 7623.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "internvl2_26b",
      "developer": "OpenGVLab",
      "release_date": null,
      "parameters": 26000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "6.08e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "vision_score": 1053.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 26,000,000,000 params \u00d7 390,000,000,000 tokens = 6.08e+22 FLOP (Generic estimate: 26B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.324239",
      "metadata": {
        "vision_score": 1053.0,
        "votes": 5265.0,
        "license": "MIT",
        "source": "openlm_arena"
      }
    },
    {
      "name": "llama_3.2_90b_vision",
      "developer": "Meta",
      "release_date": null,
      "parameters": 90000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.72e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "vision_score": 1046.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 90,000,000,000 params \u00d7 1,800,000,000,000 tokens = 9.72e+23 FLOP (Modern model: 90B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.324288",
      "metadata": {
        "vision_score": 1046.0,
        "aai_score": 33.4,
        "mmlu_pro_score": 67.1,
        "votes": 8829.0,
        "license": "Llama 3.2",
        "source": "openlm_arena"
      }
    },
    {
      "name": "hunyuan_standard_vision",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "vision_score": 1045.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-01T12:43:47.414136",
      "metadata": {
        "vision_score": 1045.0,
        "votes": 811.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "aya_vision_32b",
      "developer": "Cohere",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "vision_score": 1036.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.324354",
      "metadata": {
        "vision_score": 1036.0,
        "votes": 849.0,
        "license": "CC-BY-NC-4.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "qwen2_vl_7b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "vision_score": 1034.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.324397",
      "metadata": {
        "vision_score": 1034.0,
        "votes": 5854.0,
        "license": "Apache 2.0",
        "source": "openlm_arena"
      }
    },
    {
      "name": "yi_vision",
      "developer": "01 AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "threshold_classification": "not_sure",
      "benchmarks": {
        "vision_score": 1028.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-01T12:43:47.414358",
      "metadata": {
        "vision_score": 1028.0,
        "votes": 1237.0,
        "license": "Proprietary",
        "source": "openlm_arena"
      }
    },
    {
      "name": "llama_3.2_11b_vision",
      "developer": "Meta",
      "release_date": null,
      "parameters": 11000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.45e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "threshold_classification": "high_confidence_below_1e25",
      "benchmarks": {
        "vision_score": 1008.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 11,000,000,000 params \u00d7 220,000,000,000 tokens = 1.45e+22 FLOP (Modern model: 11B params * 20 tokens/param)",
      "last_updated": "2025-08-01T12:44:10.324458",
      "metadata": {
        "vision_score": 1008.0,
        "aai_score": 25.0,
        "mmlu_pro_score": 46.4,
        "votes": 4893.0,
        "license": "Llama 3.2",
        "source": "openlm_arena"
      }
    }
  ]
}