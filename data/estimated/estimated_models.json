{
  "metadata": {
    "saved_at": "2025-08-11T21:27:15.206475",
    "source": "consolidated_estimated",
    "last_updated": "2025-08-11T21:27:15.206360",
    "model_count": 276,
    "stage": "estimated"
  },
  "models": [
    {
      "name": "gemini_2.5_pro",
      "developer": "Google",
      "release_date": null,
      "parameters": 800000000000,
      "parameter_source": "known_specification:gemini_2.5_pro",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.20e+26",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "5.02e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.76e+25 (Medium); superclue_math: 5.20e+25 (Medium); superclue_reasoning: 2.57e+25 (Low); superclue_code: 5.30e+25 (Medium); superclue_agents: 6.47e+25 (Medium) \u2192 weighted average: 5.02e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 68.98,
        "superclue_math": 74.05,
        "superclue_reasoning": 52.59,
        "superclue_code": 82.38,
        "superclue_agents": 82.09
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Known model specification 'gemini_2.5_pro': Chinchilla scaling law: 6 \u00d7 800,000,000,000 params \u00d7 25,000,000,000,000 tokens = 1.20e+26 FLOP",
      "last_updated": "2025-08-11T21:27:15.025492",
      "metadata": {}
    },
    {
      "name": "o3",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.33e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "superclue_overall": 73.78,
        "superclue_math": 81.6,
        "superclue_reasoning": 59.7,
        "superclue_code": 83.76,
        "superclue_agents": 76.12
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 5.64e+25 (Medium); superclue_math: 6.62e+25 (Medium); superclue_reasoning: 3.52e+25 (Medium); superclue_code: 5.53e+25 (Medium); superclue_agents: 5.36e+25 (Medium) \u2192 weighted average: 5.33e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.026029",
      "metadata": {}
    },
    {
      "name": "chatgpt_4o",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.62e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1429.0,
        "coding_score": 1435.0,
        "vision_score": 1297.0,
        "aai_score": 40.0,
        "mmlu_pro_score": 80.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.21e+25 (Low); coding_score: 5.12e+25 (Low); aai_score: 3.71e+25 (Medium); mmlu_pro_score: 4.79e+25 (Medium) \u2192 weighted average: 4.62e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.026974",
      "metadata": {}
    },
    {
      "name": "gpt_4.5_preview",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "6.40e+25",
      "training_flop_confidence": "low",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "4.70e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.07e+25 (Low); coding_score: 4.95e+25 (Low); aai_score: 4.09e+25 (Medium); mmlu_pro_score: 4.89e+25 (Medium) \u2192 weighted average: 4.70e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1416.0,
        "coding_score": 1419.0,
        "vision_score": 1239.0,
        "aai_score": 42.0,
        "mmlu_pro_score": 81.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate for GPT-4.5",
      "last_updated": "2025-08-11T21:27:15.027398",
      "metadata": {}
    },
    {
      "name": "grok_4",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "6.64e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1436.0,
        "coding_score": 1441.0,
        "vision_score": 1273.0,
        "aai_score": 68.0,
        "mmlu_pro_score": 86.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.29e+25 (Low); coding_score: 5.19e+25 (Low); aai_score: 1.07e+26 (Low); mmlu_pro_score: 5.78e+25 (Medium) \u2192 weighted average: 6.64e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.027776",
      "metadata": {}
    },
    {
      "name": "claude_opus_4_thinking_16k",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.58e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1421.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1421.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 4.58e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.028015",
      "metadata": {}
    },
    {
      "name": "kimi_k2_preview",
      "developer": "Moonshot",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "5.07e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.71e+25 (Medium); coding_score: 4.82e+25 (Low); aai_score: 5.56e+25 (Medium); mmlu_pro_score: 5.11e+25 (Medium) \u2192 weighted average: 5.07e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1381.0,
        "coding_score": 1406.0,
        "aai_score": 49.0,
        "mmlu_pro_score": 82.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI startup with limited public disclosure on model training details - Minimal transparency on training compute, data sources, and methodology. Original estimate: 5.07e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.71e+25 (Medium); coding_score: 4.82e+25 (Low); aai_score: 5.56e+25 (Medium); mmlu_pro_score: 5.11e+25 (Medium) \u2192 weighted average: 5.07e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.028397",
      "metadata": {}
    },
    {
      "name": "deepseek_r1",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": 671000000000,
      "parameter_source": "known_specification:deepseek_r1",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.00e+24",
      "training_flop_confidence": "high",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "6.04e+25",
          "confidence": "low",
          "method": "scaling_laws",
          "reasoning": "Known model specification 'deepseek_r1': Chinchilla scaling law: 6 \u00d7 671,000,000,000 params \u00d7 15,000,000,000,000 tokens = 6.04e+25 FLOP"
        },
        {
          "flop": "5.10e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.62e+25 (Medium); coding_score: 4.58e+25 (Medium); aai_score: 5.79e+25 (Medium); mmlu_pro_score: 5.42e+25 (Medium) \u2192 weighted average: 5.10e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1373.0,
        "coding_score": 1382.0,
        "aai_score": 50.0,
        "mmlu_pro_score": 84.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "https://epoch.ai/gradient-updates/what-went-into-training-deepseek-r1",
      "last_updated": "2025-08-11T21:27:15.028709",
      "metadata": {}
    },
    {
      "name": "claude_opus_4",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.50e+26",
      "training_flop_confidence": "low",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "5.06e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.55e+25 (Medium); coding_score: 4.81e+25 (Low); aai_score: 5.12e+25 (Medium); mmlu_pro_score: 5.69e+25 (Medium) \u2192 weighted average: 5.06e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1366.0,
        "coding_score": 1405.0,
        "vision_score": 1212.0,
        "aai_score": 47.0,
        "mmlu_pro_score": 86.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: Speculative estimate for next-gen Claude",
      "last_updated": "2025-08-11T21:27:15.028909",
      "metadata": {}
    },
    {
      "name": "grok_3",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.95e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1423.0,
        "coding_score": 1440.0,
        "aai_score": 46.0,
        "mmlu_pro_score": 79.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.15e+25 (Low); coding_score: 5.18e+25 (Low); aai_score: 4.90e+25 (Medium); mmlu_pro_score: 4.73e+25 (Medium) \u2192 weighted average: 4.95e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.029218",
      "metadata": {}
    },
    {
      "name": "gemini_2.5_flash",
      "developer": "Google",
      "release_date": null,
      "parameters": 400000000000,
      "parameter_source": "known_specification:gemini_2.5_flash",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.80e+25",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "5.91e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.03e+25 (Low); coding_score: 4.97e+25 (Low); aai_score: 7.79e+25 (Medium); mmlu_pro_score: 5.23e+25 (Medium) \u2192 weighted average: 5.91e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1412.0,
        "coding_score": 1421.0,
        "vision_score": 1280.0,
        "aai_score": 58.0,
        "mmlu_pro_score": 83.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'gemini_2.5_flash': Chinchilla scaling law: 6 \u00d7 400,000,000,000 params \u00d7 20,000,000,000,000 tokens = 4.80e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.029590",
      "metadata": {}
    },
    {
      "name": "gpt_4.1",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.85e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1380.0,
        "coding_score": 1398.0,
        "vision_score": 1256.0,
        "aai_score": 47.0,
        "mmlu_pro_score": 80.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.70e+25 (Medium); coding_score: 4.74e+25 (Medium); aai_score: 5.12e+25 (Medium); mmlu_pro_score: 4.83e+25 (Medium) \u2192 weighted average: 4.85e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.029908",
      "metadata": {}
    },
    {
      "name": "claude_sonnet_4_thinking_32k",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.38e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1400.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1400.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 4.38e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.030116",
      "metadata": {}
    },
    {
      "name": "o1",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.18e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1366.0,
        "coding_score": 1378.0,
        "vision_score": 1216.0,
        "aai_score": 52.0,
        "mmlu_pro_score": 84.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.55e+25 (Medium); coding_score: 4.54e+25 (Medium); aai_score: 6.26e+25 (Medium); mmlu_pro_score: 5.38e+25 (Medium) \u2192 weighted average: 5.18e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.030567",
      "metadata": {}
    },
    {
      "name": "qwen3_235b_a22b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 235000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.97e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "5.30e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.81e+25 (Low); coding_score: 4.88e+25 (Low); aai_score: 6.03e+25 (Medium); mmlu_pro_score: 5.17e+25 (Medium) \u2192 weighted average: 5.30e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1391.0,
        "coding_score": 1412.0,
        "aai_score": 51.0,
        "mmlu_pro_score": 82.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 235,000,000,000 params \u00d7 3,525,000,000,000 tokens = 4.97e+24 FLOP (Modern large model: 235B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.031078",
      "metadata": {}
    },
    {
      "name": "o4_mini",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "5.47e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 5.55e+25 (Medium); superclue_math: 5.61e+25 (Medium); superclue_reasoning: 2.94e+25 (Medium); superclue_code: 5.93e+25 (Medium); superclue_agents: 7.34e+25 (Medium) \u2192 weighted average: 5.47e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 73.32,
        "superclue_math": 76.34,
        "superclue_reasoning": 55.56,
        "superclue_code": 86.14,
        "superclue_agents": 86.36
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Models with unidentified developers cannot be properly evaluated - Cannot verify training methodology or model provenance. Original estimate: 5.47e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 5.55e+25 (Medium); superclue_math: 5.61e+25 (Medium); superclue_reasoning: 2.94e+25 (Medium); superclue_code: 5.93e+25 (Medium); superclue_agents: 7.34e+25 (Medium) \u2192 weighted average: 5.47e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.031385",
      "metadata": {}
    },
    {
      "name": "qwen3_coder_480b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 480000000000,
      "parameter_source": "known_specification:qwen3_coder_480b",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.32e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.62e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.47e+25 (Medium); coding_score: 4.80e+25 (Low); aai_score: 4.69e+25 (Medium); mmlu_pro_score: 4.57e+25 (Medium) \u2192 weighted average: 4.62e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1358.0,
        "coding_score": 1404.0,
        "aai_score": 45.0,
        "mmlu_pro_score": 78.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'qwen3_coder_480b': Chinchilla scaling law: 6 \u00d7 480,000,000,000 params \u00d7 15,000,000,000,000 tokens = 4.32e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.031639",
      "metadata": {}
    },
    {
      "name": "deepseek_v3",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": 671000000000,
      "parameter_source": "known_specification:deepseek_v3",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.96e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.25e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.35e+25 (Medium); superclue_math: 3.64e+25 (Medium); superclue_reasoning: 1.47e+25 (Low); superclue_code: 5.76e+25 (Medium); superclue_agents: 5.11e+25 (Medium) \u2192 weighted average: 4.25e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "superclue_overall": 66.54,
        "superclue_math": 64.22,
        "superclue_reasoning": 42.1,
        "superclue_code": 85.15,
        "superclue_agents": 74.7
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Known model specification 'deepseek_v3': Chinchilla scaling law: 6 \u00d7 671,000,000,000 params \u00d7 14,800,000,000,000 tokens = 5.96e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.032194",
      "metadata": {}
    },
    {
      "name": "claude_sonnet_4",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.78e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1338.0,
        "coding_score": 1386.0,
        "vision_score": 1212.0,
        "aai_score": 46.0,
        "mmlu_pro_score": 83.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.28e+25 (Medium); coding_score: 4.62e+25 (Medium); aai_score: 4.90e+25 (Medium); mmlu_pro_score: 5.31e+25 (Medium) \u2192 weighted average: 4.78e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.032540",
      "metadata": {}
    },
    {
      "name": "claude_3_7_sonnet_thinking_32k",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.26e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1387.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1387.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 4.26e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.032789",
      "metadata": {}
    },
    {
      "name": "o1_preview",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.24e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1385.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1385.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 4.24e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.032994",
      "metadata": {}
    },
    {
      "name": "mistral_medium",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.84e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1171.0,
        "coding_score": 1172.0,
        "aai_score": 11.0,
        "mmlu_pro_score": 49.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.87e+25 (Low); coding_score: 2.79e+25 (Low); aai_score: 2.80e+24 (Low); mmlu_pro_score: 1.40e+25 (Low) \u2192 weighted average: 1.84e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.033341",
      "metadata": {}
    },
    {
      "name": "hunyuan_turbos",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "4.58e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 3 benchmarks: openlm_arena_elo: 4.67e+25 (Medium); coding_score: 4.64e+25 (Medium); mmlu_pro_score: 4.45e+25 (Medium) \u2192 weighted average: 4.58e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1377.0,
        "coding_score": 1388.0,
        "mmlu_pro_score": 78.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Major Chinese tech company but with limited transparency on AI model training specifics - Limited disclosure on training methodology and compute for frontier AI models. Original estimate: 4.58e+25 FLOP (Multi-benchmark estimation from 3 benchmarks: openlm_arena_elo: 4.67e+25 (Medium); coding_score: 4.64e+25 (Medium); mmlu_pro_score: 4.45e+25 (Medium) \u2192 weighted average: 4.58e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.033961",
      "metadata": {}
    },
    {
      "name": "minimax_m1",
      "developer": "MiniMax",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "5.09e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.44e+25 (Medium); coding_score: 4.45e+25 (Medium); aai_score: 6.51e+25 (Medium); mmlu_pro_score: 4.99e+25 (Medium) \u2192 weighted average: 5.09e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1354.0,
        "coding_score": 1369.0,
        "aai_score": 53.0,
        "mmlu_pro_score": 81.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company with very limited transparency on model specifications - Insufficient disclosure on training methodology, data sources, and model architecture. Original estimate: 5.09e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.44e+25 (Medium); coding_score: 4.45e+25 (Medium); aai_score: 6.51e+25 (Medium); mmlu_pro_score: 4.99e+25 (Medium) \u2192 weighted average: 5.09e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.034442",
      "metadata": {}
    },
    {
      "name": "gpt_4.1_mini",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.32e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1337.0,
        "coding_score": 1369.0,
        "vision_score": 1230.0,
        "aai_score": 42.0,
        "mmlu_pro_score": 78.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.27e+25 (Medium); coding_score: 4.45e+25 (Medium); aai_score: 4.09e+25 (Medium); mmlu_pro_score: 4.47e+25 (Medium) \u2192 weighted average: 4.32e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.034633",
      "metadata": {}
    },
    {
      "name": "qwen3_235b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 235000000000,
      "parameter_source": "known_specification:qwen3_235b",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.41e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.94e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.55e+25 (Medium); coding_score: 4.70e+25 (Medium); aai_score: 5.34e+25 (Medium); mmlu_pro_score: 5.17e+25 (Medium) \u2192 weighted average: 4.94e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1366.0,
        "coding_score": 1394.0,
        "aai_score": 48.0,
        "mmlu_pro_score": 82.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'qwen3_235b': Chinchilla scaling law: 6 \u00d7 235,000,000,000 params \u00d7 10,000,000,000,000 tokens = 1.41e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.044931",
      "metadata": {}
    },
    {
      "name": "qwen2.5_max",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.98e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1367.0,
        "coding_score": 1373.0,
        "aai_score": 34.0,
        "mmlu_pro_score": 76.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.56e+25 (Medium); coding_score: 4.49e+25 (Medium); aai_score: 2.68e+25 (Medium); mmlu_pro_score: 4.20e+25 (Medium) \u2192 weighted average: 3.98e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.045444",
      "metadata": {}
    },
    {
      "name": "claude_3_7_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.12e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1372.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1372.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 4.12e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.045692",
      "metadata": {}
    },
    {
      "name": "claude_3_5_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.84e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1340.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1340.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.84e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.045908",
      "metadata": {}
    },
    {
      "name": "gemini_2.0_flash_001",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.07e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1366.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1366.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 4.07e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.046087",
      "metadata": {}
    },
    {
      "name": "o3_mini",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.89e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1318.0,
        "coding_score": 1360.0,
        "aai_score": 53.0,
        "mmlu_pro_score": 79.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.09e+25 (Medium); coding_score: 4.36e+25 (Medium); aai_score: 6.51e+25 (Medium); mmlu_pro_score: 4.61e+25 (Medium) \u2192 weighted average: 4.89e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.046461",
      "metadata": {}
    },
    {
      "name": "gemma_3_27b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 27000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.75e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.30e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.46e+25 (Medium); coding_score: 4.25e+25 (Medium); aai_score: 1.45e+25 (Medium); mmlu_pro_score: 3.03e+25 (Medium) \u2192 weighted average: 3.30e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1357.0,
        "coding_score": 1348.0,
        "vision_score": 1214.0,
        "aai_score": 25.0,
        "mmlu_pro_score": 66.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 27,000,000,000 params \u00d7 540,000,000,000 tokens = 8.75e+22 FLOP (Modern model: 27B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.046858",
      "metadata": {}
    },
    {
      "name": "grok_3_mini",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.50e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1361.0,
        "coding_score": 1377.0,
        "aai_score": 58.0,
        "mmlu_pro_score": 82.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.50e+25 (Medium); coding_score: 4.53e+25 (Medium); aai_score: 7.79e+25 (Medium); mmlu_pro_score: 5.17e+25 (Medium) \u2192 weighted average: 5.50e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.047036",
      "metadata": {}
    },
    {
      "name": "llama_3.1_nemotron_ultra_253b_v1",
      "developer": "Meta",
      "release_date": null,
      "parameters": 253000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.59e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.12e+25 (Medium); coding_score: 4.22e+25 (Medium); aai_score: 4.90e+25 (Medium); mmlu_pro_score: 5.12e+25 (Medium) \u2192 weighted average: 4.59e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1321.0,
        "coding_score": 1345.0,
        "aai_score": 46.0,
        "mmlu_pro_score": 82.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 253,000,000,000 params \u00d7 3,795,000,000,000 tokens = 5.76e+24 FLOP (Modern large model: 253B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.047404",
      "metadata": {}
    },
    {
      "name": "gemini_2.0_flash_lite",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.54e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1331.0,
        "coding_score": 1338.0,
        "vision_score": 1147.0,
        "aai_score": 30.0,
        "mmlu_pro_score": 72.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.21e+25 (Medium); coding_score: 4.15e+25 (Medium); aai_score: 2.09e+25 (Medium); mmlu_pro_score: 3.70e+25 (Medium) \u2192 weighted average: 3.54e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.047575",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_pro_002",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.68e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1320.0,
        "coding_score": 1311.0,
        "vision_score": 1208.0,
        "aai_score": 34.0,
        "mmlu_pro_score": 75.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.11e+25 (Medium); coding_score: 3.91e+25 (Medium); aai_score: 2.68e+25 (Medium); mmlu_pro_score: 4.04e+25 (Medium) \u2192 weighted average: 3.68e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.047972",
      "metadata": {}
    },
    {
      "name": "mistral_small",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.54e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1335.0,
        "coding_score": 1361.0,
        "vision_score": 1183.0,
        "aai_score": 32.0,
        "mmlu_pro_score": 68.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.25e+25 (Medium); coding_score: 4.37e+25 (Medium); aai_score: 2.37e+25 (Medium); mmlu_pro_score: 3.17e+25 (Medium) \u2192 weighted average: 3.54e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.048255",
      "metadata": {}
    },
    {
      "name": "command_a_03",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.90e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Benchmark-based (lmarena_score): 1347.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.90e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1347.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Canadian AI company with reasonable disclosure standards and research focus. Original estimate: 3.90e+25 FLOP (Benchmark-based (lmarena_score): 1347.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.90e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.048614",
      "metadata": {}
    },
    {
      "name": "qwen_plus",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.46e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1242.0,
        "coding_score": 1263.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.42e+25 (Medium); coding_score: 3.49e+25 (Medium) \u2192 weighted average: 3.46e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.048755",
      "metadata": {}
    },
    {
      "name": "qwen3_32b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.23e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.51e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.32e+25 (Medium); coding_score: 4.52e+25 (Medium); aai_score: 4.49e+25 (Medium); mmlu_pro_score: 4.72e+25 (Medium) \u2192 weighted average: 4.51e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1342.0,
        "coding_score": 1376.0,
        "aai_score": 44.0,
        "mmlu_pro_score": 79.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 640,000,000,000 tokens = 1.23e+23 FLOP (Modern model: 32B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.049094",
      "metadata": {}
    },
    {
      "name": "glm_4_plus",
      "developer": "Zhipu AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.88e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.90e+25 (Medium); superclue_math: 3.48e+25 (Medium); superclue_reasoning: 1.07e+25 (Low); superclue_code: 5.27e+25 (Medium); superclue_agents: 4.73e+25 (Medium) \u2192 weighted average: 3.88e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 63.67,
        "superclue_math": 63.04,
        "superclue_reasoning": 37.04,
        "superclue_code": 82.18,
        "superclue_agents": 72.41
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company (same as Zhipu) with limited transparency on model training details - Insufficient disclosure on training compute and methodology. Original estimate: 3.88e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.90e+25 (Medium); superclue_math: 3.48e+25 (Medium); superclue_reasoning: 1.07e+25 (Low); superclue_code: 5.27e+25 (Medium); superclue_agents: 4.73e+25 (Medium) \u2192 weighted average: 3.88e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.049437",
      "metadata": {}
    },
    {
      "name": "hunyuan_turbo",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "4.09e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 4.05e+25 (Medium); coding_score: 4.12e+25 (Medium) \u2192 weighted average: 4.09e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1314.0,
        "coding_score": 1335.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Major Chinese tech company but with limited transparency on AI model training specifics - Limited disclosure on training methodology and compute for frontier AI models. Original estimate: 4.09e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 4.05e+25 (Medium); coding_score: 4.12e+25 (Medium) \u2192 weighted average: 4.09e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.049737",
      "metadata": {}
    },
    {
      "name": "gemma_3_12b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 12000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.73e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.94e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.25e+25 (Medium); coding_score: 3.90e+25 (Medium); aai_score: 1.33e+25 (Medium); mmlu_pro_score: 2.26e+25 (Medium) \u2192 weighted average: 2.94e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1335.0,
        "coding_score": 1310.0,
        "aai_score": 24.0,
        "mmlu_pro_score": 59.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 12,000,000,000 params \u00d7 240,000,000,000 tokens = 1.73e+22 FLOP (Modern model: 12B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.049965",
      "metadata": {}
    },
    {
      "name": "gpt_4o",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": 1760000000000,
      "parameter_source": "known_specification:gpt_4o",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.80e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "1.37e+26",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Known model specification 'gpt_4o': Chinchilla scaling law: 6 \u00d7 1,760,000,000,000 params \u00d7 13,000,000,000,000 tokens = 1.37e+26 FLOP"
        },
        {
          "flop": "4.00e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.25e+25 (Medium); superclue_math: 3.71e+25 (Medium); superclue_reasoning: 2.10e+25 (Low); superclue_code: 3.96e+25 (Medium); superclue_agents: 5.36e+25 (Medium) \u2192 weighted average: 4.00e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "superclue_overall": 65.92,
        "superclue_math": 64.74,
        "superclue_reasoning": 48.52,
        "superclue_code": 73.27,
        "superclue_agents": 76.12
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from OpenAI patterns",
      "last_updated": "2025-08-11T21:27:15.050212",
      "metadata": {}
    },
    {
      "name": "qwq_32b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.53e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.21e+25 (Medium); coding_score: 4.26e+25 (Low); aai_score: 5.34e+25 (Medium); mmlu_pro_score: 4.23e+25 (Medium) \u2192 weighted average: 4.53e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1331.0,
        "coding_score": 1349.0,
        "aai_score": 48.0,
        "mmlu_pro_score": 76.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.050600",
      "metadata": {}
    },
    {
      "name": "step_2",
      "developer": "StepFun",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "4.02e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 4.12e+25 (Medium); coding_score: 3.92e+25 (Medium) \u2192 weighted average: 4.02e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1321.0,
        "coding_score": 1313.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company with very limited public information on model training - Extremely limited disclosure on training compute, methodology, and specifications. Original estimate: 4.02e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 4.12e+25 (Medium); coding_score: 3.92e+25 (Medium) \u2192 weighted average: 4.02e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.050881",
      "metadata": {}
    },
    {
      "name": "o1_mini",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.18e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1319.0,
        "coding_score": 1366.0,
        "aai_score": 43.0,
        "mmlu_pro_score": 74.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.10e+25 (Medium); coding_score: 4.42e+25 (Medium); aai_score: 4.28e+25 (Medium); mmlu_pro_score: 3.93e+25 (Medium) \u2192 weighted average: 4.18e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.051048",
      "metadata": {}
    },
    {
      "name": "llama_3.1_405b_instruct_bf16",
      "developer": "Meta",
      "release_date": null,
      "parameters": 405000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.48e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.34e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.80e+25 (Medium); coding_score: 3.80e+25 (Medium); aai_score: 1.95e+25 (Medium); mmlu_pro_score: 3.80e+25 (Medium) \u2192 weighted average: 3.34e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1286.0,
        "coding_score": 1299.0,
        "aai_score": 29.0,
        "mmlu_pro_score": 73.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 405,000,000,000 params \u00d7 6,075,000,000,000 tokens = 1.48e+25 FLOP (Modern large model: 405B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.051383",
      "metadata": {}
    },
    {
      "name": "llama_3.1_405b_instruct_fp8",
      "developer": "Meta",
      "release_date": null,
      "parameters": 405000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.48e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.32e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.78e+25 (Medium); coding_score: 3.74e+25 (Medium); aai_score: 1.95e+25 (Medium); mmlu_pro_score: 3.80e+25 (Medium) \u2192 weighted average: 3.32e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1284.0,
        "coding_score": 1292.0,
        "aai_score": 29.0,
        "mmlu_pro_score": 73.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 405,000,000,000 params \u00d7 6,075,000,000,000 tokens = 1.48e+25 FLOP (Modern large model: 405B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.062027",
      "metadata": {}
    },
    {
      "name": "gemini_advanced",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.77e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1332.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1332.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.77e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.062375",
      "metadata": {}
    },
    {
      "name": "grok_2",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.00e+25",
      "training_flop_confidence": "high",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "3.77e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1332.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.77e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_above_1e25",
      "benchmarks": {
        "lmarena_score": 1332.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: High-precision estimate from xAI disclosure",
      "last_updated": "2025-08-11T21:27:15.062632",
      "metadata": {}
    },
    {
      "name": "qwen3_30b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 30000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.08e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.21e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.11e+25 (Medium); coding_score: 4.23e+25 (Medium); aai_score: 4.09e+25 (Medium); mmlu_pro_score: 4.41e+25 (Medium) \u2192 weighted average: 4.21e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1320.0,
        "coding_score": 1346.0,
        "aai_score": 42.0,
        "mmlu_pro_score": 77.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 30,000,000,000 params \u00d7 600,000,000,000 tokens = 1.08e+23 FLOP (Modern model: 30B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.062999",
      "metadata": {}
    },
    {
      "name": "llama_4_maverick_17b_128e",
      "developer": "Meta",
      "release_date": null,
      "parameters": 17000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.47e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.18e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.85e+25 (Medium); coding_score: 3.92e+25 (Medium); aai_score: 4.09e+25 (Medium); mmlu_pro_score: 4.88e+25 (Medium) \u2192 weighted average: 4.18e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1292.0,
        "coding_score": 1312.0,
        "vision_score": 1184.0,
        "aai_score": 42.0,
        "mmlu_pro_score": 80.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 17,000,000,000 params \u00d7 340,000,000,000 tokens = 3.47e+22 FLOP (Modern model: 17B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.063357",
      "metadata": {}
    },
    {
      "name": "llama_3.3_nemotron_49b_super_v1",
      "developer": "Nvidia",
      "release_date": null,
      "parameters": 49000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.72e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1325.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.72e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1325.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 49,000,000,000 params \u00d7 980,000,000,000 tokens = 2.88e+23 FLOP (Modern model: 49B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.063482",
      "metadata": {}
    },
    {
      "name": "yi_lightning",
      "developer": "01 AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.84e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.93e+25 (Medium); superclue_math: 3.65e+25 (Medium); superclue_reasoning: 1.32e+25 (Low); superclue_code: 4.51e+25 (Medium); superclue_agents: 4.93e+25 (Medium) \u2192 weighted average: 3.84e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 63.88,
        "superclue_math": 64.3,
        "superclue_reasoning": 40.37,
        "superclue_code": 77.23,
        "superclue_agents": 73.61
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company founded by Kai-Fu Lee, relatively new but has some disclosure. Original estimate: 3.84e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.93e+25 (Medium); superclue_math: 3.65e+25 (Medium); superclue_reasoning: 1.32e+25 (Low); superclue_code: 4.51e+25 (Medium); superclue_agents: 4.93e+25 (Medium) \u2192 weighted average: 3.84e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.063889",
      "metadata": {}
    },
    {
      "name": "hunyuan_large",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.88e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.84e+25 (Medium); coding_score: 3.91e+25 (Medium) \u2192 weighted average: 3.88e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1291.0,
        "coding_score": 1311.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Major Chinese tech company but with limited transparency on AI model training specifics - Limited disclosure on training methodology and compute for frontier AI models. Original estimate: 3.88e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.84e+25 (Medium); coding_score: 3.91e+25 (Medium) \u2192 weighted average: 3.88e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.064172",
      "metadata": {}
    },
    {
      "name": "deepseek_v2.5",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.94e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 57.67,
        "superclue_math": 60.34,
        "superclue_reasoning": 38.77,
        "superclue_code": 65.15,
        "superclue_agents": 66.42
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.05e+25 (Medium); superclue_math: 3.12e+25 (Medium); superclue_reasoning: 1.20e+25 (Low); superclue_code: 2.95e+25 (Low); superclue_agents: 3.81e+25 (Medium) \u2192 weighted average: 2.94e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.064367",
      "metadata": {}
    },
    {
      "name": "gpt_4.1_nano",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.18e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1287.0,
        "coding_score": 1312.0,
        "vision_score": 1110.0,
        "aai_score": 30.0,
        "mmlu_pro_score": 65.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.81e+25 (Medium); coding_score: 3.92e+25 (Medium); aai_score: 2.09e+25 (Medium); mmlu_pro_score: 2.90e+25 (Medium) \u2192 weighted average: 3.18e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.064749",
      "metadata": {}
    },
    {
      "name": "gpt_4_turbo",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.12e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1275.0,
        "coding_score": 1280.0,
        "vision_score": 1137.0,
        "aai_score": 28.0,
        "mmlu_pro_score": 69.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.70e+25 (Medium); coding_score: 3.64e+25 (Medium); aai_score: 1.82e+25 (Medium); mmlu_pro_score: 3.33e+25 (Medium) \u2192 weighted average: 3.12e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.065150",
      "metadata": {}
    },
    {
      "name": "claude_3_opus",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": 175000000000,
      "parameter_source": "known_specification:claude_3_opus",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.60e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "8.40e+24",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Known model specification 'claude_3_opus': Chinchilla scaling law: 6 \u00d7 175,000,000,000 params \u00d7 8,000,000,000,000 tokens = 8.40e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "olympic_overall": 25.78,
        "olympic_math": 14.84,
        "olympic_physics": 15.54,
        "olympic_chemistry": 28.57,
        "olympic_biology": 35.48
      },
      "sources": [
        "https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from industry analysis",
      "last_updated": "2025-08-11T21:27:15.065427",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_pro_001",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.67e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1320.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1320.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.67e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.065652",
      "metadata": {}
    },
    {
      "name": "amazon_nova_experimental_chat_05_14",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.66e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1318.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1318.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.66e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.065899",
      "metadata": {}
    },
    {
      "name": "llama_4_scout_17b_16e",
      "developer": "Meta",
      "release_date": null,
      "parameters": 17000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.47e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.51e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.71e+25 (Medium); coding_score: 3.72e+25 (Medium); aai_score: 2.52e+25 (Medium); mmlu_pro_score: 4.06e+25 (Medium) \u2192 weighted average: 3.51e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1276.0,
        "coding_score": 1290.0,
        "vision_score": 1171.0,
        "aai_score": 33.0,
        "mmlu_pro_score": 75.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 17,000,000,000 params \u00d7 340,000,000,000 tokens = 3.47e+22 FLOP (Modern model: 17B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.066382",
      "metadata": {}
    },
    {
      "name": "gemma_3n_e4b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 4000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.92e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.75e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.97e+25 (Medium); coding_score: 3.78e+25 (Medium); aai_score: 7.51e+24 (Low); mmlu_pro_score: 1.38e+25 (Low) \u2192 weighted average: 2.75e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1305.0,
        "coding_score": 1297.0,
        "aai_score": 18.0,
        "mmlu_pro_score": 48.8
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 4,000,000,000 params \u00d7 80,000,000,000 tokens = 1.92e+21 FLOP (Modern model: 4B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.067514",
      "metadata": {}
    },
    {
      "name": "qwen_max",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.79e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 64.47,
        "superclue_math": 66.07,
        "superclue_reasoning": 47.41,
        "superclue_code": 75.64,
        "superclue_agents": 68.76
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.02e+25 (Medium); superclue_math: 3.91e+25 (Medium); superclue_reasoning: 1.98e+25 (Low); superclue_code: 4.28e+25 (Medium); superclue_agents: 4.15e+25 (Medium) \u2192 weighted average: 3.79e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.067724",
      "metadata": {}
    },
    {
      "name": "llama_3.3_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.78e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.81e+25 (Medium); superclue_math: 2.94e+25 (Medium); superclue_reasoning: 1.11e+25 (Low); superclue_code: 2.30e+25 (Low); superclue_agents: 4.01e+25 (Medium) \u2192 weighted average: 2.78e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "superclue_overall": 55.84,
        "superclue_math": 58.92,
        "superclue_reasoning": 37.65,
        "superclue_code": 59.01,
        "superclue_agents": 67.8
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,400,000,000,000 tokens = 5.88e+23 FLOP (Modern model: 70B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.068189",
      "metadata": {}
    },
    {
      "name": "qwen2.5_plus",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.63e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1315.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1315.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.63e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.068366",
      "metadata": {}
    },
    {
      "name": "claude_3_5_haiku",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.65e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1317.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1317.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.65e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.068560",
      "metadata": {}
    },
    {
      "name": "gpt_4o_mini",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.14e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 58.82,
        "superclue_math": 55.45,
        "superclue_reasoning": 40.37,
        "superclue_code": 67.92,
        "superclue_agents": 71.56
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.20e+25 (Medium); superclue_math: 2.52e+25 (Low); superclue_reasoning: 1.32e+25 (Low); superclue_code: 3.27e+25 (Medium); superclue_agents: 4.59e+25 (Medium) \u2192 weighted average: 3.14e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.068908",
      "metadata": {}
    },
    {
      "name": "gpt_4_preview",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.55e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1266.0,
        "coding_score": 1261.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.63e+25 (Medium); coding_score: 3.48e+25 (Medium) \u2192 weighted average: 3.55e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.069111",
      "metadata": {}
    },
    {
      "name": "athene",
      "developer": "NexusFlow",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.63e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Benchmark-based (lmarena_score): 1315.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.63e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1315.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: AI company with focus on open models and reasonable disclosure practices. Original estimate: 3.63e+25 FLOP (Benchmark-based (lmarena_score): 1315.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.63e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.078589",
      "metadata": {}
    },
    {
      "name": "hunyuan_standard",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.71e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.71e+25 (Medium); coding_score: 3.71e+25 (Medium) \u2192 weighted average: 3.71e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1276.0,
        "coding_score": 1289.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Major Chinese tech company but with limited transparency on AI model training specifics - Limited disclosure on training methodology and compute for frontier AI models. Original estimate: 3.71e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.71e+25 (Medium); coding_score: 3.71e+25 (Medium) \u2192 weighted average: 3.71e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.079086",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_flash_002",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.10e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1290.0,
        "coding_score": 1273.0,
        "vision_score": 1187.0,
        "aai_score": 28.0,
        "mmlu_pro_score": 68.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.84e+25 (Medium); coding_score: 3.58e+25 (Medium); aai_score: 1.82e+25 (Medium); mmlu_pro_score: 3.16e+25 (Medium) \u2192 weighted average: 3.10e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.079274",
      "metadata": {}
    },
    {
      "name": "mistral_large",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.09e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1269.0,
        "coding_score": 1284.0,
        "aai_score": 27.0,
        "mmlu_pro_score": 69.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.65e+25 (Medium); coding_score: 3.67e+25 (Medium); aai_score: 1.69e+25 (Medium); mmlu_pro_score: 3.36e+25 (Medium) \u2192 weighted average: 3.09e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.079522",
      "metadata": {}
    },
    {
      "name": "magistral_medium",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.70e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1253.0,
        "coding_score": 1307.0,
        "aai_score": 38.0,
        "mmlu_pro_score": 75.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.51e+25 (Medium); coding_score: 3.87e+25 (Medium); aai_score: 3.35e+25 (Medium); mmlu_pro_score: 4.08e+25 (Medium) \u2192 weighted average: 3.70e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.080041",
      "metadata": {}
    },
    {
      "name": "gemma_3_4b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 4000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.92e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.49e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.86e+25 (Medium); coding_score: 3.51e+25 (Medium); aai_score: 4.54e+24 (Low); mmlu_pro_score: 9.31e+24 (Low) \u2192 weighted average: 2.49e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1293.0,
        "coding_score": 1265.0,
        "aai_score": 14.0,
        "mmlu_pro_score": 41.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 4,000,000,000 params \u00d7 80,000,000,000 tokens = 1.92e+21 FLOP (Modern model: 4B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.080435",
      "metadata": {}
    },
    {
      "name": "grok_2_mini",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.57e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1307.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1307.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.57e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.080529",
      "metadata": {}
    },
    {
      "name": "athene_70b",
      "developer": "NexusFlow",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.61e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.64e+25 (Medium); coding_score: 3.58e+25 (Medium) \u2192 weighted average: 3.61e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1268.0,
        "coding_score": 1274.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.080867",
      "metadata": {}
    },
    {
      "name": "qwen2.5_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.73e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.89e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.94e+25 (Medium); superclue_math: 3.35e+25 (Medium); superclue_reasoning: 1.14e+25 (Low); superclue_code: 2.48e+25 (Low); superclue_agents: 3.81e+25 (Medium) \u2192 weighted average: 2.89e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "superclue_overall": 56.84,
        "superclue_math": 62.11,
        "superclue_reasoning": 38.02,
        "superclue_code": 60.79,
        "superclue_agents": 66.42
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 864,000,000,000 tokens = 3.73e+23 FLOP (Mid-era model: 72B params * 12 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.081221",
      "metadata": {}
    },
    {
      "name": "llama_3.1_nemotron_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.09e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.79e+25 (Medium); coding_score: 3.71e+25 (Medium); aai_score: 1.57e+25 (Medium); mmlu_pro_score: 3.28e+25 (Medium) \u2192 weighted average: 3.09e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1285.0,
        "coding_score": 1289.0,
        "aai_score": 26.0,
        "mmlu_pro_score": 69.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,400,000,000,000 tokens = 5.88e+23 FLOP (Modern model: 70B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.081539",
      "metadata": {}
    },
    {
      "name": "hunyuan_large_vision",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.73e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.66e+25 (Medium); coding_score: 3.79e+25 (Medium) \u2192 weighted average: 3.73e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1270.0,
        "coding_score": 1298.0,
        "vision_score": 1238.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Major Chinese tech company but with limited transparency on AI model training specifics - Limited disclosure on training methodology and compute for frontier AI models. Original estimate: 3.73e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.66e+25 (Medium); coding_score: 3.79e+25 (Medium) \u2192 weighted average: 3.73e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.082877",
      "metadata": {}
    },
    {
      "name": "mistral_small_3.1_24b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 24000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.15e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.92e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.65e+25 (Medium); coding_score: 3.77e+25 (Medium); aai_score: 1.33e+25 (Medium); mmlu_pro_score: 2.92e+25 (Medium) \u2192 weighted average: 2.92e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1269.0,
        "coding_score": 1295.0,
        "vision_score": 1162.0,
        "aai_score": 24.0,
        "mmlu_pro_score": 65.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 24,000,000,000 params \u00d7 288,000,000,000 tokens = 4.15e+22 FLOP (Mid-era model: 24B params * 12 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.083309",
      "metadata": {}
    },
    {
      "name": "llama_3.1_tulu_3_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.48e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.57e+25 (Medium); coding_score: 3.39e+25 (Medium) \u2192 weighted average: 3.48e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1260.0,
        "coding_score": 1251.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,400,000,000,000 tokens = 5.88e+23 FLOP (Modern model: 70B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.083476",
      "metadata": {}
    },
    {
      "name": "llama_3.1_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "known_specification:llama_3.1_70b",
      "context_length": null,
      "architecture": null,
      "training_flop": "6.30e+24",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.23e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.32e+25 (Low); superclue_math: 2.72e+25 (Low); superclue_reasoning: 1.10e+25 (Low); superclue_code: 1.71e+25 (Low); superclue_agents: 2.96e+25 (Medium) \u2192 weighted average: 2.23e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "superclue_overall": 51.74,
        "superclue_math": 57.15,
        "superclue_reasoning": 37.41,
        "superclue_code": 52.38,
        "superclue_agents": 60.02
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Known model specification 'llama_3.1_70b': Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 15,000,000,000,000 tokens = 6.30e+24 FLOP",
      "last_updated": "2025-08-11T21:27:15.083704",
      "metadata": {}
    },
    {
      "name": "llama_3.1_nemotron_51b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 51000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.12e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.27e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.33e+25 (Medium); coding_score: 3.20e+25 (Medium) \u2192 weighted average: 3.27e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1231.0,
        "coding_score": 1227.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 51,000,000,000 params \u00d7 1,020,000,000,000 tokens = 3.12e+23 FLOP (Modern model: 51B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.083852",
      "metadata": {}
    },
    {
      "name": "amazon_nova_pro",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.12e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1262.0,
        "coding_score": 1282.0,
        "vision_score": 1029.0,
        "aai_score": 29.0,
        "mmlu_pro_score": 69.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.59e+25 (Medium); coding_score: 3.65e+25 (Medium); aai_score: 1.95e+25 (Medium); mmlu_pro_score: 3.29e+25 (Medium) \u2192 weighted average: 3.12e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.084173",
      "metadata": {}
    },
    {
      "name": "jamba_1.5_large",
      "developer": "AI21 Labs",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.54e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.42e+25 (Medium); coding_score: 3.34e+25 (Medium); aai_score: 7.51e+24 (Low); mmlu_pro_score: 2.05e+25 (Medium) \u2192 weighted average: 2.54e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1242.0,
        "coding_score": 1244.0,
        "aai_score": 18.0,
        "mmlu_pro_score": 57.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Israeli AI company with reasonable disclosure standards, academic backing. Original estimate: 2.54e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.42e+25 (Medium); coding_score: 3.34e+25 (Medium); aai_score: 7.51e+24 (Low); mmlu_pro_score: 2.05e+25 (Medium) \u2192 weighted average: 2.54e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.084741",
      "metadata": {}
    },
    {
      "name": "reka_core",
      "developer": "Reka AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.34e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.49e+25 (Medium); coding_score: 3.19e+25 (Medium) \u2192 weighted average: 3.34e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1250.0,
        "coding_score": 1226.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: AI company with research focus and reasonable disclosure practices. Original estimate: 3.34e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.49e+25 (Medium); coding_score: 3.19e+25 (Medium) \u2192 weighted average: 3.34e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.085010",
      "metadata": {}
    },
    {
      "name": "gpt_4",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": 1760000000000,
      "parameter_source": "known_specification:gpt_4",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.10e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "1.37e+26",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Known model specification 'gpt_4': Chinchilla scaling law: 6 \u00d7 1,760,000,000,000 params \u00d7 13,000,000,000,000 tokens = 1.37e+26 FLOP"
        },
        {
          "flop": "3.33e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1277.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.33e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1277.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from scaling analysis",
      "last_updated": "2025-08-11T21:27:15.085157",
      "metadata": {}
    },
    {
      "name": "gemma_2_27b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 27000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.25e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.65e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.61e+25 (Low); superclue_math: 2.90e+25 (Medium); superclue_reasoning: 9.90e+24 (Low); superclue_code: 1.87e+25 (Low); superclue_agents: 4.05e+25 (Medium) \u2192 weighted average: 2.65e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "superclue_overall": 54.21,
        "superclue_math": 58.63,
        "superclue_reasoning": 35.93,
        "superclue_code": 54.26,
        "superclue_agents": 68.04
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 27,000,000,000 params \u00d7 324,000,000,000 tokens = 5.25e+22 FLOP (Mid-era model: 27B params * 12 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.095406",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_flash_001",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.38e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "lmarena_score": 1284.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1284.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.38e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.095550",
      "metadata": {}
    },
    {
      "name": "gemma_2_9b_it_simpo",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 9000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.83e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.21e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.35e+25 (Medium); coding_score: 3.08e+25 (Medium) \u2192 weighted average: 3.21e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1233.0,
        "coding_score": 1211.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 9,000,000,000 params \u00d7 108,000,000,000 tokens = 5.83e+21 FLOP (Mid-era model: 9B params * 12 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.096105",
      "metadata": {}
    },
    {
      "name": "claude_3_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.46e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1223.0,
        "coding_score": 1232.0,
        "vision_score": 1033.0,
        "aai_score": 16.0,
        "mmlu_pro_score": 57.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.27e+25 (Medium); coding_score: 3.24e+25 (Medium); aai_score: 5.93e+24 (Low); mmlu_pro_score: 2.11e+25 (Medium) \u2192 weighted average: 2.46e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.096273",
      "metadata": {}
    },
    {
      "name": "command_r_plus_08",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.36e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Benchmark-based (lmarena_score): 1281.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.36e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1281.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Canadian AI company with reasonable disclosure standards and research focus. Original estimate: 3.36e+25 FLOP (Benchmark-based (lmarena_score): 1281.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.36e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.100419",
      "metadata": {}
    },
    {
      "name": "nemotron_4_340b",
      "developer": "Nvidia",
      "release_date": null,
      "parameters": 340000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.04e+25",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.23e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.32e+25 (Medium); coding_score: 3.15e+25 (Medium) \u2192 weighted average: 3.23e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1229.0,
        "coding_score": 1220.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 340,000,000,000 params \u00d7 5,100,000,000,000 tokens = 1.04e+25 FLOP (Generic estimate: 340B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.100694",
      "metadata": {}
    },
    {
      "name": "reka_flash",
      "developer": "Reka AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.12e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.23e+25 (Medium); coding_score: 3.00e+25 (Medium) \u2192 weighted average: 3.12e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1218.0,
        "coding_score": 1201.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: AI company with research focus and reasonable disclosure practices. Original estimate: 3.12e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.23e+25 (Medium); coding_score: 3.00e+25 (Medium) \u2192 weighted average: 3.12e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.101149",
      "metadata": {}
    },
    {
      "name": "glm_4",
      "developer": "Zhipu AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.07e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.07e+25 (Medium); coding_score: 3.06e+25 (Medium) \u2192 weighted average: 3.07e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1198.0,
        "coding_score": 1209.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company (same as Zhipu) with limited transparency on model training details - Insufficient disclosure on training compute and methodology. Original estimate: 3.07e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.07e+25 (Medium); coding_score: 3.06e+25 (Medium) \u2192 weighted average: 3.07e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.101512",
      "metadata": {}
    },
    {
      "name": "llama_3_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "known_specification:llama_3_70b",
      "context_length": null,
      "architecture": null,
      "training_flop": "6.30e+24",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.42e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.28e+25 (Medium); coding_score: 3.12e+25 (Medium); aai_score: 5.93e+24 (Low); mmlu_pro_score: 2.07e+25 (Medium) \u2192 weighted average: 2.42e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1224.0,
        "coding_score": 1216.0,
        "aai_score": 16.0,
        "mmlu_pro_score": 57.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'llama_3_70b': Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 15,000,000,000,000 tokens = 6.30e+24 FLOP",
      "last_updated": "2025-08-11T21:27:15.101830",
      "metadata": {}
    },
    {
      "name": "mistral_small_24b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 24000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.15e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.74e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.39e+25 (Medium); coding_score: 3.39e+25 (Medium); aai_score: 1.33e+25 (Medium); mmlu_pro_score: 2.85e+25 (Medium) \u2192 weighted average: 2.74e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1238.0,
        "coding_score": 1251.0,
        "aai_score": 24.0,
        "mmlu_pro_score": 65.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 24,000,000,000 params \u00d7 288,000,000,000 tokens = 4.15e+22 FLOP (Mid-era model: 24B params * 12 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.102103",
      "metadata": {}
    },
    {
      "name": "qwen2.5_coder_32b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.37e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.78e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.37e+25 (Medium); coding_score: 3.63e+25 (Medium); aai_score: 1.45e+25 (Medium); mmlu_pro_score: 2.66e+25 (Medium) \u2192 weighted average: 2.78e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1235.0,
        "coding_score": 1279.0,
        "aai_score": 25.0,
        "mmlu_pro_score": 63.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 384,000,000,000 tokens = 7.37e+22 FLOP (Mid-era model: 32B params * 12 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.102317",
      "metadata": {}
    },
    {
      "name": "c4ai_aya_expanse_32b",
      "developer": "Cohere",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.26e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1269.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.26e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1269.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.102532",
      "metadata": {}
    },
    {
      "name": "olmo_2_32b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.19e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.27e+25 (Medium); coding_score: 3.11e+25 (Medium) \u2192 weighted average: 3.19e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1223.0,
        "coding_score": 1215.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.102725",
      "metadata": {}
    },
    {
      "name": "qwen2_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.73e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "olympic_overall": 26.89,
        "olympic_math": 16.02,
        "olympic_physics": 16.48,
        "olympic_chemistry": 30.52,
        "olympic_biology": 37.1
      },
      "sources": [
        "https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 864,000,000,000 tokens = 3.73e+23 FLOP (Mid-era model: 72B params * 12 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.102837",
      "metadata": {}
    },
    {
      "name": "command_r_plus",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.24e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Benchmark-based (lmarena_score): 1266.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.24e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1266.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Canadian AI company with reasonable disclosure standards and research focus. Original estimate: 3.24e+25 FLOP (Benchmark-based (lmarena_score): 1266.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.24e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.103085",
      "metadata": {}
    },
    {
      "name": "gemma_2_9b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 9000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.83e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.09e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.19e+25 (Medium); coding_score: 2.95e+25 (Low); aai_score: 2.32e+24 (Low); mmlu_pro_score: 1.43e+25 (Low) \u2192 weighted average: 2.09e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1213.0,
        "coding_score": 1194.0,
        "aai_score": 10.0,
        "mmlu_pro_score": 49.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 9,000,000,000 params \u00d7 108,000,000,000 tokens = 5.83e+21 FLOP (Mid-era model: 9B params * 12 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.103430",
      "metadata": {}
    },
    {
      "name": "deepseek_coder_v2",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": 236000000000,
      "parameter_source": "known_specification:deepseek_coder_v2",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.50e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.26e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.06e+25 (Medium); coding_score: 3.46e+25 (Medium) \u2192 weighted average: 3.26e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1196.0,
        "coding_score": 1259.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'deepseek_coder_v2': Chinchilla scaling law: 6 \u00d7 236,000,000,000 params \u00d7 6,000,000,000,000 tokens = 8.50e+24 FLOP",
      "last_updated": "2025-08-11T21:27:15.103630",
      "metadata": {}
    },
    {
      "name": "claude_3_haiku",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.20e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1200.0,
        "coding_score": 1208.0,
        "vision_score": 1000.0,
        "aai_score": 12.0,
        "mmlu_pro_score": 50.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.09e+25 (Medium); coding_score: 3.06e+25 (Medium); aai_score: 3.34e+24 (Low); mmlu_pro_score: 1.47e+25 (Low) \u2192 weighted average: 2.20e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.103784",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_flash_8b_001",
      "developer": "Google",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.49e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.33e+25 (Medium); coding_score: 3.21e+25 (Medium); aai_score: 8.36e+24 (Low); mmlu_pro_score: 2.02e+25 (Medium) \u2192 weighted average: 2.49e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1231.0,
        "coding_score": 1228.0,
        "vision_score": 1092.0,
        "aai_score": 19.0,
        "mmlu_pro_score": 56.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.104232",
      "metadata": {}
    },
    {
      "name": "amazon_nova_lite",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.61e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1233.0,
        "coding_score": 1253.0,
        "vision_score": 1039.0,
        "aai_score": 25.0,
        "mmlu_pro_score": 59.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.35e+25 (Medium); coding_score: 3.41e+25 (Medium); aai_score: 1.45e+25 (Medium); mmlu_pro_score: 2.22e+25 (Medium) \u2192 weighted average: 2.61e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.104394",
      "metadata": {}
    },
    {
      "name": "phi_4",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.99e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.26e+25 (Medium); coding_score: 3.32e+25 (Medium); aai_score: 1.82e+25 (Medium); mmlu_pro_score: 3.57e+25 (Medium) \u2192 weighted average: 2.99e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1222.0,
        "coding_score": 1242.0,
        "aai_score": 28.0,
        "mmlu_pro_score": 71.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: So far they are not generating large proprietary models. Original estimate: 2.99e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.26e+25 (Medium); coding_score: 3.32e+25 (Medium); aai_score: 1.82e+25 (Medium); mmlu_pro_score: 3.57e+25 (Medium) \u2192 weighted average: 2.99e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.105112",
      "metadata": {}
    },
    {
      "name": "command_r_08",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.14e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Benchmark-based (lmarena_score): 1253.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.14e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1253.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Canadian AI company with reasonable disclosure standards and research focus. Original estimate: 3.14e+25 FLOP (Benchmark-based (lmarena_score): 1253.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.14e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.113552",
      "metadata": {}
    },
    {
      "name": "amazon_nova_micro",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.45e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1215.0,
        "coding_score": 1228.0,
        "aai_score": 20.0,
        "mmlu_pro_score": 53.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.20e+25 (Medium); coding_score: 3.21e+25 (Medium); aai_score: 9.27e+24 (Low); mmlu_pro_score: 1.70e+25 (Low) \u2192 weighted average: 2.45e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.113835",
      "metadata": {}
    },
    {
      "name": "jamba_1.5_mini",
      "developer": "AI21 Labs",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.01e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.03e+25 (Medium); coding_score: 2.97e+25 (Low) \u2192 weighted average: 3.01e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1193.0,
        "coding_score": 1197.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Israeli AI company with reasonable disclosure standards, academic backing. Original estimate: 3.01e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.03e+25 (Medium); coding_score: 2.97e+25 (Low) \u2192 weighted average: 3.01e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.114373",
      "metadata": {}
    },
    {
      "name": "hunyuan_standard_256k",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.25e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.16e+25 (Medium); coding_score: 3.34e+25 (Medium) \u2192 weighted average: 3.25e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1209.0,
        "coding_score": 1244.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Major Chinese tech company but with limited transparency on AI model training specifics - Limited disclosure on training methodology and compute for frontier AI models. Original estimate: 3.25e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.16e+25 (Medium); coding_score: 3.34e+25 (Medium) \u2192 weighted average: 3.25e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.114663",
      "metadata": {}
    },
    {
      "name": "ministral_8b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.07e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.07e+25 (Medium); coding_score: 3.14e+25 (Medium); aai_score: 2.32e+24 (Low); mmlu_pro_score: 7.82e+24 (Low) \u2192 weighted average: 2.07e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1198.0,
        "coding_score": 1219.0,
        "aai_score": 10.0,
        "mmlu_pro_score": 38.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.114899",
      "metadata": {}
    },
    {
      "name": "qwen1.5_110b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 110000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.16e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.09e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 3 benchmarks: openlm_arena_elo: 2.94e+25 (Low); coding_score: 2.94e+25 (Low); aai_score: 3.92e+24 (Low) \u2192 weighted average: 2.09e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1180.0,
        "coding_score": 1192.0,
        "aai_score": 13.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 110,000,000,000 params \u00d7 1,760,000,000,000 tokens = 1.16e+24 FLOP (Chinese model: 110B params * 16 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.115084",
      "metadata": {}
    },
    {
      "name": "qwen1.5_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.98e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.84e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.88e+25 (Low); coding_score: 2.81e+25 (Low) \u2192 weighted average: 2.84e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1172.0,
        "coding_score": 1175.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 1,152,000,000,000 tokens = 4.98e+23 FLOP (Chinese model: 72B params * 16 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.115239",
      "metadata": {}
    },
    {
      "name": "reka_flash_21b_online",
      "developer": "Reka AI",
      "release_date": null,
      "parameters": 21000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.97e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.01e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1235.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.01e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1235.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 21,000,000,000 params \u00d7 315,000,000,000 tokens = 3.97e+22 FLOP (Generic estimate: 21B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.115358",
      "metadata": {}
    },
    {
      "name": "gemini_pro_dev_api",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.00e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1234.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1234.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 3.00e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.115435",
      "metadata": {}
    },
    {
      "name": "mixtral_8x22b_instruct",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 22000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.23e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.94e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.85e+25 (Low); coding_score: 2.81e+25 (Low); aai_score: 4.54e+24 (Low); mmlu_pro_score: 1.75e+25 (Medium) \u2192 weighted average: 1.94e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1168.0,
        "coding_score": 1175.0,
        "aai_score": 14.0,
        "mmlu_pro_score": 53.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 22,000,000,000 params \u00d7 396,000,000,000 tokens = 5.23e+22 FLOP (Specialized model: 22B params * 18 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.115826",
      "metadata": {}
    },
    {
      "name": "command_r",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "1.50e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.85e+25 (Low); coding_score: 2.58e+25 (Low); aai_score: 9.27e+22 (Low); mmlu_pro_score: 5.46e+24 (Low) \u2192 weighted average: 1.50e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1169.0,
        "coding_score": 1141.0,
        "aai_score": 2.0,
        "mmlu_pro_score": 33.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Canadian AI company with reasonable disclosure standards and research focus. Original estimate: 1.50e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.85e+25 (Low); coding_score: 2.58e+25 (Low); aai_score: 9.27e+22 (Low); mmlu_pro_score: 5.46e+24 (Low) \u2192 weighted average: 1.50e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.116245",
      "metadata": {}
    },
    {
      "name": "reka_flash_21b",
      "developer": "Reka AI",
      "release_date": null,
      "parameters": 21000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.97e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.97e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1230.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.97e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1230.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 21,000,000,000 params \u00d7 315,000,000,000 tokens = 3.97e+22 FLOP (Generic estimate: 21B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.116398",
      "metadata": {}
    },
    {
      "name": "llama_3.1_tulu_3_8b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.68e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.04e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.09e+25 (Medium); coding_score: 2.97e+25 (Low) \u2192 weighted average: 3.04e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1200.0,
        "coding_score": 1197.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 160,000,000,000 tokens = 7.68e+21 FLOP (Modern model: 8B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.116567",
      "metadata": {}
    },
    {
      "name": "gemini_pro",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.91e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1222.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1222.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.91e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.116649",
      "metadata": {}
    },
    {
      "name": "gpt_3.5_turbo",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.59e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1138.0,
        "coding_score": 1136.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.63e+25 (Low); coding_score: 2.54e+25 (Low) \u2192 weighted average: 2.59e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.116860",
      "metadata": {}
    },
    {
      "name": "c4ai_aya_expanse_8b",
      "developer": "Cohere",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.93e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1224.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.93e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1224.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.117058",
      "metadata": {}
    },
    {
      "name": "llama_3_8b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "known_specification:llama_3_8b",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.20e+23",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.66e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.87e+25 (Low); coding_score: 2.73e+25 (Low); aai_score: 1.88e+24 (Low); mmlu_pro_score: 8.65e+24 (Low) \u2192 weighted average: 1.66e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1171.0,
        "coding_score": 1164.0,
        "aai_score": 9.0,
        "mmlu_pro_score": 40.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'llama_3_8b': Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 15,000,000,000,000 tokens = 7.20e+23 FLOP",
      "last_updated": "2025-08-11T21:27:15.117265",
      "metadata": {}
    },
    {
      "name": "zephyr_orpo_141b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 141000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.79e+24",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.66e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.72e+25 (Low); coding_score: 2.60e+25 (Low) \u2192 weighted average: 2.66e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1150.0,
        "coding_score": 1144.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 141,000,000,000 params \u00d7 2,115,000,000,000 tokens = 1.79e+24 FLOP (Generic estimate: 141B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.117446",
      "metadata": {}
    },
    {
      "name": "yi_1.5_34b",
      "developer": "01 AI",
      "release_date": null,
      "parameters": 34000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.11e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.33e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.43e+25 (Low); superclue_math: 2.63e+25 (Low); superclue_reasoning: 1.02e+25 (Low); superclue_code: 2.11e+25 (Low); superclue_agents: 3.09e+25 (Medium) \u2192 weighted average: 2.33e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "superclue_overall": 52.69,
        "superclue_math": 56.35,
        "superclue_reasoning": 36.3,
        "superclue_code": 57.03,
        "superclue_agents": 61.08
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 34,000,000,000 params \u00d7 544,000,000,000 tokens = 1.11e+23 FLOP (Chinese model: 34B params * 16 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.117682",
      "metadata": {}
    },
    {
      "name": "granite_3.1_8b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.85e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.77e+25 (Low); coding_score: 2.93e+25 (Low) \u2192 weighted average: 2.85e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1158.0,
        "coding_score": 1191.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.117855",
      "metadata": {}
    },
    {
      "name": "llama_3.1_8b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "known_specification:llama_3.1_8b",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.20e+23",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.14e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.03e+25 (Medium); coding_score: 3.02e+25 (Medium); aai_score: 3.34e+24 (Low); mmlu_pro_score: 1.30e+25 (Low) \u2192 weighted average: 2.14e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1193.0,
        "coding_score": 1203.0,
        "aai_score": 12.0,
        "mmlu_pro_score": 47.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'llama_3.1_8b': Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 15,000,000,000,000 tokens = 7.20e+23 FLOP",
      "last_updated": "2025-08-11T21:27:15.118053",
      "metadata": {}
    },
    {
      "name": "qwen1.5_32b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.83e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.70e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.68e+25 (Low); coding_score: 2.73e+25 (Low) \u2192 weighted average: 2.70e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1144.0,
        "coding_score": 1163.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 512,000,000,000 tokens = 9.83e+22 FLOP (Chinese model: 32B params * 16 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.118215",
      "metadata": {}
    },
    {
      "name": "phi_3_medium_4k",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "1.86e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.68e+25 (Low); coding_score: 2.61e+25 (Low); aai_score: 3.92e+24 (Low); mmlu_pro_score: 1.80e+25 (Medium) \u2192 weighted average: 1.86e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1144.0,
        "coding_score": 1146.0,
        "aai_score": 13.0,
        "mmlu_pro_score": 54.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: So far they are not generating large proprietary models. Original estimate: 1.86e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.68e+25 (Low); coding_score: 2.61e+25 (Low); aai_score: 3.92e+24 (Low); mmlu_pro_score: 1.80e+25 (Medium) \u2192 weighted average: 1.86e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.129342",
      "metadata": {}
    },
    {
      "name": "mixtral_8x7b_instruct",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.29e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.50e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.63e+25 (Low); coding_score: 2.54e+25 (Low); aai_score: 5.79e+23 (Low); mmlu_pro_score: 7.72e+24 (Low) \u2192 weighted average: 1.50e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1138.0,
        "coding_score": 1136.0,
        "aai_score": 5.0,
        "mmlu_pro_score": 38.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 126,000,000,000 tokens = 5.29e+21 FLOP (Specialized model: 7B params * 18 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.130081",
      "metadata": {}
    },
    {
      "name": "gemma_2_2b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.88e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.66e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.81e+25 (Low); coding_score: 2.50e+25 (Low) \u2192 weighted average: 2.66e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1163.0,
        "coding_score": 1130.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 24,000,000,000 tokens = 2.88e+20 FLOP (Mid-era model: 2B params * 12 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.130352",
      "metadata": {}
    },
    {
      "name": "internlm2_5_20b",
      "developer": "InternLM",
      "release_date": null,
      "parameters": 20000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.76e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1200.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.76e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1200.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 20,000,000,000 params \u00d7 300,000,000,000 tokens = 3.60e+22 FLOP (Generic estimate: 20B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.130555",
      "metadata": {}
    },
    {
      "name": "qwen1.5_14b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 14000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.88e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.60e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.61e+25 (Low); coding_score: 2.60e+25 (Low) \u2192 weighted average: 2.60e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1135.0,
        "coding_score": 1144.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 14,000,000,000 params \u00d7 224,000,000,000 tokens = 1.88e+22 FLOP (Chinese model: 14B params * 16 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.130749",
      "metadata": {}
    },
    {
      "name": "dbrx_instruct_preview",
      "developer": "Databricks",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.56e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.55e+25 (Low); coding_score: 2.58e+25 (Low) \u2192 weighted average: 2.56e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1126.0,
        "coding_score": 1141.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Open source focused company with transparent model development practices. Original estimate: 2.56e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.55e+25 (Low); coding_score: 2.58e+25 (Low) \u2192 weighted average: 2.56e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.131160",
      "metadata": {}
    },
    {
      "name": "wizardlm_70b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.41e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.55e+25 (Low); coding_score: 2.26e+25 (Low) \u2192 weighted average: 2.41e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1126.0,
        "coding_score": 1093.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.131362",
      "metadata": {}
    },
    {
      "name": "granite_3.0_8b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.42e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.43e+25 (Low); coding_score: 2.40e+25 (Low) \u2192 weighted average: 2.42e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1108.0,
        "coding_score": 1115.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.131522",
      "metadata": {}
    },
    {
      "name": "deepseek_llm_67b",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": 67000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.04e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.65e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 3 benchmarks: openlm_arena_elo: 2.45e+25 (Low); coding_score: 2.35e+25 (Low); aai_score: 1.48e+24 (Low) \u2192 weighted average: 1.65e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1111.0,
        "coding_score": 1106.0,
        "aai_score": 8.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 67,000,000,000 params \u00d7 1,005,000,000,000 tokens = 4.04e+23 FLOP (Generic estimate: 67B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.131714",
      "metadata": {}
    },
    {
      "name": "yi_34b",
      "developer": "01 AI",
      "release_date": null,
      "parameters": 34000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.11e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.55e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.61e+25 (Low); coding_score: 2.49e+25 (Low) \u2192 weighted average: 2.55e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1134.0,
        "coding_score": 1129.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 34,000,000,000 params \u00d7 544,000,000,000 tokens = 1.11e+23 FLOP (Chinese model: 34B params * 16 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.131961",
      "metadata": {}
    },
    {
      "name": "openchat",
      "developer": "OpenChat",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.28e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.40e+25 (Low); coding_score: 2.17e+25 (Low) \u2192 weighted average: 2.28e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1103.0,
        "coding_score": 1077.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Open source project with transparent development practices. Original estimate: 2.28e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.40e+25 (Low); coding_score: 2.17e+25 (Low) \u2192 weighted average: 2.28e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.132331",
      "metadata": {}
    },
    {
      "name": "granite_3.1_2b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+20",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.68e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.62e+25 (Low); coding_score: 2.75e+25 (Low) \u2192 weighted average: 2.68e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1136.0,
        "coding_score": 1166.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 30,000,000,000 tokens = 3.60e+20 FLOP (Generic estimate: 2B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.132538",
      "metadata": {}
    },
    {
      "name": "tulu_2_dpo_70b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.50e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.56e+25 (Low); coding_score: 2.44e+25 (Low) \u2192 weighted average: 2.50e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1127.0,
        "coding_score": 1120.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.132917",
      "metadata": {}
    },
    {
      "name": "openhermes_2.5_mistral_7b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.29e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.38e+25 (Low); coding_score: 2.20e+25 (Low) \u2192 weighted average: 2.29e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1100.0,
        "coding_score": 1083.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.133123",
      "metadata": {}
    },
    {
      "name": "snowflake_arctic",
      "developer": "Snowflake",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.38e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.44e+25 (Low); coding_score: 2.31e+25 (Low) \u2192 weighted average: 2.38e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1109.0,
        "coding_score": 1101.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Data platform company with transparent model development practices. Original estimate: 2.38e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.44e+25 (Low); coding_score: 2.31e+25 (Low) \u2192 weighted average: 2.38e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.133450",
      "metadata": {}
    },
    {
      "name": "gemma_1.1_7b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.37e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.39e+25 (Low); coding_score: 2.34e+25 (Low) \u2192 weighted average: 2.37e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1102.0,
        "coding_score": 1105.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.133628",
      "metadata": {}
    },
    {
      "name": "vicuna_33b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 33000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.80e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.36e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.46e+25 (Low); coding_score: 2.25e+25 (Low) \u2192 weighted average: 2.36e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1113.0,
        "coding_score": 1091.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 33,000,000,000 params \u00d7 495,000,000,000 tokens = 9.80e+22 FLOP (Generic estimate: 33B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.133764",
      "metadata": {}
    },
    {
      "name": "phi_3_small_8k",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.47e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.49e+25 (Low); coding_score: 2.46e+25 (Low) \u2192 weighted average: 2.47e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1117.0,
        "coding_score": 1123.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: So far they are not generating large proprietary models. Original estimate: 2.47e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.49e+25 (Low); coding_score: 2.46e+25 (Low) \u2192 weighted average: 2.47e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.134021",
      "metadata": {}
    },
    {
      "name": "starling_lm_7b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.64e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.64e+25 (Low); coding_score: 2.64e+25 (Low) \u2192 weighted average: 2.64e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1139.0,
        "coding_score": 1151.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.134282",
      "metadata": {}
    },
    {
      "name": "llama_2_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "known_specification:llama_2_70b",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.40e+23",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.46e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.52e+25 (Low); coding_score: 2.30e+25 (Low); aai_score: 1.48e+24 (Low); mmlu_pro_score: 8.76e+24 (Low) \u2192 weighted average: 1.46e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1122.0,
        "coding_score": 1099.0,
        "aai_score": 8.0,
        "mmlu_pro_score": 40.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'llama_2_70b': Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 2,000,000,000,000 tokens = 8.40e+23 FLOP",
      "last_updated": "2025-08-11T21:27:15.134675",
      "metadata": {}
    },
    {
      "name": "nous_hermes_2_mixtral_8x7b_dpo",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.41e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.50e+25 (Low); coding_score: 2.33e+25 (Low) \u2192 weighted average: 2.41e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1119.0,
        "coding_score": 1103.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.134933",
      "metadata": {}
    },
    {
      "name": "qwq_32b_preview",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.58e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1174.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.58e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1174.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.135080",
      "metadata": {}
    },
    {
      "name": "llama_3.2_3b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 3000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.08e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.37e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.50e+25 (Low); coding_score: 2.29e+25 (Low); aai_score: 1.14e+24 (Low); mmlu_pro_score: 5.88e+24 (Low) \u2192 weighted average: 1.37e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1118.0,
        "coding_score": 1097.0,
        "aai_score": 7.0,
        "mmlu_pro_score": 34.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 3,000,000,000 params \u00d7 60,000,000,000 tokens = 1.08e+21 FLOP (Modern model: 3B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.135463",
      "metadata": {}
    },
    {
      "name": "starling_lm_7b_alpha",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.40e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.47e+25 (Low); coding_score: 2.33e+25 (Low) \u2192 weighted average: 2.40e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1114.0,
        "coding_score": 1104.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.135632",
      "metadata": {}
    },
    {
      "name": "llama2_70b_steerlm",
      "developer": "Nvidia",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.52e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1164.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.52e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1164.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.135742",
      "metadata": {}
    },
    {
      "name": "solar_10.7b_instruct",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 10700000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.24e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.23e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.32e+25 (Low); coding_score: 2.14e+25 (Low) \u2192 weighted average: 2.23e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1091.0,
        "coding_score": 1073.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 10,700,000,000 params \u00d7 192,600,000,000 tokens = 1.24e+22 FLOP (Specialized model: 11B params * 18 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.136052",
      "metadata": {}
    },
    {
      "name": "dolphin_2.2.1_mistral_7b",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.15e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.30e+25 (Low); coding_score: 2.00e+25 (Low) \u2192 weighted average: 2.15e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1088.0,
        "coding_score": 1049.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.145568",
      "metadata": {}
    },
    {
      "name": "granite_3.0_2b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+20",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.33e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.32e+25 (Low); coding_score: 2.33e+25 (Low) \u2192 weighted average: 2.33e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1091.0,
        "coding_score": 1104.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 30,000,000,000 tokens = 3.60e+20 FLOP (Generic estimate: 2B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.145931",
      "metadata": {}
    },
    {
      "name": "mpt_30b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 30000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.10e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.13e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.23e+25 (Low); coding_score: 2.04e+25 (Low) \u2192 weighted average: 2.13e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1076.0,
        "coding_score": 1055.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 30,000,000,000 params \u00d7 450,000,000,000 tokens = 8.10e+22 FLOP (Generic estimate: 30B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.146138",
      "metadata": {}
    },
    {
      "name": "mistral_7b_instruct",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.22e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.36e+25 (Low); coding_score: 2.27e+25 (Low); aai_score: 2.32e+22 (Low); mmlu_pro_score: 2.46e+24 (Low) \u2192 weighted average: 1.22e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1097.0,
        "coding_score": 1094.0,
        "aai_score": 1.0,
        "mmlu_pro_score": 24.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.146343",
      "metadata": {}
    },
    {
      "name": "falcon_180b",
      "developer": "TII",
      "release_date": null,
      "parameters": 180000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.92e+24",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.40e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1145.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.40e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1145.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 180,000,000,000 params \u00d7 2,700,000,000,000 tokens = 2.92e+24 FLOP (Generic estimate: 180B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.146470",
      "metadata": {}
    },
    {
      "name": "wizardlm_13b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.14e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.28e+25 (Low); coding_score: 2.00e+25 (Low) \u2192 weighted average: 2.14e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1084.0,
        "coding_score": 1048.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.146631",
      "metadata": {}
    },
    {
      "name": "phi_3_mini_4k_instruct_june",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.42e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Benchmark-based (lmarena_score): 1149.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.42e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1149.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: So far they are not generating large proprietary models. Original estimate: 2.42e+25 FLOP (Benchmark-based (lmarena_score): 1149.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.42e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.147006",
      "metadata": {}
    },
    {
      "name": "llama_2_13b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.22e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.38e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.33e+25 (Low); coding_score: 2.17e+25 (Low); aai_score: 1.48e+24 (Low); mmlu_pro_score: 8.71e+24 (Low) \u2192 weighted average: 1.38e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1093.0,
        "coding_score": 1077.0,
        "aai_score": 8.0,
        "mmlu_pro_score": 40.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 156,000,000,000 tokens = 1.22e+22 FLOP (Mid-era model: 13B params * 12 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.147252",
      "metadata": {}
    },
    {
      "name": "qwen1.5_7b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.70e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.34e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.31e+25 (Low); coding_score: 2.37e+25 (Low) \u2192 weighted average: 2.34e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1090.0,
        "coding_score": 1110.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 112,000,000,000 tokens = 4.70e+21 FLOP (Chinese model: 7B params * 16 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.147452",
      "metadata": {}
    },
    {
      "name": "vicuna_13b",
      "developer": "LMSYS",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.40e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1146.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.40e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1146.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.147598",
      "metadata": {}
    },
    {
      "name": "codellama_34b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 34000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.39e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.15e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.21e+25 (Low); coding_score: 2.09e+25 (Low) \u2192 weighted average: 2.15e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1073.0,
        "coding_score": 1065.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 34,000,000,000 params \u00d7 680,000,000,000 tokens = 1.39e+23 FLOP (Modern model: 34B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.147750",
      "metadata": {}
    },
    {
      "name": "palm_2",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.37e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1141.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Benchmark-based (lmarena_score): 1141.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.37e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.147911",
      "metadata": {}
    },
    {
      "name": "qwen_14b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 14000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.88e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.37e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1140.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.37e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1140.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 14,000,000,000 params \u00d7 224,000,000,000 tokens = 1.88e+22 FLOP (Chinese model: 14B params * 16 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.148217",
      "metadata": {}
    },
    {
      "name": "gemma_7b",
      "developer": "Google",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.36e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1139.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.36e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1139.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.148333",
      "metadata": {}
    },
    {
      "name": "codellama_70b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.29e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.31e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1131.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.31e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1131.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,260,000,000,000 tokens = 5.29e+23 FLOP (Specialized model: 70B params * 18 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.148439",
      "metadata": {}
    },
    {
      "name": "smollm2_1.7b",
      "developer": "HuggingFace",
      "release_date": null,
      "parameters": 1700000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.60e+20",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.32e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1132.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.32e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1132.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 1,700,000,000 params \u00d7 25,500,000,000 tokens = 2.60e+20 FLOP (Generic estimate: 2B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.148551",
      "metadata": {}
    },
    {
      "name": "zephyr_7b_alpha",
      "developer": "HuggingFace",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.32e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1132.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.32e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1132.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.148671",
      "metadata": {}
    },
    {
      "name": "phi_3_mini_128k",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.35e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Benchmark-based (lmarena_score): 1138.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.35e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1138.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: So far they are not generating large proprietary models. Original estimate: 2.35e+25 FLOP (Benchmark-based (lmarena_score): 1138.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.35e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.149056",
      "metadata": {}
    },
    {
      "name": "zephyr_7b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.13e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.23e+25 (Low); coding_score: 2.02e+25 (Low) \u2192 weighted average: 2.13e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1076.0,
        "coding_score": 1053.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.149262",
      "metadata": {}
    },
    {
      "name": "phi_3_mini_4k",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.29e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.26e+25 (Low); coding_score: 2.32e+25 (Low) \u2192 weighted average: 2.29e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1082.0,
        "coding_score": 1102.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: So far they are not generating large proprietary models. Original estimate: 2.29e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.26e+25 (Low); coding_score: 2.32e+25 (Low) \u2192 weighted average: 2.29e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.149540",
      "metadata": {}
    },
    {
      "name": "guanaco_33b",
      "developer": "UW",
      "release_date": null,
      "parameters": 33000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.80e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.30e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1129.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.30e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1129.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 33,000,000,000 params \u00d7 495,000,000,000 tokens = 9.80e+22 FLOP (Generic estimate: 33B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.149674",
      "metadata": {}
    },
    {
      "name": "stripedhyena_nous_7b",
      "developer": "Together AI",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.27e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1124.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.27e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1124.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.149784",
      "metadata": {}
    },
    {
      "name": "vicuna_7b",
      "developer": "LMSYS",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.24e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1119.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.24e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1119.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.149887",
      "metadata": {}
    },
    {
      "name": "llama_3.2_1b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 1000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.20e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.10e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.17e+25 (Low); coding_score: 2.08e+25 (Low); aai_score: 2.32e+22 (Low); mmlu_pro_score: 1.48e+24 (Low) \u2192 weighted average: 1.10e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1067.0,
        "coding_score": 1063.0,
        "aai_score": 1.0,
        "mmlu_pro_score": 20.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 1,000,000,000 params \u00d7 20,000,000,000 tokens = 1.20e+20 FLOP (Modern model: 1B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.150086",
      "metadata": {}
    },
    {
      "name": "mistral_7b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.21e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1114.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.21e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1114.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.150212",
      "metadata": {}
    },
    {
      "name": "llama_2_7b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "known_specification:llama_2_7b",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.40e+22",
      "training_flop_confidence": "high",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.20e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1113.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.20e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_below_1e25",
      "benchmarks": {
        "lmarena_score": 1113.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Known model specification 'llama_2_7b': Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 2,000,000,000,000 tokens = 8.40e+22 FLOP",
      "last_updated": "2025-08-11T21:27:15.150323",
      "metadata": {}
    },
    {
      "name": "gemma_1.1_2b",
      "developer": "Google",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+20",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.20e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1112.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.20e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1112.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 30,000,000,000 tokens = 3.60e+20 FLOP (Generic estimate: 2B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.150430",
      "metadata": {}
    },
    {
      "name": "gemma_2b",
      "developer": "Google",
      "release_date": null,
      "parameters": 2000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.88e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.08e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1092.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.08e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1092.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 2,000,000,000 params \u00d7 24,000,000,000 tokens = 2.88e+20 FLOP (Mid-era model: 2B params * 12 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.150541",
      "metadata": {}
    },
    {
      "name": "qwen1.5_4b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 4000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.54e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.07e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1091.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.07e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1091.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 4,000,000,000 params \u00d7 64,000,000,000 tokens = 1.54e+21 FLOP (Chinese model: 4B params * 16 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.151295",
      "metadata": {}
    },
    {
      "name": "olmo_7b",
      "developer": "Allen AI",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.02e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1082.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 2.02e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1082.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.151441",
      "metadata": {}
    },
    {
      "name": "koala_13b",
      "developer": "UC Berkeley",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.97e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1072.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.97e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1072.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.151551",
      "metadata": {}
    },
    {
      "name": "gpt4all_13b_snoozy",
      "developer": "Nomic AI",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.95e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1068.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.95e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1068.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.151659",
      "metadata": {}
    },
    {
      "name": "alpaca_13b",
      "developer": "Stanford",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.52e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.93e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1066.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.93e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1066.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 195,000,000,000 tokens = 1.52e+22 FLOP (Generic estimate: 13B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.151769",
      "metadata": {}
    },
    {
      "name": "mpt_7b",
      "developer": "MosaicML",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.92e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1063.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.92e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1063.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.164118",
      "metadata": {}
    },
    {
      "name": "chatglm3_6b",
      "developer": "Tsinghua",
      "release_date": null,
      "parameters": 6000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.89e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.88e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1055.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.88e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1055.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 6,000,000,000 params \u00d7 108,000,000,000 tokens = 3.89e+21 FLOP (Specialized model: 6B params * 18 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.164437",
      "metadata": {}
    },
    {
      "name": "rwkv_4_raven_14b",
      "developer": "RWKV",
      "release_date": null,
      "parameters": 14000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.76e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.82e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1044.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.82e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1044.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 14,000,000,000 params \u00d7 210,000,000,000 tokens = 1.76e+22 FLOP (Generic estimate: 14B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.164596",
      "metadata": {}
    },
    {
      "name": "chatglm2_6b",
      "developer": "Tsinghua",
      "release_date": null,
      "parameters": 6000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.89e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.76e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1033.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.76e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1033.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 6,000,000,000 params \u00d7 108,000,000,000 tokens = 3.89e+21 FLOP (Specialized model: 6B params * 18 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.164767",
      "metadata": {}
    },
    {
      "name": "oasst_pythia_12b",
      "developer": "OpenAssistant",
      "release_date": null,
      "parameters": 12000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.30e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.70e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1021.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.70e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 1021.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 12,000,000,000 params \u00d7 180,000,000,000 tokens = 1.30e+22 FLOP (Generic estimate: 12B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.164892",
      "metadata": {}
    },
    {
      "name": "chatglm_6b",
      "developer": "Tsinghua",
      "release_date": null,
      "parameters": 6000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.89e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.62e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 1004.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.62e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 1004.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 6,000,000,000 params \u00d7 108,000,000,000 tokens = 3.89e+21 FLOP (Specialized model: 6B params * 18 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.165014",
      "metadata": {}
    },
    {
      "name": "fastchat_t5_3b",
      "developer": "LMSYS",
      "release_date": null,
      "parameters": 3000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.72e+20",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.57e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 995.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.57e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "lmarena_score": 995.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 3,000,000,000 params \u00d7 54,000,000,000 tokens = 9.72e+20 FLOP (Specialized model: 3B params * 18 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.165160",
      "metadata": {}
    },
    {
      "name": "llama_13b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 13000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "8.11e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.49e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 978.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.49e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 978.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 13,000,000,000 params \u00d7 104,000,000,000 tokens = 8.11e+21 FLOP (Early era model: 13B params * 8 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.165289",
      "metadata": {}
    },
    {
      "name": "dolly_v2_12b",
      "developer": "Databricks",
      "release_date": null,
      "parameters": 12000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.30e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.48e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 976.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.48e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 976.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 12,000,000,000 params \u00d7 180,000,000,000 tokens = 1.30e+22 FLOP (Generic estimate: 12B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.165411",
      "metadata": {}
    },
    {
      "name": "stablelm_tuned_alpha_7b",
      "developer": "Stability AI",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.40e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Benchmark-based (lmarena_score): 956.0 vs reference llama_3.1_405b (1335.0, 3.80e+25 FLOP) with \u03b1=3.0 = 1.40e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "lmarena_score": 956.0
      },
      "sources": [
        "CSV file with manually collected leaderboard data (LMArena Manual Data Collection)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.165548",
      "metadata": {}
    },
    {
      "name": "claude_3.5_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": 250000000000,
      "parameter_source": "known_specification:claude_3.5_sonnet",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "1.50e+25",
          "confidence": "medium",
          "method": "scaling_laws",
          "reasoning": "Known model specification 'claude_3.5_sonnet': Chinchilla scaling law: 6 \u00d7 250,000,000,000 params \u00d7 10,000,000,000,000 tokens = 1.50e+25 FLOP"
        },
        {
          "flop": "3.96e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.11e+25 (Medium); superclue_math: 2.88e+25 (Medium); superclue_reasoning: 1.79e+25 (Low); superclue_code: 4.72e+25 (Medium); superclue_agents: 5.57e+25 (Medium) \u2192 weighted average: 3.96e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "superclue_overall": 65.01,
        "superclue_math": 58.45,
        "superclue_reasoning": 45.56,
        "superclue_code": 78.61,
        "superclue_agents": 77.31
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: Low-precision estimate from benchmarks",
      "last_updated": "2025-08-11T21:27:15.165814",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_pro",
      "developer": "Google",
      "release_date": null,
      "parameters": 300000000000,
      "parameter_source": "known_specification:gemini_1.5_pro",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.16e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "olympic_overall": 35.09,
        "olympic_math": 21.05,
        "olympic_physics": 23.16,
        "olympic_chemistry": 39.74,
        "olympic_biology": 50.0
      },
      "sources": [
        "https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects)"
      ],
      "reasoning": "Known model specification 'gemini_1.5_pro': Chinchilla scaling law: 6 \u00d7 300,000,000,000 params \u00d7 12,000,000,000,000 tokens = 2.16e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.166129",
      "metadata": {}
    },
    {
      "name": "llama_3.1_405b",
      "developer": "Meta",
      "release_date": null,
      "parameters": 405000000000,
      "parameter_source": "known_specification:llama_3.1_405b",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.80e+25",
      "training_flop_confidence": "high",
      "estimation_method": "epoch_estimate",
      "alternative_estimates": [
        {
          "flop": "3.65e+25",
          "confidence": "high",
          "method": "scaling_laws",
          "reasoning": "Known model specification 'llama_3.1_405b': Chinchilla scaling law: 6 \u00d7 405,000,000,000 params \u00d7 15,000,000,000,000 tokens = 3.65e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "confirmed_above_1e25",
      "benchmarks": {
        "olympic_overall": 28.67,
        "olympic_math": 17.58,
        "olympic_physics": 18.56,
        "olympic_chemistry": 32.47,
        "olympic_biology": 40.32
      },
      "sources": [
        "https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects)"
      ],
      "reasoning": "https://epoch.ai/data-insights/models-over-1e25-flop: High-precision estimate from Meta disclosure",
      "last_updated": "2025-08-11T21:27:15.166400",
      "metadata": {}
    },
    {
      "name": "gemini_1.0_pro",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "olympic_overall": 24.12,
        "olympic_math": 13.28,
        "olympic_physics": 14.09,
        "olympic_chemistry": 26.62,
        "olympic_biology": 32.26
      },
      "sources": [
        "https://gair-nlp.github.io/OlympicArena (OlympicArena - Chinese LLM benchmark across STEM subjects)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-11T21:24:13.056584",
      "metadata": {}
    },
    {
      "name": "gpt_5",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "5.47e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "superclue_overall": 75.34,
        "superclue_math": 76.86,
        "superclue_reasoning": 54.2,
        "superclue_code": 87.92,
        "superclue_agents": 83.21
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 5.94e+25 (Medium); superclue_math: 5.70e+25 (Medium); superclue_reasoning: 2.77e+25 (Medium); superclue_code: 6.24e+25 (Medium); superclue_agents: 6.69e+25 (Medium) \u2192 weighted average: 5.47e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.178698",
      "metadata": {}
    },
    {
      "name": "glm",
      "developer": "Zhipu AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "5.86e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.26e+25 (Low); coding_score: 5.24e+25 (Low); aai_score: 7.27e+25 (Medium); mmlu_pro_score: 5.28e+25 (Medium) \u2192 weighted average: 5.86e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1433.0,
        "coding_score": 1446.0,
        "aai_score": 56.0,
        "mmlu_pro_score": 83.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company (same as Zhipu) with limited transparency on model training details - Insufficient disclosure on training compute and methodology. Original estimate: 5.86e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 5.26e+25 (Low); coding_score: 5.24e+25 (Low); aai_score: 7.27e+25 (Medium); mmlu_pro_score: 5.28e+25 (Medium) \u2192 weighted average: 5.86e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.179350",
      "metadata": {}
    },
    {
      "name": "gemini_2.0_pro",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.40e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1397.0,
        "coding_score": 1398.0,
        "vision_score": 1220.0,
        "aai_score": 38.0,
        "mmlu_pro_score": 80.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.87e+25 (Low); coding_score: 4.74e+25 (Medium); aai_score: 3.35e+25 (Medium); mmlu_pro_score: 4.82e+25 (Medium) \u2192 weighted average: 4.40e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.179549",
      "metadata": {}
    },
    {
      "name": "gemini_2.0_flash",
      "developer": "Google",
      "release_date": null,
      "parameters": 300000000000,
      "parameter_source": "known_specification:gemini_2.0_flash",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.24e+25",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.22e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.59e+25 (Medium); coding_score: 4.47e+25 (Medium); aai_score: 3.35e+25 (Medium); mmlu_pro_score: 4.48e+25 (Medium) \u2192 weighted average: 4.22e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1370.0,
        "coding_score": 1371.0,
        "vision_score": 1241.0,
        "aai_score": 38.0,
        "mmlu_pro_score": 78.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Known model specification 'gemini_2.0_flash': Chinchilla scaling law: 6 \u00d7 300,000,000,000 params \u00d7 18,000,000,000,000 tokens = 3.24e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.179967",
      "metadata": {}
    },
    {
      "name": "glm_4.5_air",
      "developer": "Zhipu AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "5.12e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.83e+25 (Low); coding_score: 4.97e+25 (Low); aai_score: 5.56e+25 (Medium); mmlu_pro_score: 4.97e+25 (Medium) \u2192 weighted average: 5.12e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1393.0,
        "coding_score": 1421.0,
        "aai_score": 49.0,
        "mmlu_pro_score": 81.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company (same as Zhipu) with limited transparency on model training details - Insufficient disclosure on training compute and methodology. Original estimate: 5.12e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.83e+25 (Low); coding_score: 4.97e+25 (Low); aai_score: 5.56e+25 (Medium); mmlu_pro_score: 4.97e+25 (Medium) \u2192 weighted average: 5.12e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.180349",
      "metadata": {}
    },
    {
      "name": "mistral_medium_3",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.23e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1370.0,
        "coding_score": 1388.0,
        "vision_score": 1196.0,
        "aai_score": 39.0,
        "mmlu_pro_score": 76.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.59e+25 (Medium); coding_score: 4.64e+25 (Medium); aai_score: 3.52e+25 (Medium); mmlu_pro_score: 4.17e+25 (Medium) \u2192 weighted average: 4.23e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.180520",
      "metadata": {}
    },
    {
      "name": "gpt_oss_120b",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": 120000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.30e+24",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "6.35e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: aai_score: 8.06e+25 (Medium); mmlu_pro_score: 4.64e+25 (Medium) \u2192 weighted average: 6.35e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "aai_score": 59.0,
        "mmlu_pro_score": 79.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 120,000,000,000 params \u00d7 1,800,000,000,000 tokens = 1.30e+24 FLOP (Generic estimate: 120B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.180831",
      "metadata": {}
    },
    {
      "name": "llama_3.3_nemotron_super_49b_v1.5",
      "developer": "Meta",
      "release_date": null,
      "parameters": 49000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.95e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.29e+25 (Medium); coding_score: 4.28e+25 (Medium); aai_score: 6.26e+25 (Medium); mmlu_pro_score: 4.96e+25 (Medium) \u2192 weighted average: 4.95e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1339.0,
        "coding_score": 1352.0,
        "aai_score": 52.0,
        "mmlu_pro_score": 81.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 49,000,000,000 params \u00d7 980,000,000,000 tokens = 2.88e+23 FLOP (Modern model: 49B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.181061",
      "metadata": {}
    },
    {
      "name": "command_a",
      "developer": "Cohere",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.55e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.17e+25 (Medium); coding_score: 4.12e+25 (Medium); aai_score: 2.37e+25 (Medium); mmlu_pro_score: 3.55e+25 (Medium) \u2192 weighted average: 3.55e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1326.0,
        "coding_score": 1334.0,
        "aai_score": 32.0,
        "mmlu_pro_score": 71.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Canadian AI company with reasonable disclosure standards and research focus. Original estimate: 3.55e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.17e+25 (Medium); coding_score: 4.12e+25 (Medium); aai_score: 2.37e+25 (Medium); mmlu_pro_score: 3.55e+25 (Medium) \u2192 weighted average: 3.55e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.181390",
      "metadata": {}
    },
    {
      "name": "amazon_nova_chat_05_14",
      "developer": "Amazon",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.73e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1322.0,
        "coding_score": 1335.0,
        "aai_score": 35.0,
        "mmlu_pro_score": 73.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.13e+25 (Medium); coding_score: 4.12e+25 (Medium); aai_score: 2.84e+25 (Medium); mmlu_pro_score: 3.81e+25 (Medium) \u2192 weighted average: 3.73e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.181573",
      "metadata": {}
    },
    {
      "name": "claude_3.7_sonnet",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.02e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1301.0,
        "coding_score": 1341.0,
        "vision_score": 1195.0,
        "aai_score": 37.0,
        "mmlu_pro_score": 80.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.93e+25 (Medium); coding_score: 4.18e+25 (Medium); aai_score: 3.17e+25 (Medium); mmlu_pro_score: 4.79e+25 (Medium) \u2192 weighted average: 4.02e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.181905",
      "metadata": {}
    },
    {
      "name": "gpt_oss_20b",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": 20000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.71e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: aai_score: 5.56e+25 (Medium); mmlu_pro_score: 3.85e+25 (Medium) \u2192 weighted average: 4.71e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "aai_score": 49.0,
        "mmlu_pro_score": 73.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 20,000,000,000 params \u00d7 300,000,000,000 tokens = 3.60e+22 FLOP (Generic estimate: 20B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.182205",
      "metadata": {}
    },
    {
      "name": "llama_3.3_nemotron_super_49b_v1",
      "developer": "Meta",
      "release_date": null,
      "parameters": 49000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "2.88e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "4.06e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 4.02e+25 (Medium); coding_score: 3.99e+25 (Medium); aai_score: 3.71e+25 (Medium); mmlu_pro_score: 4.53e+25 (Medium) \u2192 weighted average: 4.06e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1310.0,
        "coding_score": 1320.0,
        "aai_score": 40.0,
        "mmlu_pro_score": 78.5
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 49,000,000,000 params \u00d7 980,000,000,000 tokens = 2.88e+23 FLOP (Modern model: 49B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.182392",
      "metadata": {}
    },
    {
      "name": "grok_2_08_13",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.27e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1306.0,
        "coding_score": 1298.0,
        "aai_score": 28.0,
        "mmlu_pro_score": 70.9
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.98e+25 (Medium); coding_score: 3.79e+25 (Medium); aai_score: 1.82e+25 (Medium); mmlu_pro_score: 3.51e+25 (Medium) \u2192 weighted average: 3.27e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.182553",
      "metadata": {}
    },
    {
      "name": "athene_v2_chat_72b",
      "developer": "NexusFlow",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.60e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "3.93e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.87e+25 (Medium); coding_score: 3.99e+25 (Medium) \u2192 weighted average: 3.93e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1294.0,
        "coding_score": 1320.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 1,296,000,000,000 tokens = 5.60e+23 FLOP (Specialized model: 72B params * 18 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.182827",
      "metadata": {}
    },
    {
      "name": "yi_lightning_lite",
      "developer": "01 AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.73e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.78e+25 (Medium); coding_score: 3.69e+25 (Medium) \u2192 weighted average: 3.73e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1284.0,
        "coding_score": 1286.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company founded by Kai-Fu Lee, relatively new but has some disclosure. Original estimate: 3.73e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.78e+25 (Medium); coding_score: 3.69e+25 (Medium) \u2192 weighted average: 3.73e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.183034",
      "metadata": {}
    },
    {
      "name": "grok_2_mini_08_13",
      "developer": "xAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.70e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1283.0,
        "coding_score": 1279.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.77e+25 (Medium); coding_score: 3.63e+25 (Medium) \u2192 weighted average: 3.70e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.183153",
      "metadata": {}
    },
    {
      "name": "claude_3.5_haiku",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.97e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 56.12,
        "superclue_math": 49.26,
        "superclue_reasoning": 33.83,
        "superclue_code": 66.93,
        "superclue_agents": 74.48
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.84e+25 (Medium); superclue_math: 1.88e+25 (Low); superclue_reasoning: 8.52e+24 (Low); superclue_code: 3.15e+25 (Medium); superclue_agents: 5.07e+25 (Medium) \u2192 weighted average: 2.97e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.183403",
      "metadata": {}
    },
    {
      "name": "deepseek_v2_api",
      "developer": "DeepSeek",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.44e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1240.0,
        "coding_score": 1260.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 3.41e+25 (Medium); coding_score: 3.47e+25 (Medium) \u2192 weighted average: 3.44e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.183623",
      "metadata": {}
    },
    {
      "name": "yi_large",
      "developer": "01 AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.43e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.37e+25 (Low); coding_score: 3.29e+25 (Medium); aai_score: 5.93e+24 (Low); mmlu_pro_score: 2.18e+25 (Medium) \u2192 weighted average: 2.43e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1236.0,
        "coding_score": 1238.0,
        "aai_score": 16.0,
        "mmlu_pro_score": 58.6
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company founded by Kai-Fu Lee, relatively new but has some disclosure. Original estimate: 2.43e+25 FLOP (Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.37e+25 (Low); coding_score: 3.29e+25 (Medium); aai_score: 5.93e+24 (Low); mmlu_pro_score: 2.18e+25 (Medium) \u2192 weighted average: 2.43e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.184168",
      "metadata": {}
    },
    {
      "name": "aya_expanse_32b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.09e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.32e+25 (Medium); coding_score: 3.08e+25 (Medium); aai_score: 1.48e+24 (Low); mmlu_pro_score: 7.23e+24 (Low) \u2192 weighted average: 2.09e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1229.0,
        "coding_score": 1211.0,
        "aai_score": 8.0,
        "mmlu_pro_score": 37.7
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.184397",
      "metadata": {}
    },
    {
      "name": "aya_expanse_8b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 8000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "5.76e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "1.76e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 3.03e+25 (Medium); coding_score: 2.88e+25 (Low); aai_score: 3.71e+23 (Low); mmlu_pro_score: 4.51e+24 (Low) \u2192 weighted average: 1.76e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1193.0,
        "coding_score": 1184.0,
        "aai_score": 4.0,
        "mmlu_pro_score": 31.2
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 8,000,000,000 params \u00d7 120,000,000,000 tokens = 5.76e+21 FLOP (Generic estimate: 8B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.184596",
      "metadata": {}
    },
    {
      "name": "claude_1",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.82e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1178.0,
        "coding_score": 1161.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.92e+25 (Low); coding_score: 2.71e+25 (Low) \u2192 weighted average: 2.82e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.184709",
      "metadata": {}
    },
    {
      "name": "internlm2.5_20b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 20000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.60e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.84e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.85e+25 (Low); coding_score: 2.84e+25 (Low) \u2192 weighted average: 2.84e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1168.0,
        "coding_score": 1179.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 20,000,000,000 params \u00d7 300,000,000,000 tokens = 3.60e+22 FLOP (Generic estimate: 20B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.189915",
      "metadata": {}
    },
    {
      "name": "gemini_1.0_pro_001",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.61e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "openlm_arena_elo": 1155.0,
        "coding_score": 1125.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.75e+25 (Low); coding_score: 2.47e+25 (Low) \u2192 weighted average: 2.61e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.190093",
      "metadata": {}
    },
    {
      "name": "claude_instant_1",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.55e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1135.0,
        "coding_score": 1136.0,
        "aai_score": 2.0,
        "mmlu_pro_score": 43.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 4 benchmarks: openlm_arena_elo: 2.61e+25 (Low); coding_score: 2.54e+25 (Low); aai_score: 9.27e+22 (Low); mmlu_pro_score: 1.03e+25 (Low) \u2192 weighted average: 1.55e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.190480",
      "metadata": {}
    },
    {
      "name": "nv_llama2_70b_steerlm",
      "developer": "Meta",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.20e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.42e+25 (Low); coding_score: 1.99e+25 (Low) \u2192 weighted average: 2.20e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1106.0,
        "coding_score": 1047.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.190838",
      "metadata": {}
    },
    {
      "name": "pplx_70b_online",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 70000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.20e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.37e+25 (Low); coding_score: 2.04e+25 (Low) \u2192 weighted average: 2.20e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1099.0,
        "coding_score": 1055.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 70,000,000,000 params \u00d7 1,050,000,000,000 tokens = 4.41e+23 FLOP (Generic estimate: 70B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.190994",
      "metadata": {}
    },
    {
      "name": "phi_3_mini_4k_instruct_june_24",
      "developer": "Microsoft",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.30e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.30e+25 (Low); coding_score: 2.29e+25 (Low) \u2192 weighted average: 2.30e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "openlm_arena_elo": 1088.0,
        "coding_score": 1098.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: So far they are not generating large proprietary models. Original estimate: 2.30e+25 FLOP (Multi-benchmark estimation from 2 benchmarks: openlm_arena_elo: 2.30e+25 (Low); coding_score: 2.29e+25 (Low) \u2192 weighted average: 2.30e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.191261",
      "metadata": {}
    },
    {
      "name": "qwen2.5_vl_32b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "7.37e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1198.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 384,000,000,000 tokens = 7.37e+22 FLOP (Mid-era model: 32B params * 12 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.191360",
      "metadata": {}
    },
    {
      "name": "step_1o_vision_32k",
      "developer": "StepFun",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1169.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-11T21:24:13.037902",
      "metadata": {}
    },
    {
      "name": "qwen2.5_vl_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.73e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1154.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 864,000,000,000 tokens = 3.73e+23 FLOP (Mid-era model: 72B params * 12 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.191512",
      "metadata": {}
    },
    {
      "name": "pixtral_large",
      "developer": "Mistral",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.49e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "vision_score": 1138.0,
        "aai_score": 26.0,
        "mmlu_pro_score": 70.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: aai_score: 1.57e+25 (Medium); mmlu_pro_score: 3.41e+25 (Medium) \u2192 weighted average: 2.49e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.191640",
      "metadata": {}
    },
    {
      "name": "qwen_vl_max",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1106.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-11T21:24:13.038186",
      "metadata": {}
    },
    {
      "name": "qwen2_vl_72b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.73e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1094.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 864,000,000,000 tokens = 3.73e+23 FLOP (Mid-era model: 72B params * 12 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.191876",
      "metadata": {}
    },
    {
      "name": "step_1v_32k",
      "developer": "StepFun",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1093.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-11T21:24:13.038348",
      "metadata": {}
    },
    {
      "name": "molmo_72b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 72000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.67e+23",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1060.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 72,000,000,000 params \u00d7 1,080,000,000,000 tokens = 4.67e+23 FLOP (Generic estimate: 72B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.191989",
      "metadata": {}
    },
    {
      "name": "pixtral_12b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 12000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.30e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "7.78e+24",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: aai_score: 2.80e+24 (Low); mmlu_pro_score: 1.28e+25 (Low) \u2192 weighted average: 7.78e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1056.0,
        "aai_score": 11.0,
        "mmlu_pro_score": 47.3
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 12,000,000,000 params \u00d7 180,000,000,000 tokens = 1.30e+22 FLOP (Generic estimate: 12B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.192171",
      "metadata": {}
    },
    {
      "name": "internvl2_26b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 26000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "6.08e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1053.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 26,000,000,000 params \u00d7 390,000,000,000 tokens = 6.08e+22 FLOP (Generic estimate: 26B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.192260",
      "metadata": {}
    },
    {
      "name": "llama_3.2_90b_vision",
      "developer": "Meta",
      "release_date": null,
      "parameters": 90000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.72e+23",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.09e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: aai_score: 1.12e+25 (Medium); mmlu_pro_score: 3.06e+25 (Medium) \u2192 weighted average: 2.09e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1047.0,
        "aai_score": 22.0,
        "mmlu_pro_score": 67.1
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 90,000,000,000 params \u00d7 1,800,000,000,000 tokens = 9.72e+23 FLOP (Modern model: 90B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.192413",
      "metadata": {}
    },
    {
      "name": "hunyuan_standard_vision",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1046.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-11T21:24:13.038648",
      "metadata": {}
    },
    {
      "name": "aya_vision_32b",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 32000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "9.22e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1042.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 32,000,000,000 params \u00d7 480,000,000,000 tokens = 9.22e+22 FLOP (Generic estimate: 32B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.192632",
      "metadata": {}
    },
    {
      "name": "qwen2_vl_7b",
      "developer": "Alibaba",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "3.53e+21",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1038.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 84,000,000,000 tokens = 3.53e+21 FLOP (Mid-era model: 7B params * 12 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.192728",
      "metadata": {}
    },
    {
      "name": "yi_vision",
      "developer": "01 AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": null,
      "training_flop_confidence": "speculative",
      "estimation_method": "manual_research",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1028.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "",
      "last_updated": "2025-08-11T21:24:13.040186",
      "metadata": {}
    },
    {
      "name": "llama_3.2_11b_vision",
      "developer": "Meta",
      "release_date": null,
      "parameters": 11000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "1.45e+22",
      "training_flop_confidence": "medium",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "8.04e+24",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 2 benchmarks: aai_score: 3.92e+24 (Low); mmlu_pro_score: 1.22e+25 (Low) \u2192 weighted average: 8.04e+24 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "likely_below_1e25",
      "benchmarks": {
        "vision_score": 1014.0,
        "aai_score": 13.0,
        "mmlu_pro_score": 46.4
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 11,000,000,000 params \u00d7 220,000,000,000 tokens = 1.45e+22 FLOP (Modern model: 11B params * 20 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.192915",
      "metadata": {}
    },
    {
      "name": "molmo_7b_d",
      "developer": "Unknown",
      "release_date": null,
      "parameters": 7000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.41e+21",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "vision_score": 1007.0
      },
      "sources": [
        "https://openlm.ai/chatbot-arena/ (OpenLM Chatbot Arena leaderboard)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 7,000,000,000 params \u00d7 105,000,000,000 tokens = 4.41e+21 FLOP (Generic estimate: 7B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.193004",
      "metadata": {}
    },
    {
      "name": "videopoet",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "1.10e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Threshold-based (physics_iq_score): 20.3 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level)"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "physics_iq_score": 20.3
      },
      "sources": [
        "https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Models with unidentified developers cannot be properly evaluated - Cannot verify training methodology or model provenance. Original estimate: 1.10e+25 FLOP (Threshold-based (physics_iq_score): 20.3 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level))",
      "last_updated": "2025-08-11T21:27:15.193485",
      "metadata": {}
    },
    {
      "name": "lumiere",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "1.10e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Threshold-based (physics_iq_score): 19.0 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level)"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "physics_iq_score": 19.0
      },
      "sources": [
        "https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Models with unidentified developers cannot be properly evaluated - Cannot verify training methodology or model provenance. Original estimate: 1.10e+25 FLOP (Threshold-based (physics_iq_score): 19.0 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level))",
      "last_updated": "2025-08-11T21:27:15.193718",
      "metadata": {}
    },
    {
      "name": "runway_gen_3",
      "developer": "Runway",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.10e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "physics_iq_score": 22.8
      },
      "sources": [
        "https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models)"
      ],
      "reasoning": "Threshold-based (physics_iq_score): 22.8 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level)",
      "last_updated": "2025-08-11T21:27:15.193822",
      "metadata": {}
    },
    {
      "name": "stable_video_diffusion",
      "developer": "Stability AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.10e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "physics_iq_score": 14.8
      },
      "sources": [
        "https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models)"
      ],
      "reasoning": "Threshold-based (physics_iq_score): 14.8 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level)",
      "last_updated": "2025-08-11T21:27:15.194005",
      "metadata": {}
    },
    {
      "name": "pika",
      "developer": "Pika Labs",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "1.10e+25",
          "confidence": "medium",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Threshold-based (physics_iq_score): 13.0 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level)"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "physics_iq_score": 13.0
      },
      "sources": [
        "https://physics-iq.github.io/ (Physics-IQ - Benchmark for physical understanding in video generation models)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Insufficient disclosure on training methodology and specifications. Original estimate: 1.10e+25 FLOP (Threshold-based (physics_iq_score): 13.0 > sora (10.0) \u2192 assumed >1e25 FLOP (reference model assumed frontier-level))",
      "last_updated": "2025-08-11T21:27:15.194309",
      "metadata": {}
    },
    {
      "name": "sora",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.00e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "video_arena_elo": 1234.0,
        "video_quality": 84.3
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 1.00e+25 (Medium); video_quality: 1.00e+25 (Medium) \u2192 weighted average: 1.00e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.194438",
      "metadata": {}
    },
    {
      "name": "doubao_seed_1.6",
      "developer": "ByteDance",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "5.15e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.60e+25 (Medium); superclue_math: 4.64e+25 (Medium); superclue_reasoning: 1.33e+25 (Low); superclue_code: 5.63e+25 (Medium); superclue_agents: 8.29e+25 (Medium) \u2192 weighted average: 5.15e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 68.04,
        "superclue_math": 70.77,
        "superclue_reasoning": 40.49,
        "superclue_code": 84.36,
        "superclue_agents": 90.67
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese tech company with limited transparency on AI model training specifics - Limited disclosure on training methodology and data sources for frontier models. Original estimate: 5.15e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.60e+25 (Medium); superclue_math: 4.64e+25 (Medium); superclue_reasoning: 1.33e+25 (Low); superclue_code: 5.63e+25 (Medium); superclue_agents: 8.29e+25 (Medium) \u2192 weighted average: 5.15e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.194907",
      "metadata": {}
    },
    {
      "name": "claude_opus_4_reasoning",
      "developer": "Anthropic",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "4.27e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 67.02,
        "superclue_math": 60.16,
        "superclue_reasoning": 45.93,
        "superclue_code": 80.4,
        "superclue_agents": 80.6
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 4.43e+25 (Medium); superclue_math: 3.09e+25 (Medium); superclue_reasoning: 1.83e+25 (Low); superclue_code: 4.99e+25 (Medium); superclue_agents: 6.18e+25 (Medium) \u2192 weighted average: 4.27e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.196825",
      "metadata": {}
    },
    {
      "name": "hunyuan_pro",
      "developer": "Tencent",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.78e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.78e+25 (Medium); superclue_math: 2.88e+25 (Medium); superclue_reasoning: 1.06e+25 (Low); superclue_code: 5.24e+25 (Medium); superclue_agents: 5.03e+25 (Medium) \u2192 weighted average: 3.78e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 62.91,
        "superclue_math": 58.51,
        "superclue_reasoning": 36.91,
        "superclue_code": 81.98,
        "superclue_agents": 74.23
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Major Chinese tech company but with limited transparency on AI model training specifics - Limited disclosure on training methodology and compute for frontier AI models. Original estimate: 3.78e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.78e+25 (Medium); superclue_math: 2.88e+25 (Medium); superclue_reasoning: 1.06e+25 (Low); superclue_code: 5.24e+25 (Medium); superclue_agents: 5.03e+25 (Medium) \u2192 weighted average: 3.78e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.201015",
      "metadata": {}
    },
    {
      "name": "doubao_pro",
      "developer": "ByteDance",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.63e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.73e+25 (Medium); superclue_math: 3.13e+25 (Medium); superclue_reasoning: 1.55e+25 (Low); superclue_code: 3.89e+25 (Medium); superclue_agents: 5.15e+25 (Medium) \u2192 weighted average: 3.63e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 62.53,
        "superclue_math": 60.45,
        "superclue_reasoning": 42.96,
        "superclue_code": 72.77,
        "superclue_agents": 74.93
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese tech company with limited transparency on AI model training specifics - Limited disclosure on training methodology and data sources for frontier models. Original estimate: 3.63e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.73e+25 (Medium); superclue_math: 3.13e+25 (Medium); superclue_reasoning: 1.55e+25 (Low); superclue_code: 3.89e+25 (Medium); superclue_agents: 5.15e+25 (Medium) \u2192 weighted average: 3.63e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.201618",
      "metadata": {}
    },
    {
      "name": "step_2_16k",
      "developer": "StepFun",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.57e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.61e+25 (Medium); superclue_math: 3.99e+25 (Medium); superclue_reasoning: 1.10e+25 (Low); superclue_code: 3.52e+25 (Medium); superclue_agents: 4.83e+25 (Medium) \u2192 weighted average: 3.57e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 61.74,
        "superclue_math": 66.61,
        "superclue_reasoning": 37.41,
        "superclue_code": 69.9,
        "superclue_agents": 73.04
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company with very limited public information on model training - Extremely limited disclosure on training compute, methodology, and specifications. Original estimate: 3.57e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.61e+25 (Medium); superclue_math: 3.99e+25 (Medium); superclue_reasoning: 1.10e+25 (Low); superclue_code: 3.52e+25 (Medium); superclue_agents: 4.83e+25 (Medium) \u2192 weighted average: 3.57e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.201975",
      "metadata": {}
    },
    {
      "name": "minimax_01",
      "developer": "MiniMax",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.60e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.55e+25 (Medium); superclue_math: 2.48e+25 (Low); superclue_reasoning: 1.18e+25 (Low); superclue_code: 4.23e+25 (Medium); superclue_agents: 5.39e+25 (Medium) \u2192 weighted average: 3.60e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 61.29,
        "superclue_math": 55.09,
        "superclue_reasoning": 38.52,
        "superclue_code": 75.25,
        "superclue_agents": 76.31
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company with very limited transparency on model specifications - Insufficient disclosure on training methodology, data sources, and model architecture. Original estimate: 3.60e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.55e+25 (Medium); superclue_math: 2.48e+25 (Low); superclue_reasoning: 1.18e+25 (Low); superclue_code: 4.23e+25 (Medium); superclue_agents: 5.39e+25 (Medium) \u2192 weighted average: 3.60e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.202296",
      "metadata": {}
    },
    {
      "name": "moonshot",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.44e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.49e+25 (Medium); superclue_math: 2.90e+25 (Medium); superclue_reasoning: 1.24e+25 (Low); superclue_code: 3.42e+25 (Medium); superclue_agents: 5.44e+25 (Medium) \u2192 weighted average: 3.44e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 60.89,
        "superclue_math": 58.63,
        "superclue_reasoning": 39.26,
        "superclue_code": 69.11,
        "superclue_agents": 76.57
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Models with unidentified developers cannot be properly evaluated - Cannot verify training methodology or model provenance. Original estimate: 3.44e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.49e+25 (Medium); superclue_math: 2.90e+25 (Medium); superclue_reasoning: 1.24e+25 (Low); superclue_code: 3.42e+25 (Medium); superclue_agents: 5.44e+25 (Medium) \u2192 weighted average: 3.44e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.202681",
      "metadata": {}
    },
    {
      "name": "baichuan_4",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "3.25e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.30e+25 (Medium); superclue_math: 2.91e+25 (Medium); superclue_reasoning: 1.06e+25 (Low); superclue_code: 3.67e+25 (Medium); superclue_agents: 4.58e+25 (Medium) \u2192 weighted average: 3.25e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 59.54,
        "superclue_math": 58.68,
        "superclue_reasoning": 36.91,
        "superclue_code": 71.09,
        "superclue_agents": 71.48
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Models with unidentified developers cannot be properly evaluated - Cannot verify training methodology or model provenance. Original estimate: 3.25e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.30e+25 (Medium); superclue_math: 2.91e+25 (Medium); superclue_reasoning: 1.06e+25 (Low); superclue_code: 3.67e+25 (Medium); superclue_agents: 4.58e+25 (Medium) \u2192 weighted average: 3.25e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.202963",
      "metadata": {}
    },
    {
      "name": "gemini_1.5_flash",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "3.11e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 57.89,
        "superclue_math": 54.55,
        "superclue_reasoning": 35.56,
        "superclue_code": 68.91,
        "superclue_agents": 72.56
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 3.07e+25 (Medium); superclue_math: 2.42e+25 (Low); superclue_reasoning: 9.65e+24 (Low); superclue_code: 3.39e+25 (Medium); superclue_agents: 4.75e+25 (Medium) \u2192 weighted average: 3.11e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.203172",
      "metadata": {}
    },
    {
      "name": "internlm_3",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.80e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.82e+25 (Medium); superclue_math: 3.10e+25 (Medium); superclue_reasoning: 1.18e+25 (Low); superclue_code: 2.11e+25 (Low); superclue_agents: 4.04e+25 (Medium) \u2192 weighted average: 2.80e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 55.93,
        "superclue_math": 60.22,
        "superclue_reasoning": 38.52,
        "superclue_code": 57.03,
        "superclue_agents": 67.97
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Models with unidentified developers cannot be properly evaluated - Cannot verify training methodology or model provenance. Original estimate: 2.80e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.82e+25 (Medium); superclue_math: 3.10e+25 (Medium); superclue_reasoning: 1.18e+25 (Low); superclue_code: 2.11e+25 (Low); superclue_agents: 4.04e+25 (Medium) \u2192 weighted average: 2.80e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.203538",
      "metadata": {}
    },
    {
      "name": "glm_4_flash",
      "developer": "Zhipu AI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.90e+24",
      "training_flop_confidence": "speculative",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [
        {
          "flop": "2.80e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Original estimate before developer policy cap: Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.70e+25 (Low); superclue_math: 1.93e+25 (Low); superclue_reasoning: 7.05e+24 (Low); superclue_code: 3.67e+25 (Medium); superclue_agents: 3.98e+25 (Medium) \u2192 weighted average: 2.80e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 54.97,
        "superclue_math": 49.85,
        "superclue_reasoning": 31.36,
        "superclue_code": 71.09,
        "superclue_agents": 67.56
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "FLOP estimate capped at 9.9e+24 due to developer blacklist policy: Chinese AI company (same as Zhipu) with limited transparency on model training details - Insufficient disclosure on training compute and methodology. Original estimate: 2.80e+25 FLOP (Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.70e+25 (Low); superclue_math: 1.93e+25 (Low); superclue_reasoning: 7.05e+24 (Low); superclue_code: 3.67e+25 (Medium); superclue_agents: 3.98e+25 (Medium) \u2192 weighted average: 2.80e+25 FLOP)",
      "last_updated": "2025-08-11T21:27:15.204135",
      "metadata": {}
    },
    {
      "name": "marco_o1",
      "developer": "OpenAI",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "2.72e+25",
      "training_flop_confidence": "low",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 54.76,
        "superclue_math": 60.97,
        "superclue_reasoning": 39.75,
        "superclue_code": 50.59,
        "superclue_agents": 67.73
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.68e+25 (Low); superclue_math: 3.20e+25 (Medium); superclue_reasoning: 1.27e+25 (Low); superclue_code: 1.57e+25 (Low); superclue_agents: 4.00e+25 (Medium) \u2192 weighted average: 2.72e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.204406",
      "metadata": {}
    },
    {
      "name": "mixtral_8x22b",
      "developer": "Mistral",
      "release_date": null,
      "parameters": 22000000000,
      "parameter_source": "extracted_from_name",
      "context_length": null,
      "architecture": null,
      "training_flop": "4.36e+22",
      "training_flop_confidence": "low",
      "estimation_method": "scaling_laws",
      "alternative_estimates": [
        {
          "flop": "2.20e+25",
          "confidence": "low",
          "method": "benchmark_based",
          "reasoning": "Multi-benchmark estimation from 5 benchmarks: superclue_overall: 2.22e+25 (Low); superclue_math: 1.98e+25 (Low); superclue_reasoning: 9.73e+24 (Low); superclue_code: 1.84e+25 (Low); superclue_agents: 3.40e+25 (Medium) \u2192 weighted average: 2.20e+25 FLOP"
        }
      ],
      "inference_flop_per_token": null,
      "status": "uncertain",
      "benchmarks": {
        "superclue_overall": 50.86,
        "superclue_math": 50.33,
        "superclue_reasoning": 35.68,
        "superclue_code": 53.96,
        "superclue_agents": 63.46
      },
      "sources": [
        "https://www.superclueai.com/ (SuperCLUE - Comprehensive Chinese language understanding benchmark)"
      ],
      "reasoning": "Parameter-based Chinchilla scaling: Chinchilla scaling law: 6 \u00d7 22,000,000,000 params \u00d7 330,000,000,000 tokens = 4.36e+22 FLOP (Generic estimate: 22B params * 15 tokens/param)",
      "last_updated": "2025-08-11T21:27:15.204761",
      "metadata": {}
    },
    {
      "name": "veo_3_preview",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.10e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "video_arena_elo": 1298.0,
        "video_quality": 89.2
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 1.10e+25 (Medium); video_quality: 1.10e+25 (Medium) \u2192 weighted average: 1.10e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.204870",
      "metadata": {}
    },
    {
      "name": "veo_2",
      "developer": "Google",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "1.10e+25",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "video_arena_elo": 1267.0,
        "video_quality": 86.7
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 1.10e+25 (Medium); video_quality: 1.10e+25 (Medium) \u2192 weighted average: 1.10e+25 FLOP",
      "last_updated": "2025-08-11T21:27:15.205058",
      "metadata": {}
    },
    {
      "name": "seedance",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "9.25e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "video_arena_elo": 1205.0,
        "video_quality": 81.5
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 9.31e+24 (Low); video_quality: 9.19e+24 (Low) \u2192 weighted average: 9.25e+24 FLOP",
      "last_updated": "2025-08-11T21:27:15.205236",
      "metadata": {}
    },
    {
      "name": "waver",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "8.81e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "video_arena_elo": 1187.0,
        "video_quality": 79.8
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 8.90e+24 (Low); video_quality: 8.72e+24 (Low) \u2192 weighted average: 8.81e+24 FLOP",
      "last_updated": "2025-08-11T21:27:15.205602",
      "metadata": {}
    },
    {
      "name": "runway_gen3",
      "developer": "Runway",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "7.87e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "video_arena_elo": 1156.0,
        "video_quality": 75.2
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 8.22e+24 (Low); video_quality: 7.52e+24 (Low) \u2192 weighted average: 7.87e+24 FLOP",
      "last_updated": "2025-08-11T21:27:15.205904",
      "metadata": {}
    },
    {
      "name": "kling",
      "developer": "Unknown",
      "release_date": null,
      "parameters": null,
      "parameter_source": null,
      "context_length": null,
      "architecture": null,
      "training_flop": "7.35e+24",
      "training_flop_confidence": "medium",
      "estimation_method": "benchmark_based",
      "alternative_estimates": [],
      "inference_flop_per_token": null,
      "status": "likely_above_1e25",
      "benchmarks": {
        "video_arena_elo": 1134.0,
        "video_quality": 72.8
      },
      "sources": [
        "https://artificialanalysis.ai/text-to-video/arena (Artificial Analysis Video Arena - Text-to-video model rankings)"
      ],
      "reasoning": "Multi-benchmark estimation from 2 benchmarks: video_arena_elo: 7.76e+24 (Low); video_quality: 6.93e+24 (Low) \u2192 weighted average: 7.35e+24 FLOP",
      "last_updated": "2025-08-11T21:27:15.206020",
      "metadata": {}
    }
  ]
}